import json
import os
import datetime
import re
from glob import glob
from typing import Dict, Any, List
import requests
from bs4 import BeautifulSoup
import time
from langchain_ollama import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ì½”ìŠ¤ë‹¥ ìƒì¥ ê·œì • í…ìŠ¤íŠ¸ (ê°€ë…ì„± ê°œì„ )
KOSDAQ_LISTING_REQUIREMENTS = """
### 1. ê¸°ë³¸ ìš”ê±´

#### 1-1. ë²•ì¸ í˜•íƒœ ë° ì‚¬ì—… ê¸°ê°„
- **ë²•ì¸ í˜•íƒœ**: ì£¼ì‹íšŒì‚¬ì—¬ì•¼ í•¨
- **ì˜ì—…í™œë™ ê¸°ê°„**: 3ë…„ ì´ìƒì˜ ê³„ì†ì‚¬ì—… ì˜ìœ„ (ë‹¨, ê¸°ìˆ íŠ¹ë¡€ ì‹œ ì™„í™” ê°€ëŠ¥)

#### 1-2. ìë³¸ ìš”ê±´
- **ë‚©ì…ìë³¸ê¸ˆ**: 3ì–µì› ì´ìƒ
- **ìê¸°ìë³¸**: 10ì–µì› ì´ìƒ

### 2. ì£¼ì‹ ë¶„ì‚° ìš”ê±´ (ë‹¤ìŒ ì¤‘ 1ê°œ ì¶©ì¡±)
- **íŠ¸ë™ 1 (ì†Œì•¡ì£¼ì£¼ ê¸°ì¤€)**:
  - ì†Œì•¡ì£¼ì£¼ ìˆ˜: 500ëª… ì´ìƒ
  - ì†Œì•¡ì£¼ì£¼ ì§€ë¶„ìœ¨: 25% ì´ìƒ
- **íŠ¸ë™ 2 (ëŒ€ê·œëª¨ ìê¸°ìë³¸ ê¸°ì¤€)**:
  - ìê¸°ìë³¸: 500ì–µì› ì´ìƒ
  - ì†Œì•¡ì£¼ì£¼ ìˆ˜: 500ëª… ì´ìƒ

### 3. ê²½ì˜ì„±ê³¼ ìš”ê±´ (ë‹¤ìŒ íŠ¸ë™ ì¤‘ 1ê°œ ì„ íƒ)

#### 3-1. ìˆ˜ìµì„±Â·ë§¤ì¶œì•¡ ê¸°ì¤€ (ë‹¤ìŒ ì¤‘ 1ê°œ ì¶©ì¡±)
- **íŠ¸ë™ 1**: ë²•ì¸ì„¸ ì°¨ê°ì „ ê³„ì†ì‚¬ì—…ì´ìµ 20ì–µì› ì´ìƒ (ë²¤ì²˜ê¸°ì—… 10ì–µì›) + ì‹œê°€ì´ì•¡ 90ì–µì› ì´ìƒ
- **íŠ¸ë™ 2**: ë²•ì¸ì„¸ ì°¨ê°ì „ ê³„ì†ì‚¬ì—…ì´ìµ 20ì–µì› ì´ìƒ (ë²¤ì²˜ê¸°ì—… 10ì–µì›) + ìê¸°ìë³¸ 30ì–µì› ì´ìƒ (ë²¤ì²˜ê¸°ì—… 15ì–µì›)
- **íŠ¸ë™ 3**: ë²•ì¸ì„¸ ì°¨ê°ì „ ê³„ì†ì‚¬ì—…ì´ìµ ì–‘ìˆ˜ + ì‹œê°€ì´ì•¡ 200ì–µì› ì´ìƒ + ë§¤ì¶œì•¡ 100ì–µì› ì´ìƒ (ë²¤ì²˜ê¸°ì—… 50ì–µì›)
- **íŠ¸ë™ 4**: ë²•ì¸ì„¸ ì°¨ê°ì „ ê³„ì†ì‚¬ì—…ì´ìµ 50ì–µì› ì´ìƒ

#### 3-2. ì‹œì¥í‰ê°€Â·ì„±ì¥ì„± ê¸°ì¤€ (ë‹¤ìŒ ì¤‘ 1ê°œ ì¶©ì¡±)
- **íŠ¸ë™ 1**: ì‹œê°€ì´ì•¡ 500ì–µì› ì´ìƒ + ë§¤ì¶œì•¡ 30ì–µì› ì´ìƒ + ìµœê·¼ 2ë…„ í‰ê·  ë§¤ì¶œì¦ê°€ìœ¨ 20% ì´ìƒ
- **íŠ¸ë™ 2**: ì‹œê°€ì´ì•¡ 300ì–µì› ì´ìƒ + ë§¤ì¶œì•¡ 100ì–µì› ì´ìƒ (ë²¤ì²˜ê¸°ì—… 50ì–µì›)
- **íŠ¸ë™ 3**: ì‹œê°€ì´ì•¡ 500ì–µì› ì´ìƒ + PBR 200% ì´ìƒ
- **íŠ¸ë™ 4**: ì‹œê°€ì´ì•¡ 1,000ì–µì› ì´ìƒ
- **íŠ¸ë™ 5**: ìê¸°ìë³¸ 250ì–µì› ì´ìƒ

### 4. ê¸°ìˆ í‰ê°€ íŠ¹ë¡€ ê¸°ì¤€ (ë‹¤ìŒ ì¤‘ 1ê°œ ì¶©ì¡±)

#### 4-1. ì¼ë°˜ ê¸°ìˆ í‰ê°€ íŠ¹ë¡€
- **ê¸°ìˆ í‰ê°€**: ì „ë¬¸í‰ê°€ê¸°ê´€ 2ê³³ì—ì„œ BBB ì´ìƒ + A ì´ìƒ ë“±ê¸‰ íšë“
- **ìê¸°ìë³¸**: 10ì–µì› ì´ìƒ
- **ì‹œê°€ì´ì•¡**: 90ì–µì› ì´ìƒ

#### 4-2. ì²¨ë‹¨ê¸°ìˆ  ë‹¨ìˆ˜í‰ê°€ íŠ¹ë¡€
- **ê¸°ìˆ í‰ê°€**: 1ê°œ ê¸°ê´€ì—ì„œ A ì´ìƒ ë“±ê¸‰
- **ì‹œê°€ì´ì•¡**: 1,000ì–µì› ì´ìƒ
- **ë²¤ì²˜íˆ¬ì ìœ ì¹˜**: ìµœê·¼ 5ë…„ê°„ 100ì–µì› ì´ìƒ

### 5. ê¸°íƒ€ í•„ìˆ˜ ìš”ê±´
- **ê°ì‚¬ì˜ê²¬**: ìµœê·¼ ì‚¬ì—…ì—°ë„ ì¬ë¬´ì œí‘œì— ëŒ€í•´ 'ì ì •' ì˜ê²¬ í•„ìˆ˜
- **ê²½ì˜íˆ¬ëª…ì„±**: ì§€ë°°êµ¬ì¡° íˆ¬ëª…ì„± í™•ë³´ ë° ë‚´ë¶€í†µì œì‹œìŠ¤í…œ êµ¬ì¶•
- **ì£¼ì‹ì–‘ë„ ì œí•œ**: ì£¼ì‹ì–‘ë„ì— ì œí•œì´ ì—†ì–´ì•¼ í•¨

### 6. ì½”ìŠ¤ë‹¥ ìƒì¥ ì²´í¬ë¦¬ìŠ¤íŠ¸ ìš”ì•½
- **1ë‹¨ê³„: ê¸°ë³¸ ìš”ê±´**
  - ì£¼ì‹íšŒì‚¬ í˜•íƒœ, 3ë…„ ì´ìƒ ì˜ì—…í™œë™, ë‚©ì…ìë³¸ê¸ˆ 3ì–µì› ì´ìƒ, ìê¸°ìë³¸ 10ì–µì› ì´ìƒ
- **2ë‹¨ê³„: ì£¼ì‹ ë¶„ì‚°**
  - ì†Œì•¡ì£¼ì£¼ 500ëª… + ì§€ë¶„ìœ¨ 25% ì´ìƒ, ë˜ëŠ” ìê¸°ìë³¸ 500ì–µì› + ì†Œì•¡ì£¼ì£¼ 500ëª…
- **3ë‹¨ê³„: ê²½ì˜ì„±ê³¼ (íƒ1)**
  - ìˆ˜ìµì„±/ë§¤ì¶œì•¡ ê¸°ì¤€ (4ê°œ íŠ¸ë™ ì¤‘ íƒ1)
  - ì‹œì¥í‰ê°€/ì„±ì¥ì„± ê¸°ì¤€ (5ê°œ íŠ¸ë™ ì¤‘ íƒ1)
  - ê¸°ìˆ í‰ê°€ íŠ¹ë¡€ ê¸°ì¤€
- **4ë‹¨ê³„: ê¸°íƒ€ ìš”ê±´**
  - ìµœê·¼ ì‚¬ì—…ì—°ë„ ì ì • ê°ì‚¬ì˜ê²¬, ê²½ì˜íˆ¬ëª…ì„± í™•ë³´, ì£¼ì‹ì–‘ë„ ì œí•œ ì—†ìŒ
"""

# LLM ì„¤ì • (IPO_deep_research.pyì—ì„œ ê°€ì ¸ì˜´)
def setup_llm():
    """LLMì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    print("LLMì„ ë¡œë“œí•©ë‹ˆë‹¤...")
    return ChatOllama(
        model="qwen3:32b",
        base_url="http://192.168.120.102:11434",
        temperature=0
    )

def load_company_data(json_file_path: str) -> Dict[str, Any]:
    """JSON íŒŒì¼ì—ì„œ ê¸°ì—… ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"âœ… {data.get('company_name', 'Unknown')} ê¸°ì—… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        return data
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_file_path}")
        return {}
    except json.JSONDecodeError:
        print(f"âŒ JSON íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜: {json_file_path}")
        return {}

def extract_analysis_data(company_data: Dict[str, Any]) -> Dict[str, Any]:
    """Newsë¥¼ ì œì™¸í•˜ê³  ìƒì¥ ë¶„ì„ì— í•„ìš”í•œ ë°ì´í„°ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    analysis_data = {
        "company_name": company_data.get("company_name", ""),
        "extraction_date": company_data.get("extraction_date", ""),
        "url": company_data.get("url", ""),
        "basic_info": company_data.get("basic_info", {}),
        "overview": company_data.get("overview", {}),
        "keywords": company_data.get("keywords", []),
        "key_info": company_data.get("key_info", {}),
        "products": company_data.get("products", [])
    }
    
    # News ë°ì´í„° ê°œìˆ˜ë§Œ í¬í•¨ (í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì ˆì•½)
    news_count = len(company_data.get("news", []))
    analysis_data["news_count"] = news_count
    
    return analysis_data

def extract_article_content(soup):
    """
    ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” ê°•í™”ëœ í•¨ìˆ˜
    ì—¬ëŸ¬ CSS ì…€ë ‰í„°ë¥¼ ì‹œë„í•˜ì—¬ ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ëŒ€ì‘
    """
    # 1ë‹¨ê³„: êµ¬ì²´ì ì¸ ë³¸ë¬¸ ì…€ë ‰í„°ë“¤ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
    high_priority_selectors = [
        # ê°€ì¥ êµ¬ì²´ì ì¸ ê¸°ì‚¬ ë³¸ë¬¸ í´ë˜ìŠ¤ë“¤
        '.article-body', '.news-body', '.post-body', '.content-body',
        '.article-content', '.news-content', '.post-content', 
        '.article-text', '.news-text', '.post-text',
        '.view-content', '.view-body', '.view-text',
        
        # í•œêµ­ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ íŠ¹í™” í´ë˜ìŠ¤ë“¤ (ìƒˆë¡œ ë°œê²¬ëœ íŒ¨í„´ë“¤ ì¶”ê°€)
        '.article-view-content', '.article-veiw-body', '.grid.body',
        '.article_txt', '.news_txt', '.view_txt', '.cont_txt',
        '.article_view', '.news_view', '.view_area',
        '.detail_txt', '.detail_view', '.detail_content',
        
        # ID ê¸°ë°˜ (êµ¬ì²´ì )
        '#articleViewCon', '#article-view-content-div',
        '#article-content', '#news-content', '#article-body', '#news-body',
    ]
    
    # 2ë‹¨ê³„: ì¼ë°˜ì ì¸ ì…€ë ‰í„°ë“¤ (ìš°ì„ ìˆœìœ„ ì¤‘ê°„)
    medium_priority_selectors = [
        # ì†ì„± ê¸°ë°˜ (ë” êµ¬ì²´ì ì¸ ê²ƒë¶€í„°)
        'div[class*="article"][class*="content"]',
        'div[class*="news"][class*="content"]', 
        'div[class*="article"][class*="body"]',
        'div[class*="news"][class*="body"]',
        'div[class*="content"]', 'div[class*="article"]', 'div[class*="news"]',
        'div[class*="text"]', 'div[class*="body"]', 'div[class*="view"]',
        'section[class*="content"]', 'section[class*="article"]',
        
        # ì¼ë°˜ í´ë˜ìŠ¤
        '.content', '.body', '.text',
    ]
    
    # 3ë‹¨ê³„: ê´‘ë²”ìœ„í•œ ì…€ë ‰í„°ë“¤ (ìš°ì„ ìˆœìœ„ ë‚®ìŒ)
    low_priority_selectors = [
        # ì‹œë©˜í‹± íƒœê·¸ (ê°€ì¥ ë§ˆì§€ë§‰ì— ì‹œë„)
        'main',
        'article',
        
        # ë°ì´í„° ì†ì„±
        '[data-article-body]', '[data-content]', '[data-news-content]'
    ]
    
    all_selectors = high_priority_selectors + medium_priority_selectors + low_priority_selectors
    
    print("    ğŸ” ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„ ì¤‘...")
    
    for i, selector in enumerate(all_selectors):
        try:
            elements = soup.select(selector)
            if elements:
                for element in elements:
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° í’ˆì§ˆ ê²€ì¦
                    text_content = extract_text_from_element(element)
                    
                    # í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦ ê°•í™”
                    if is_valid_article_content(text_content):
                        print(f"    âœ… ì…€ë ‰í„° '{selector}'ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ (ê¸¸ì´: {len(text_content)}ì)")
                        return text_content
                        
        except Exception as e:
            print(f"    âš ï¸  ì…€ë ‰í„° '{selector}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
    
    print("    ğŸ”„ ê¸°ë³¸ ì…€ë ‰í„°ë¡œ ì¶”ì¶œ ì‹¤íŒ¨, í´ë°± ë°©ë²• ì‹œë„...")
    
    # í´ë°±: body ì „ì²´ì—ì„œ p íƒœê·¸ë“¤ ìˆ˜ì§‘ (ê¸°ì¡´ ë¡œì§)
    if soup.body:
        paragraphs = soup.body.find_all(['p', 'div'], string=True)
        all_text = []
        
        for p in paragraphs:
            text = p.get_text(strip=True)
            if len(text) > 20 and not is_navigation_text(text):
                all_text.append(text)
        
        combined_text = '\n'.join(all_text)
        if len(combined_text.strip()) > 100:
            print(f"    âœ… í´ë°± ë°©ë²•ìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ (ê¸¸ì´: {len(combined_text)}ì)")
            return combined_text
    
    print("    âŒ ëª¨ë“  ì¶”ì¶œ ë°©ë²• ì‹¤íŒ¨")
    return "ë³¸ë¬¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

def is_valid_article_content(text):
    """ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì‹¤ì œ ê¸°ì‚¬ ë³¸ë¬¸ì¸ì§€ ê²€ì¦í•©ë‹ˆë‹¤."""
    if len(text.strip()) < 100:
        print(f"    âŒ í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶€ì¡±: {len(text.strip())}ì")
        return False
    
    # ê³µìœ /ë„¤ë¹„ê²Œì´ì…˜ í…ìŠ¤íŠ¸ ë¹„ìœ¨ ê²€ì‚¬ (ë” ê´€ëŒ€í•˜ê²Œ)
    nav_ratio = calculate_navigation_ratio(text)
    print(f"    ğŸ“Š ë„¤ë¹„ê²Œì´ì…˜ ë¹„ìœ¨: {nav_ratio:.2%}")
    if nav_ratio > 0.7:  # 70% ì´ìƒì¼ ë•Œë§Œ ì œì™¸ (ê¸°ì¡´ 50%ì—ì„œ ì™„í™”)
        print(f"    âŒ ë„¤ë¹„ê²Œì´ì…˜ í…ìŠ¤íŠ¸ ë¹„ìœ¨ì´ ë„ˆë¬´ ë†’ìŒ: {nav_ratio:.2%}")
        return False
    
    # ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš©ì˜ íŠ¹ì§•ë“¤ í™•ì¸ (ë” ê´€ëŒ€í•˜ê²Œ)
    article_indicators = [
        'ê¸°ì', 'ë‰´ìŠ¤', 'ë³´ë„', 'ë°œí‘œ', 'according to', 'ì— ë”°ë¥´ë©´', 'ë°í˜”ë‹¤', 'ë§í–ˆë‹¤',
        'íšŒì‚¬', 'ê¸°ì—…', 'ì—…ê³„', 'ì‹œì¥', 'ì •ë¶€', 'ë°œí‘œí–ˆë‹¤', 'ì „í–ˆë‹¤', 'ì„¤ëª…í–ˆë‹¤',
        'ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ',  # ì§€ì—­ëª… ì¶”ê°€
        'ì˜¬í•´', 'ì‘ë…„', 'ë‚´ë…„', 'ìµœê·¼',  # ì‹œê°„ í‘œí˜„ ì¶”ê°€
        'ì–µì›', 'ì¡°ì›', 'ë§Œì›', '%',     # ìˆ«ì/ë¹„ìœ¨ í‘œí˜„ ì¶”ê°€
    ]
    
    indicator_count = sum(1 for indicator in article_indicators if indicator in text)
    print(f"    ğŸ“Š ê¸°ì‚¬ ì§€í‘œì–´ ê°œìˆ˜: {indicator_count}ê°œ")
    
    if indicator_count >= 2:  # ê¸°ì‚¬ ì§€í‘œì–´ê°€ 2ê°œ ì´ìƒ ìˆìœ¼ë©´ ìœ íš¨í•œ ê¸°ì‚¬ë¡œ íŒë‹¨
        print(f"    âœ… ê¸°ì‚¬ ì§€í‘œì–´ ì¡°ê±´ í†µê³¼")
        return True
    
    # ë¬¸ì¥ êµ¬ì¡° ê²€ì‚¬ (ë” ê´€ëŒ€í•˜ê²Œ)
    sentences = [s.strip() for s in text.split('.') if len(s.strip()) > 5]  # ìµœì†Œ ê¸¸ì´ ì™„í™”
    print(f"    ğŸ“Š ì™„ì „í•œ ë¬¸ì¥ ê°œìˆ˜: {len(sentences)}ê°œ")
    if len(sentences) >= 3:  # ì™„ì „í•œ ë¬¸ì¥ì´ 3ê°œ ì´ìƒ ìˆìœ¼ë©´ ê¸°ì‚¬ë¡œ íŒë‹¨
        print(f"    âœ… ë¬¸ì¥ êµ¬ì¡° ì¡°ê±´ í†µê³¼")
        return True
    
    # í•œêµ­ì–´ ê¸°ì‚¬ íŠ¹ì„± ê²€ì‚¬ (ìƒˆë¡œ ì¶”ê°€)
    korean_patterns = ['ë‹¤.', 'í–ˆë‹¤.', 'ëœë‹¤.', 'ìˆë‹¤.', 'ì´ë‹¤.', 'í•œë‹¤.']
    korean_ending_count = sum(1 for pattern in korean_patterns if pattern in text)
    print(f"    ğŸ“Š í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²° íŒ¨í„´: {korean_ending_count}ê°œ")
    if korean_ending_count >= 5:  # í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²°ì´ 5ê°œ ì´ìƒ
        print(f"    âœ… í•œêµ­ì–´ ë¬¸ì¥ íŒ¨í„´ ì¡°ê±´ í†µê³¼")
        return True
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ì¶©ë¶„íˆ ê¸¸ë©´ ê¸°ì‚¬ë¡œ ê°„ì£¼ (ìƒˆë¡œ ì¶”ê°€)
    if len(text.strip()) > 500:  # 500ì ì´ìƒì´ë©´ ê¸°ì‚¬ë¡œ ê°„ì£¼
        print(f"    âœ… ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ ê¸¸ì´ ì¡°ê±´ í†µê³¼: {len(text.strip())}ì")
        return True
        
    print(f"    âŒ ëª¨ë“  ìœ íš¨ì„± ê²€ì¦ ì¡°ê±´ ì‹¤íŒ¨")
    print(f"    ğŸ“„ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {text[:100]}...")
    return False

def calculate_navigation_ratio(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ë„¤ë¹„ê²Œì´ì…˜/ê³µìœ  ìš”ì†Œì˜ ë¹„ìœ¨ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
    nav_keywords = [
        'í˜ì´ìŠ¤ë¶', 'íŠ¸ìœ„í„°', 'ì¹´ì¹´ì˜¤í†¡', 'URLë³µì‚¬', 'ì´ë©”ì¼', 'ê³µìœ ', 'ê¸°ì‚¬ë³´ë‚´ê¸°',
        'ëŒ“ê¸€', 'êµ¬ë…', 'ë¡œê·¸ì¸', 'íšŒì›ê°€ì…', 'ë°”ë¡œê°€ê¸°',
        'ë³¸ë¬¸ ê¸€ì”¨', 'ê¸€ì”¨ í‚¤ìš°ê¸°', 'ê¸€ì”¨ ì¤„ì´ê¸°', 'ê¸°ì‚¬ì €ì¥', 'ìŠ¤í¬ë©'
        # 'í™ˆ', 'ë‰´ìŠ¤' ë“±ì€ ì œê±° (ì‹¤ì œ ê¸°ì‚¬ì—ì„œë„ ìì£¼ ì‚¬ìš©ë¨)
    ]
    
    nav_text_length = 0
    for keyword in nav_keywords:
        # í‚¤ì›Œë“œê°€ ì—¬ëŸ¬ ë²ˆ ë‚˜ì™€ë„ í•œ ë²ˆë§Œ ê³„ì‚° (ì¤‘ë³µ íŒ¨ë„í‹° ë°©ì§€)
        if keyword in text:
            nav_text_length += len(keyword)
    
    return nav_text_length / len(text) if len(text) > 0 else 0

def extract_text_from_element(element):
    """HTML ìš”ì†Œì—ì„œ ê¹”ë”í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not element:
        return ""
    
    # ìš”ì†Œ ë³µì‚¬ (ì›ë³¸ ìˆ˜ì • ë°©ì§€)
    element_copy = element.__copy__()
    
    # ë¶ˆí•„ìš”í•œ íƒœê·¸ë“¤ ì œê±°
    unwanted_tags = ['script', 'style', 'nav', 'header', 'footer', 'aside', 'iframe', 'noscript']
    for tag_name in unwanted_tags:
        for tag in element_copy.find_all(tag_name):
            tag.decompose()
    
    # ê³µìœ /ì†Œì…œ ë²„íŠ¼ ê´€ë ¨ í´ë˜ìŠ¤ ì œê±°
    social_classes = ['share', 'social', 'sns', 'facebook', 'twitter', 'kakao', 'url-copy']
    for class_name in social_classes:
        for elem in element_copy.find_all(class_=lambda x: x and any(cls in ' '.join(x).lower() for cls in social_classes)):
            elem.decompose()
    
    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text_parts = []
    
    # p íƒœê·¸ ìš°ì„  ì¶”ì¶œ
    paragraphs = element_copy.find_all('p')
    if paragraphs:
        for p in paragraphs:
            text = p.get_text(strip=True)
            if len(text) > 15 and not is_navigation_text(text):  # ìµœì†Œ ê¸¸ì´ ì¦ê°€
                text_parts.append(text)
    
    # p íƒœê·¸ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ div íƒœê·¸ë„ ì‹œë„
    if len('\n'.join(text_parts)) < 200:
        divs = element_copy.find_all('div')
        for div in divs:
            text = div.get_text(strip=True)
            if len(text) > 30 and text not in text_parts and not is_navigation_text(text):
                text_parts.append(text)
    
    return '\n'.join(text_parts)

def is_navigation_text(text):
    """ë„¤ë¹„ê²Œì´ì…˜ì´ë‚˜ ë©”ë‰´ í…ìŠ¤íŠ¸ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
    nav_keywords = [
        'í™ˆ', 'ë‰´ìŠ¤', 'ì •ì¹˜', 'ê²½ì œ', 'ì‚¬íšŒ', 'ë¬¸í™”', 'ìŠ¤í¬ì¸ ', 'ì—°ì˜ˆ', 'êµ­ì œ',
        'ë¡œê·¸ì¸', 'íšŒì›ê°€ì…', 'êµ¬ë…', 'ê³µìœ ', 'ëŒ“ê¸€', 'ì´ì „', 'ë‹¤ìŒ', 'ëª©ë¡',
        'ì¹´í…Œê³ ë¦¬', 'íƒœê·¸', 'ê´€ë ¨ê¸°ì‚¬', 'ì¸ê¸°ê¸°ì‚¬', 'ìµœì‹ ê¸°ì‚¬',
        'Home', 'News', 'Login', 'Subscribe', 'Share', 'Comment',
        'í˜ì´ìŠ¤ë¶', 'íŠ¸ìœ„í„°', 'ì¹´ì¹´ì˜¤í†¡', 'URLë³µì‚¬', 'ì´ë©”ì¼', 'ê¸°ì‚¬ë³´ë‚´ê¸°',
        'ë³¸ë¬¸ ê¸€ì”¨', 'ê¸€ì”¨ í‚¤ìš°ê¸°', 'ê¸€ì”¨ ì¤„ì´ê¸°', 'ê¸°ì‚¬ì €ì¥', 'ë°”ë¡œê°€ê¸°'
    ]
    
    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ë„¤ë¹„ê²Œì´ì…˜ìœ¼ë¡œ ê°„ì£¼
    if len(text) < 15:
        return True
    
    # ë„¤ë¹„ê²Œì´ì…˜ í‚¤ì›Œë“œ ë¹„ìœ¨ ê²€ì‚¬
    keyword_matches = sum(1 for keyword in nav_keywords if keyword in text)
    if keyword_matches >= 3:  # ë„¤ë¹„ê²Œì´ì…˜ í‚¤ì›Œë“œê°€ 3ê°œ ì´ìƒì´ë©´ ë„¤ë¹„ê²Œì´ì…˜ìœ¼ë¡œ ê°„ì£¼
        return True
    
    # ì§§ì€ í…ìŠ¤íŠ¸ì—ì„œ ë„¤ë¹„ê²Œì´ì…˜ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë„¤ë¹„ê²Œì´ì…˜ìœ¼ë¡œ ê°„ì£¼
    for keyword in nav_keywords:
        if keyword in text and len(text) < 50:
            return True
    
    return False

def crawl_news_articles(news_list: List[Dict[str, Any]], max_articles_to_crawl: int = 3) -> List[Dict[str, Any]]:
    """
    ì£¼ì–´ì§„ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ì—ì„œ URLì„ í¬ë¡¤ë§í•˜ì—¬ ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.

    Args:
        news_list: ë‰´ìŠ¤ ê¸°ì‚¬ ë”•ì…”ë„ˆë¦¬ ëª©ë¡. ê° ë”•ì…”ë„ˆë¦¬ëŠ” 'link' í‚¤ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
        max_articles_to_crawl: í¬ë¡¤ë§í•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜.

    Returns:
        í¬ë¡¤ë§ëœ ë³¸ë¬¸('crawled_content')ì´ ì¶”ê°€ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë”•ì…”ë„ˆë¦¬ ëª©ë¡.
    """
    crawled_count = 0
    if not news_list:
        return []

    print("\n" + "="*80)
    print("ğŸ“° ë‰´ìŠ¤ ë°ì´í„° í¬ë¡¤ë§ ì‹œì‘...")

    for news_item in news_list:
        if crawled_count >= max_articles_to_crawl:
            print(f"\nì„¤ì •ëœ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜({max_articles_to_crawl}ê°œ)ê¹Œì§€ë§Œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
            break

        if 'link' in news_item and news_item['link']:
            url = news_item['link']
            print(f"\nğŸ“° '{news_item.get('title', 'ì œëª© ì—†ìŒ')}' ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œë„:")
            print(f"    URL: {url}")
            
            try:
                # ë‹¤ì–‘í•œ User-Agent ì‹œë„
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1'
                }
                
                response = requests.get(url, headers=headers, timeout=15)
                response.raise_for_status()
                response.encoding = response.apparent_encoding  # ì¸ì½”ë”© ìë™ ê°ì§€
                
                print(f"    âœ… HTTP ì‘ë‹µ ì„±ê³µ (ìƒíƒœ ì½”ë“œ: {response.status_code})")
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ê°•í™”ëœ ë³¸ë¬¸ ì¶”ì¶œ
                content = extract_article_content(soup)
                
                news_item['crawled_content'] = content
                
                print("    ğŸ“„ í¬ë¡¤ë§ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
                print("    " + "-" * 50)
                preview = content[:300] + "..." if len(content) > 300 else content
                print("    " + preview.replace('\n', '\n    '))
                print("    " + "-" * 50)

                crawled_count += 1
                time.sleep(2)  # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•´ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                
            except requests.exceptions.RequestException as e:
                print(f"    âŒ HTTP ìš”ì²­ ì‹¤íŒ¨: {e}")
                news_item['crawled_content'] = f"í¬ë¡¤ë§ ì‹¤íŒ¨: {e}"
            except Exception as e:
                print(f"    âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                news_item['crawled_content'] = f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        else:
            print(f"ğŸŸ¡ '{news_item.get('title', 'ì œëª© ì—†ìŒ')}' ê¸°ì‚¬ëŠ” URL ì •ë³´ê°€ ì—†ì–´ í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            news_item['crawled_content'] = "URL ì •ë³´ê°€ ì—†ì–´ í¬ë¡¤ë§í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    print("="*80)
    return news_list

def create_ipo_analysis_prompt():
    """ì½”ìŠ¤ë‹¥ ìƒì¥ ê°€ëŠ¥ì„± ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    template = """ë‹¹ì‹ ì€ ì½”ìŠ¤ë‹¥ ìƒì¥ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

ì•„ë˜ ê¸°ì—… ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì½”ìŠ¤ë‹¥ ìƒì¥ ê°€ëŠ¥ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

## ì½”ìŠ¤ë‹¥ ìƒì¥ ê¸°ì¤€
{kosdaq_requirements}

## ë¶„ì„ ëŒ€ìƒ ê¸°ì—… ì •ë³´
{company_data}

## ë¶„ì„ ìš”êµ¬ì‚¬í•­ ë° ë°°ì  ê¸°ì¤€
ë‹¤ìŒ í•­ëª©ë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  **ê° í•­ëª©ë³„ ì ìˆ˜**ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”:

### 1. ê¸°ë³¸ ìš”ê±´ ì¶©ì¡±ë„ (25ì  ë§Œì )
- **ì˜ì—…í™œë™ ê¸°ê°„** (5ì ): 3ë…„ ì´ìƒ ì¶©ì¡± ì‹œ 5ì , 2-3ë…„ 3ì , 1-2ë…„ 1ì , 1ë…„ ë¯¸ë§Œ 0ì 
- **ë‚©ì…ìë³¸ê¸ˆ** (5ì ): 3ì–µ ì› ì´ìƒ ì¶©ì¡± ì‹œ 5ì , ë¶€ì¡± ì‹œ 0ì 
- **ìê¸°ìë³¸** (10ì ): 10ì–µ ì› ì´ìƒ 10ì , 5-10ì–µ 5ì , 3-5ì–µ 3ì , 3ì–µ ë¯¸ë§Œ 0ì 
- **ì†Œì•¡ì£¼ì£¼ í™•ë³´ ê°€ëŠ¥ì„±** (5ì ): í˜„ì¬ ì£¼ì£¼ êµ¬ì¡° ë° í™•ë³´ ê°€ëŠ¥ì„± í‰ê°€

### 2. ê²½ì˜ì„±ê³¼ ë° ì¬ë¬´ ë¶„ì„ (30ì  ë§Œì )
- **ë§¤ì¶œ ì„±ì¥ì„±** (10ì ): ìµœê·¼ 2ë…„ ì—°ì† 20% ì´ìƒ ì„±ì¥ 10ì , 10-20% 7ì , 0-10% 4ì , ë§ˆì´ë„ˆìŠ¤ ì„±ì¥ 0ì 
- **ìˆ˜ìµì„±** (10ì ): í‘ì ê¸°ì—… 10ì , ì†ìµë¶„ê¸°ì  ê·¼ì ‘ 5ì , ì ì ê¸°ì—… 0ì 
- **ì¬ë¬´ ì•ˆì •ì„±** (10ì ): ë¶€ì±„ë¹„ìœ¨, ìœ ë™ì„± ë“± ì¢…í•© í‰ê°€

### 3. ìƒì¥ íŠ¸ë™ ì í•©ì„± (20ì  ë§Œì )
- **ì´ìµì‹¤í˜„ ìƒì¥ìš”ê±´** (10ì ): ê° íŠ¸ë™ë³„ ìš”ê±´ ì¶©ì¡±ë„ í‰ê°€
- **ì´ìµë¯¸ì‹¤í˜„ ìƒì¥ìš”ê±´(í…ŒìŠ¬ë¼ ìš”ê±´)** (10ì ): ì‹œê°€ì´ì•¡, ë§¤ì¶œ ê¸°ì¤€ ì¶©ì¡±ë„ í‰ê°€

### 4. ê¸°ìˆ ì„±ì¥ê¸°ì—… íŠ¹ë¡€ ì ìš© ê°€ëŠ¥ì„± (10ì  ë§Œì )
- **ê¸°ìˆ  í˜ì‹ ì„±** (5ì ): AI, í—¬ìŠ¤ì¼€ì–´ ë“± í˜ì‹  ê¸°ìˆ  ë³´ìœ ë„
- **ê¸°ìˆ  í‰ê°€ ê°€ëŠ¥ì„±** (5ì ): ì „ë¬¸í‰ê°€ê¸°ê´€ Aë“±ê¸‰ íšë“ ê°€ëŠ¥ì„±

### 5. ì •ì„±ì  ìš”ì†Œ (10ì  ë§Œì )
- **ì‹œì¥ ì„±ì¥ì„±** (3ì ): ë°˜ë ¤ë™ë¬¼ í—¬ìŠ¤ì¼€ì–´ ì‹œì¥ ë“± ì„±ì¥ ì‹œì¥ ì§„ì¶œ
- **ê²½ìŸ ìš°ìœ„** (3ì ): ì°¨ë³„í™”ëœ ê¸°ìˆ /ì„œë¹„ìŠ¤ ë³´ìœ 
- **ê²½ì˜ì§„ ì—­ëŸ‰** (2ì ): ëŒ€í‘œì ë° ê²½ì˜ì§„ì˜ ì—…ê³„ ê²½í—˜
- **ê±°ë²„ë„ŒìŠ¤** (2ì ): íˆ¬ëª…í•œ ì§€ë°°êµ¬ì¡° ë° ë‚´ë¶€í†µì œ

### 6. ì •ë³´ ì¶©ì¡±ë„ ë° ì¤€ë¹„ ìƒíƒœ (5ì  ë§Œì )
- **ê³µì‹œ ì •ë³´ ì¶©ì¡±ë„** (3ì ): ìƒì¥ì— í•„ìš”í•œ ì •ë³´ ê³µê°œ ìˆ˜ì¤€
- **ìƒì¥ ì¤€ë¹„ë„** (2ì ): ê°ì‚¬ ì²´ê³„, ë‚´ë¶€í†µì œ ë“± ì¤€ë¹„ ì •ë„

## ë‹µë³€ í˜•ì‹ (í•„ìˆ˜)
**ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:**

### 1. ê¸°ë³¸ ìš”ê±´ ì¶©ì¡±ë„ (25ì  ë§Œì )
- ì˜ì—…í™œë™ ê¸°ê°„: Xì  (ê·¼ê±°)
- ë‚©ì…ìë³¸ê¸ˆ: Xì  (ê·¼ê±°)
- ìê¸°ìë³¸: Xì  (ê·¼ê±°)
- ì†Œì•¡ì£¼ì£¼ í™•ë³´: Xì  (ê·¼ê±°)
**ì†Œê³„: X/25ì **

### 2. ê²½ì˜ì„±ê³¼ ë° ì¬ë¬´ ë¶„ì„ (30ì  ë§Œì )
- ë§¤ì¶œ ì„±ì¥ì„±: Xì  (ê·¼ê±°)
- ìˆ˜ìµì„±: Xì  (ê·¼ê±°)
- ì¬ë¬´ ì•ˆì •ì„±: Xì  (ê·¼ê±°)
**ì†Œê³„: X/30ì **

### 3. ìƒì¥ íŠ¸ë™ ì í•©ì„± (20ì  ë§Œì )
- ì´ìµì‹¤í˜„ ìƒì¥ìš”ê±´: Xì  (ê·¼ê±°)
- ì´ìµë¯¸ì‹¤í˜„ ìƒì¥ìš”ê±´: Xì  (ê·¼ê±°)
**ì†Œê³„: X/20ì **

### 4. ê¸°ìˆ ì„±ì¥ê¸°ì—… íŠ¹ë¡€ (10ì  ë§Œì )
- ê¸°ìˆ  í˜ì‹ ì„±: Xì  (ê·¼ê±°)
- ê¸°ìˆ  í‰ê°€ ê°€ëŠ¥ì„±: Xì  (ê·¼ê±°)
**ì†Œê³„: X/10ì **

### 5. ì •ì„±ì  ìš”ì†Œ (10ì  ë§Œì )
- ì‹œì¥ ì„±ì¥ì„±: Xì  (ê·¼ê±°)
- ê²½ìŸ ìš°ìœ„: Xì  (ê·¼ê±°)
- ê²½ì˜ì§„ ì—­ëŸ‰: Xì  (ê·¼ê±°)
- ê±°ë²„ë„ŒìŠ¤: Xì  (ê·¼ê±°)
**ì†Œê³„: X/10ì **

### 6. ì •ë³´ ì¶©ì¡±ë„ ë° ì¤€ë¹„ ìƒíƒœ (5ì  ë§Œì )
- ê³µì‹œ ì •ë³´ ì¶©ì¡±ë„: Xì  (ê·¼ê±°)
- ìƒì¥ ì¤€ë¹„ë„: Xì  (ê·¼ê±°)
**ì†Œê³„: X/5ì **

### ğŸ¯ ì¢…í•© í‰ê°€
**ì´ì : X/100ì **

**ìƒì¥ ê°€ëŠ¥ì„± ë“±ê¸‰:**
- 90-100ì : Aë“±ê¸‰ (ìƒì¥ ê°€ëŠ¥ì„± ë§¤ìš° ë†’ìŒ)
- 80-89ì : Bë“±ê¸‰ (ìƒì¥ ê°€ëŠ¥ì„± ë†’ìŒ)
- 70-79ì : Cë“±ê¸‰ (ìƒì¥ ê°€ëŠ¥ì„± ë³´í†µ)
- 60-69ì : Dë“±ê¸‰ (ìƒì¥ ê°€ëŠ¥ì„± ë‚®ìŒ)
- 60ì  ë¯¸ë§Œ: Fë“±ê¸‰ (ìƒì¥ ê°€ëŠ¥ì„± ë§¤ìš° ë‚®ìŒ)

**ì˜ˆìƒ ìƒì¥ íƒ€ì„ë¼ì¸:** Xê°œì›”~Xë…„ í›„

**í•µì‹¬ ê°œì„  ê³¼ì œ:**
1. ê°€ì¥ ì¤‘ìš”í•œ ê°œì„  ì‚¬í•­
2. ë‘ ë²ˆì§¸ ê°œì„  ì‚¬í•­
3. ì„¸ ë²ˆì§¸ ê°œì„  ì‚¬í•­

ì ìˆ˜ëŠ” í˜„ì¬ ê³µê°œëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì ì´ê³  ë³´ìˆ˜ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”."""

    return PromptTemplate.from_template(template)

def analyze_ipo_possibility(company_data: Dict[str, Any], llm) -> str:
    """ê¸°ì—… ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì½”ìŠ¤ë‹¥ ìƒì¥ ê°€ëŠ¥ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤."""
    
    # News ì œì™¸í•œ ë¶„ì„ ë°ì´í„° ì¶”ì¶œ
    analysis_data = extract_analysis_data(company_data)
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = create_ipo_analysis_prompt()
    
    # ë¶„ì„ ì²´ì¸ êµ¬ì„±
    analysis_chain = prompt | llm | StrOutputParser()
    
    print(f"ğŸ” {analysis_data['company_name']} ì½”ìŠ¤ë‹¥ ìƒì¥ ê°€ëŠ¥ì„± ë¶„ì„ ì¤‘...")
    
    try:
        # ë¶„ì„ ì‹¤í–‰
        result = analysis_chain.invoke({
            "kosdaq_requirements": KOSDAQ_LISTING_REQUIREMENTS,
            "company_data": json.dumps(analysis_data, ensure_ascii=False, indent=2)
        })
        return result
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def save_analysis_result(company_name: str, analysis_result: str, output_dir: str = "analysis_results"):
    """ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{company_name}_ipo_analysis_{timestamp}.txt"
    filepath = os.path.join(output_dir, filename)
    
    cleaned_result = re.sub(r'<think>.*?</think>', '', analysis_result, flags=re.DOTALL).strip()
    
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"# {company_name} ì½”ìŠ¤ë‹¥ ìƒì¥ ê°€ëŠ¥ì„± ë¶„ì„ ë³´ê³ ì„œ\n")
            f.write(f"ë¶„ì„ ì¼ì‹œ: {datetime.datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}\n\n")
            f.write(cleaned_result)
        
        print(f"ğŸ“„ ë¶„ì„ ê²°ê³¼ ì €ì¥: {filepath}")
        return filepath
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return None

def process_single_company(json_file_path: str, llm):
    """ë‹¨ì¼ ê¸°ì—… ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    # ê¸°ì—… ë°ì´í„° ë¡œë“œ
    company_data = load_company_data(json_file_path)
    if not company_data:
        return None
    
    # ìƒì¥ ê°€ëŠ¥ì„± ë¶„ì„
    analysis_result = analyze_ipo_possibility(company_data, llm)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*80)
    print(f"ğŸ“Š {company_data.get('company_name', 'Unknown')} ë¶„ì„ ê²°ê³¼")
    print("="*80)
    print(analysis_result)
    print("="*80 + "\n")
    
    # ê²°ê³¼ ì €ì¥
    company_name = company_data.get('company_name', 'Unknown')
    save_analysis_result(company_name, analysis_result)
    
    return analysis_result

def process_multiple_companies(data_directory: str, llm):
    """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  ê¸°ì—… ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    json_files = glob(os.path.join(data_directory, "*.json"))
    
    if not json_files:
        print(f"âŒ {data_directory}ì—ì„œ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ {len(json_files)}ê°œì˜ ê¸°ì—… ë°ì´í„° íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
    
    results = {}
    for json_file in json_files:
        print(f"\nğŸ¢ ì²˜ë¦¬ ì¤‘: {os.path.basename(json_file)}")
        try:
            result = process_single_company(json_file, llm)
            if result:
                company_data = load_company_data(json_file)
                company_name = company_data.get('company_name', os.path.basename(json_file))
                results[company_name] = result
        except Exception as e:
            print(f"âŒ {json_file} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    print(f"\nâœ… ì´ {len(results)}ê°œ ê¸°ì—… ë¶„ì„ ì™„ë£Œ")
    return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì½”ìŠ¤ë‹¥ IPO ê°€ëŠ¥ì„± ë¶„ì„ê¸°ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # LLM ì´ˆê¸°í™”
    llm = setup_llm()
    
    # ì‚¬ìš©ë²• ì•ˆë‚´
    print("\nğŸ“‹ ì‚¬ìš© ë°©ë²•:")
    print("1. ë‹¨ì¼ íŒŒì¼ ë¶„ì„: python kosdaq_ipo_analyzer.py --file <JSON_íŒŒì¼_ê²½ë¡œ>")
    print("2. ë””ë ‰í† ë¦¬ ì¼ê´„ ë¶„ì„: python kosdaq_ipo_analyzer.py --directory <ë””ë ‰í† ë¦¬_ê²½ë¡œ>")
    print("3. í˜„ì¬ ë””ë ‰í† ë¦¬ í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„: python kosdaq_ipo_analyzer.py --crawling")
    print("4. ë‰´ìŠ¤ í¬ë¡¤ë§ë§Œ ì‹¤í–‰: python kosdaq_ipo_analyzer.py --crawl-news <JSON_íŒŒì¼_ê²½ë¡œ>")
    
    import sys
    
    if len(sys.argv) < 2:
        # ê¸°ë³¸ê°’: í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„
        print("\nğŸ” í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ í¬ë¡¤ë§ ë°ì´í„°ë¥¼ ì°¾ì•„ ë¶„ì„í•©ë‹ˆë‹¤...")
        crawling_files = glob("crawlling_data/company_data_*.json")
        if crawling_files:
            print(f"ğŸ“Š {len(crawling_files)}ê°œì˜ í¬ë¡¤ë§ ë°ì´í„° íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
            for file in crawling_files:
                process_single_company(file, llm)
        else:
            print("âŒ í¬ë¡¤ë§ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ì˜ˆì‹œ íŒŒì¼ë¡œ ì‹­ì¼ë¦¬í„° ë°ì´í„°ë¥¼ ë¶„ì„í•´ë³´ì„¸ìš”:")
            print("python kosdaq_ipo_analyzer.py --file crawlling_data/company_data_ì‹­ì¼ë¦¬í„°_20250630_173418.json")
    
    elif sys.argv[1] == "--file" and len(sys.argv) > 2:
        # ë‹¨ì¼ íŒŒì¼ ë¶„ì„
        file_path = sys.argv[2]
        print(f"\nğŸ“„ ë‹¨ì¼ íŒŒì¼ ë¶„ì„: {file_path}")
        process_single_company(file_path, llm)
    
    elif sys.argv[1] == "--directory" and len(sys.argv) > 2:
        # ë””ë ‰í† ë¦¬ ì¼ê´„ ë¶„ì„
        directory_path = sys.argv[2]
        print(f"\nğŸ“ ë””ë ‰í† ë¦¬ ì¼ê´„ ë¶„ì„: {directory_path}")
        process_multiple_companies(directory_path, llm)
    
    elif sys.argv[1] == "--crawling":
        # í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„
        print("\nğŸ” í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„...")
        process_multiple_companies("crawlling_data", llm)
    
    elif sys.argv[1] == "--crawl-news" and len(sys.argv) > 2:
        # ë‰´ìŠ¤ í¬ë¡¤ë§ë§Œ ì‹¤í–‰
        file_path = sys.argv[2]
        print(f"\nğŸ“° ë‰´ìŠ¤ í¬ë¡¤ë§ë§Œ ì‹¤í–‰: {file_path}")
        company_data = load_company_data(file_path)
        if company_data and "news" in company_data and company_data["news"]:
            crawled_news = crawl_news_articles(company_data["news"])
            print("\n" + "="*80)
            print(f"ğŸ“Š {company_data.get('company_name', 'Unknown')} ìµœì¢… í¬ë¡¤ë§ ê²°ê³¼")
            print("="*80)
            for i, news_item in enumerate(crawled_news):
                print(f"\n--- {i+1}. {news_item.get('title')} ---")
                print(f"URL: {news_item.get('link')}")
                print(f"Crawled Content:\n{news_item.get('crawled_content', 'ë‚´ìš© ì—†ìŒ')[:500]}...")
            print("="*80 + "\n")
        else:
            print("âŒ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    else:
        print("âŒ ì˜ëª»ëœ ì¸ìˆ˜ì…ë‹ˆë‹¤. ì‚¬ìš©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main() 