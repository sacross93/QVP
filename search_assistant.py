#!/usr/bin/env python3
"""
Whoogle ê¸°ë°˜ ê²€ìƒ‰ ì •ë³´ ì •ë¦¬ ë„êµ¬
ë‚´ë¶€ Whoogle ì¸ìŠ¤í„´ìŠ¤(http://192.168.110.102:5000)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ , 
ê·¸ ê²°ê³¼ë¥¼ Local LLMìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.

**ì‚¬ìš©ë²•:**
uv run search_assistant.py "ê²€ìƒ‰ì–´"

**ì‚¬ìš© ì˜ˆì‹œ:**
uv run search_assistant.py "íŒŒì´ì¬ FastAPI"
uv run search_assistant.py "AI ìµœì‹  ë™í–¥" --num-results 15
"""

import sys
import json
import argparse
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
from playwright_stealth.stealth import Stealth
from bs4 import BeautifulSoup
from langchain_ollama import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser


def setup_llm():
    """Local LLM ì„¤ì •"""
    return ChatOllama(
        model="qwen3:32b",
        base_url="http://192.168.120.102:11434",
        temperature=0.3
    )


def search_web(query: str, num_results: int = 1):
    """
    Whoogle ì¸ìŠ¤í„´ìŠ¤ë¥¼ Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰í•˜ê³  ê²°ê³¼ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
    (ê³ ì •ëœ ì£¼ì†Œ: http://192.168.110.102:5000)

    Args:
        query: ê²€ìƒ‰ì–´
        num_results: ê°€ì ¸ì˜¬ ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆ˜

    Returns:
        (list, str): ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì™€ ì„±ê³µí•œ ê²€ìƒ‰ íƒ€ì… ('whoogle')
    """
    whoogle_url = "http://192.168.110.102:5000"
    search_url = f"{whoogle_url}/search?q={query}"
    
    print(f"â„¹ï¸ Playwrightë¡œ Whoogle ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤: {search_url}")

    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            # ìŠ¤í…”ìŠ¤ ëª¨ë“œ ì ìš© (í´ë˜ìŠ¤ ê¸°ë°˜)
            stealth = Stealth()
            stealth.apply_stealth_sync(page)
            
            # í˜ì´ì§€ë¡œ ì´ë™í•˜ê³ , íƒ€ì„ì•„ì›ƒì„ 20ì´ˆë¡œ ì„¤ì •
            page.goto(search_url, timeout=20000, wait_until='domcontentloaded')
            
            # ê²€ìƒ‰ ê²°ê³¼ ì»¨í…Œì´ë„ˆê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°
            page.wait_for_selector('div[class*="ezO2md"]', timeout=10000)
            
            html_content = page.content()
            browser.close()

        soup = BeautifulSoup(html_content, 'html.parser')
        
        results = []
        # ì‹¤ì œ Whoogle HTML êµ¬ì¡°ì— ë§ëŠ” ì„ íƒì ì‚¬ìš©
        result_nodes = soup.select('div[class*="ezO2md"]')

        if not result_nodes:
            print("âš ï¸ Playwrightë¡œ í˜ì´ì§€ë¥¼ ë¡œë“œí–ˆì§€ë§Œ, ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return [], None

        for i, node in enumerate(result_nodes):
            if i >= num_results:
                break
            
            # ì‹¤ì œ Whoogle HTML êµ¬ì¡°ì— ë§ëŠ” íŒŒì‹±
            title_tag = node.find('a', class_='fuLhoc ZWRArf')
            if not title_tag:
                title_tag = node.find('a')
            
            link_tag = title_tag
            snippet_tag = node.find('span', class_='fYyStc')

            title = title_tag.get_text(strip=True) if title_tag else "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"
            link = link_tag['href'] if link_tag and link_tag.has_attr('href') else ""
            
            if link and not link.startswith('http'):
                from urllib.parse import urljoin
                link = urljoin(whoogle_url, link)

            if link and '/url?q=' in link:
                from urllib.parse import unquote, urlparse, parse_qs
                parsed_link = urlparse(link)
                link = parse_qs(parsed_link.query).get('q', [""])[0]

            snippet = snippet_tag.get_text(strip=True) if snippet_tag else "ìš”ì•½ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"

            results.append({'title': title, 'snippet': snippet, 'link': link})
        
        print(f"âœ… Playwrightë¥¼ í†µí•´ Whoogleì—ì„œ {len(results)}ê°œì˜ ê²°ê³¼ íšë“")
        return results, "whoogle"

    except PlaywrightTimeoutError:
        print("âŒ Whoogle í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì´ˆê³¼ (20ì´ˆ). ì„œë²„ê°€ ì‘ë‹µí•˜ì§€ ì•Šê±°ë‚˜, Whoogleì˜ ë´‡ íƒì§€ì— ì˜í•´ ì°¨ë‹¨ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return [], None
    except Exception as e:
        print(f"âŒ Playwright ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return [], None


def create_summary_prompt():
    """ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±"""
    
    prompt_template = """
ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì •ë³´ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²€ìƒ‰ì–´ì— ëŒ€í•œ í¬ê´„ì ì´ê³  ì •í™•í•œ ì •ë³´ë¥¼ í•œêµ­ì–´ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ê²€ìƒ‰ì–´: {query}

ê²€ìƒ‰ ê²°ê³¼:
{search_results}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

# {query}ì— ëŒ€í•œ ì •ë³´ ì •ë¦¬

## ğŸ“‹ ìš”ì•½
(í•µì‹¬ ë‚´ìš©ì„ 2-3ì¤„ë¡œ ìš”ì•½)

## ğŸ” ì£¼ìš” ì •ë³´
(ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë“¤ì„ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ì •ë¦¬)

## ğŸ“Š ì„¸ë¶€ ë‚´ìš©
(êµ¬ì²´ì ì¸ ë°ì´í„°, ì‚¬ì‹¤, ë°°ê²½ ì •ë³´ ë“±ì„ ìì„¸íˆ ì„¤ëª…)

## ğŸ”— ì°¸ê³  ìë£Œ
(ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìœ ìš©í•œ ë§í¬ë“¤ì„ ì œê³µ)

**ì£¼ì˜ì‚¬í•­:**
- ê²€ìƒ‰ ê²°ê³¼ì— ê¸°ë°˜í•œ ì •í™•í•œ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”
- ì¶”ì¸¡ì´ë‚˜ ê°€ì •ì€ í”¼í•˜ê³  ì‚¬ì‹¤ë§Œ ê¸°ìˆ í•˜ì„¸ìš”
- ì¶œì²˜ê°€ ëª…í™•í•œ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”
- í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•˜ì„¸ìš”
"""
    
    return PromptTemplate.from_template(prompt_template)


def format_search_results(results):
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ í¬ë§·íŒ…"""
    formatted_results = []
    
    for i, result in enumerate(results, 1):
        formatted_result = f"""
--- ê²°ê³¼ {i} ---
ì œëª©: {result.get('title', 'N/A')}
ë‚´ìš©: {result.get('snippet', 'N/A')}
ë§í¬: {result.get('link', 'N/A')}
"""
        formatted_results.append(formatted_result)
    
    return "\n".join(formatted_results)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="DuckDuckGo ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLMìœ¼ë¡œ ì •ë¦¬í•´ì£¼ëŠ” ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  uv run search_assistant.py "íŒŒì´ì¬ FastAPI"
  uv run search_assistant.py "AI ë‰´ìŠ¤" --search-type simple --num-results 5
  uv run search_assistant.py "ì½”ë¡œë‚˜ ë°±ì‹ " --region kr-ko --time-range week
  uv run search_assistant.py "ê¸°ìˆ  ë™í–¥" --search-type api --safesearch off

ê²€ìƒ‰ íƒ€ì…:
  detailed: ìƒì„¸í•œ JSON ê²°ê³¼ (ì œëª©, ìš”ì•½, ë§í¬) - ê¸°ë³¸ê°’
  simple: ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ê²°ê³¼
  api: API ë˜í¼ë¥¼ í†µí•œ ê³ ê¸‰ ê²€ìƒ‰

ì§€ì—­ ì„¤ì •:
  wt-wt: ì „ ì„¸ê³„ (ê¸°ë³¸ê°’)
  us-en: ë¯¸êµ­ (ì˜ì–´)
  kr-ko: í•œêµ­ (í•œêµ­ì–´)
  jp-jp: ì¼ë³¸ (ì¼ë³¸ì–´)
  gb-en: ì˜êµ­ (ì˜ì–´)

ì‹œê°„ ë²”ìœ„:
  day: ìµœê·¼ í•˜ë£¨
  week: ìµœê·¼ ì¼ì£¼ì¼
  month: ìµœê·¼ í•œ ë‹¬
  year: ìµœê·¼ ì¼ë…„
  (ë¹ˆ ê°’): ëª¨ë“  ì‹œê°„ - ê¸°ë³¸ê°’
        """
    )
    
    parser.add_argument("query", help="ê²€ìƒ‰í•  í‚¤ì›Œë“œ")
    parser.add_argument("--num-results", type=int, default=1, 
                       help="ê°€ì ¸ì˜¬ ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆ˜ (ê¸°ë³¸ê°’: 1)")
    
    args = parser.parse_args()
    
    if not args.query:
        print("âŒ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        print("ì‚¬ìš©ë²•: uv run search_assistant.py \"ê²€ìƒ‰ì–´\"")
        return
    
    # ê²€ìƒ‰ ì„¤ì • ì •ë³´ ì¶œë ¥
    print(f"ğŸ” '{args.query}' ê²€ìƒ‰ ì¤‘...")
    print(f"ğŸ“Š ê²€ìƒ‰ ì„¤ì •:")
    print(f"   - Whoogle URL: http://192.168.110.102:5000")
    print(f"   - ê²°ê³¼ ê°œìˆ˜: {args.num_results}")
    
    # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
    search_results, successful_type = search_web(
        args.query,
        args.num_results
    )
    
    if not search_results:
        print("âŒ ìµœì¢…ì ìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ í•´ê²° ë°©ë²•:")
        print("   1. Whoogle ì»¨í…Œì´ë„ˆê°€ http://192.168.110.102:5000 ì—ì„œ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
        print("   2. ê²€ìƒ‰ì–´ë¥¼ ë³€ê²½í•´ë³´ì„¸ìš”.")
        print("   3. `playwright install --with-deps`ê°€ ì œëŒ€ë¡œ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    if successful_type != "whoogle":
        print(f"\nâ„¹ï¸ ì°¸ê³ : ìš”ì²­í•˜ì‹  'whoogle' íƒ€ì… ê²€ìƒ‰ì— ì‹¤íŒ¨í•˜ì—¬ '{successful_type}' íƒ€ì…ìœ¼ë¡œ ê²°ê³¼ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")

    print(f"\nâœ… {len(search_results)}ê°œì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLM ì •ë¦¬ ì‹œì‘...")
    
    # LLM ì„¤ì •
    try:
        llm = setup_llm()
    except Exception as e:
        print(f"âŒ LLM ì—°ê²° ì˜¤ë¥˜: {e}")
        print("Local LLM ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬ (ì„±ê³µí•œ íƒ€ì… ê¸°ì¤€ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±)
    formatted_results = format_search_results(search_results)
    prompt = create_summary_prompt()
    
    chain = prompt | llm | StrOutputParser()
    
    try:
        summary = chain.invoke({
            "query": args.query,
            "search_results": formatted_results
        })
        
        print("\n" + "=" * 50)
        print("ğŸ“ ì •ë¦¬ëœ ì •ë³´")
        print("=" * 50)
        print(summary)
        
    except Exception as e:
        print(f"âŒ ì •ë³´ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("\nğŸ“‹ ì›ì‹œ ê²€ìƒ‰ ê²°ê³¼:")
        for i, result in enumerate(search_results, 1):
            print(f"\n--- ê²°ê³¼ {i} ---")
            print(f"ì œëª©: {result.get('title', 'N/A')}")
            print(f"ë‚´ìš©: {result.get('snippet', 'N/A')}")
            print(f"ë§í¬: {result.get('link', 'N/A')}")


if __name__ == "__main__":
    main() 