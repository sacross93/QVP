#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
IR ìë£Œ PDF íŒŒì‹± ë° ìš”ì•½ ì‹œìŠ¤í…œ
- PyMuPDFë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
- ì„¹ì…˜ë³„ ë¶„ë¥˜ ë° êµ¬ì¡°í™”
- í‘œ/ì°¨íŠ¸ ë°ì´í„° ì²˜ë¦¬
- ë¡œì»¬ LLM(ChatOllama)ì„ ì‚¬ìš©í•œ ìë™ ìš”ì•½
"""

import fitz  # PyMuPDF
import pandas as pd
import json
import re
from typing import Dict, List, Tuple, Any
from pathlib import Path
import logging
from datetime import datetime
import argparse

# ë¡œì»¬ LLM ì—°ë™
try:
    from langchain_ollama import ChatOllama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    print("âš ï¸ langchain_ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install langchain-ollamaë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")

# LLM ì„¤ì •
MODEL_ID = "qwen3:32b"
BASE_URL = "http://192.168.120.102:11434"

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class IRPDFParser:
    """IR ìë£Œ PDF íŒŒì‹± ë° ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, use_llm: bool = True):
        self.use_llm = use_llm and OLLAMA_AVAILABLE
        
        if self.use_llm:
            try:
                self.llm = ChatOllama(model=MODEL_ID, base_url=BASE_URL, temperature=0)
                logger.info(f"ë¡œì»¬ LLM ì´ˆê¸°í™” ì™„ë£Œ: {MODEL_ID}")
            except Exception as e:
                logger.warning(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}. í”„ë¡¬í”„íŠ¸ë§Œ ìƒì„±í•©ë‹ˆë‹¤.")
                self.use_llm = False
        
        self.section_keywords = {
            'company_overview': [
                'íšŒì‚¬ê°œìš”', 'ê¸°ì—…ê°œìš”', 'íšŒì‚¬ì†Œê°œ', 'company overview', 
                'ì‚¬ì—…ì˜ì—­', 'ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸', 'íšŒì‚¬ì—°í˜', 'ê¸°ì—…í˜„í™©'
            ],
            'financial_data': [
                'ì¬ë¬´ì œí‘œ', 'ì¬ë¬´í˜„í™©', 'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'ìˆœì´ìµ', 
                'revenue', 'profit', 'ì†ìµê³„ì‚°ì„œ', 'ì¬ë¬´ìƒíƒœí‘œ', 
                'í˜„ê¸ˆíë¦„í‘œ', 'ì¬ë¬´ì§€í‘œ', 'ebitda', 'roe', 'roa'
            ],
            'market_analysis': [
                'ì‹œì¥ë¶„ì„', 'ì‹œì¥í˜„í™©', 'ê²½ìŸì‚¬', 'ì‹œì¥ê·œëª¨', 'íƒ€ê²Ÿì‹œì¥',
                'market size', 'tam', 'sam', 'ì—…ê³„ë™í–¥', 'ì‹œì¥ì „ë§',
                'ê²½ìŸêµ¬ë„', 'ë§ˆì¼“ì…°ì–´'
            ],
            'technology': [
                'ê¸°ìˆ ', 'í•µì‹¬ê¸°ìˆ ', 'íŠ¹í—ˆ', 'r&d', 'ì—°êµ¬ê°œë°œ',
                'ê¸°ìˆ ë ¥', 'ì°¨ë³„í™”', 'í˜ì‹ ', 'í”Œë«í¼', 'ì†”ë£¨ì…˜',
                'ì•Œê³ ë¦¬ì¦˜', 'ê°œë°œí˜„í™©'
            ],
            'organization': [
                'ì¡°ì§', 'ì¸ë ¥', 'ê²½ì˜ì§„', 'ì„ì§ì›', 'ì¡°ì§ë„', 
                'íŒ€êµ¬ì„±', 'ì±„ìš©', 'ì¸ì‚¬', 'ê²½ì˜íŒ€', 'ceo', 'cto',
                'ëŒ€í‘œ', 'ëŒ€í‘œì´ì‚¬', 'founder', 'ì†Œì¥', 'ë¶€íšŒì¥',
                'ë°•ì‚¬', 'ì„ì‚¬', 'í•™ì‚¬', 'ê²½ë ¥', 'ì „ë¬¸ê°€', 'ì—°êµ¬ì›'
            ],
            'business_model': [
                'ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸', 'ìˆ˜ìµëª¨ë¸', 'ì‚¬ì—…êµ¬ì¡°', 'ì„œë¹„ìŠ¤',
                'ì œí’ˆ', 'ê³ ê°', 'íŒŒíŠ¸ë„ˆì‹­', 'í˜‘ë ¥ì‚¬'
            ],
            'investment': [
                'íˆ¬ì', 'ìê¸ˆì¡°ë‹¬', 'íˆ¬ììœ ì¹˜', 'ë°¸ë¥˜ì—ì´ì…˜',
                'íˆ¬ìê³„íš', 'ìê¸ˆê³„íš', 'ì¦ì', 'í€ë”©'
            ]
        }
    
    def extract_text_from_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """PDFì—ì„œ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        logger.info(f"PDF íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
        
        try:
            doc = fitz.open(pdf_path)
            pages_data = []
            
            for page_num in range(doc.page_count):
                page = doc[page_num]
                
                # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                raw_text = page.get_text()
                
                # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ
                text_dict = page.get_text("dict")
                
                # ì´ë¯¸ì§€ ì •ë³´
                images = page.get_images()
                
                # í˜ì´ì§€ ì •ë³´ ì €ì¥
                page_data = {
                    'page_number': page_num + 1,
                    'raw_text': raw_text,
                    'structured_text': text_dict,
                    'image_count': len(images),
                    'word_count': len(raw_text.split())
                }
                
                pages_data.append(page_data)
                logger.info(f"í˜ì´ì§€ {page_num + 1} ì²˜ë¦¬ ì™„ë£Œ - ë‹¨ì–´ ìˆ˜: {len(raw_text.split())}")
            
            doc.close()
            
            return {
                'total_pages': len(pages_data),
                'pages': pages_data,
                'extraction_time': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise
    
    def classify_content_by_section(self, pages_data: List[Dict]) -> Dict[str, List[Dict]]:
        """ì„¹ì…˜ë³„ ì»¨í…ì¸  ë¶„ë¥˜"""
        logger.info("ì„¹ì…˜ë³„ ì»¨í…ì¸  ë¶„ë¥˜ ì‹œì‘")
        
        classified_sections = {section: [] for section in self.section_keywords.keys()}
        classified_sections['uncategorized'] = []
        
        for page_data in pages_data:
            page_text = page_data['raw_text'].lower()
            page_classified = False
            
            # ê° ì„¹ì…˜ë³„ í‚¤ì›Œë“œ ë§¤ì¹­
            for section, keywords in self.section_keywords.items():
                keyword_matches = sum(1 for keyword in keywords if keyword in page_text)
                
                # í‚¤ì›Œë“œê°€ 2ê°œ ì´ìƒ ë§¤ì¹­ë˜ê±°ë‚˜, ê°•í•œ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¥˜
                if keyword_matches >= 2 or any(strong_keyword in page_text for strong_keyword in keywords[:3]):
                    classified_sections[section].append({
                        'page_number': page_data['page_number'],
                        'content': page_data['raw_text'],
                        'keyword_matches': keyword_matches,
                        'word_count': page_data['word_count']
                    })
                    page_classified = True
                    logger.info(f"í˜ì´ì§€ {page_data['page_number']}: {section} ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¥˜ (í‚¤ì›Œë“œ ë§¤ì¹­: {keyword_matches})")
                    break
            
            # ë¶„ë¥˜ë˜ì§€ ì•Šì€ í˜ì´ì§€
            if not page_classified:
                classified_sections['uncategorized'].append({
                    'page_number': page_data['page_number'],
                    'content': page_data['raw_text'],
                    'word_count': page_data['word_count']
                })
        
        # ì„¹ì…˜ë³„ í†µê³„
        for section, content in classified_sections.items():
            if content:
                total_words = sum(item['word_count'] for item in content)
                logger.info(f"{section} ì„¹ì…˜: {len(content)}í˜ì´ì§€, ì´ {total_words}ë‹¨ì–´")
        
        return classified_sections
    
    def extract_financial_tables(self, pdf_path: str) -> List[pd.DataFrame]:
        """ì¬ë¬´ì œí‘œ ë° í‘œ ë°ì´í„° ì¶”ì¶œ"""
        logger.info("í‘œ ë°ì´í„° ì¶”ì¶œ ì‹œì‘")
        
        tables = []
        
        try:
            # camelotì„ ì‚¬ìš©í•œ í‘œ ì¶”ì¶œ
            import camelot
            camelot_tables = camelot.read_pdf(pdf_path, pages='all', flavor='lattice')
            
            for i, table in enumerate(camelot_tables):
                if len(table.df.columns) > 1 and len(table.df) > 1:  # ìœ íš¨í•œ í‘œë§Œ
                    tables.append({
                        'table_id': i + 1,
                        'page': table.page,
                        'dataframe': table.df,
                        'accuracy': table.accuracy if hasattr(table, 'accuracy') else 0.0
                    })
                    logger.info(f"í‘œ {i+1} ì¶”ì¶œ ì™„ë£Œ (í˜ì´ì§€ {table.page}): {table.df.shape}")
            
        except ImportError:
            logger.warning("camelot ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ í‘œ ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤")
        except Exception as e:
            logger.warning(f"í‘œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        return tables
    
    def extract_key_metrics(self, classified_content: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """í•µì‹¬ ì§€í‘œ ì¶”ì¶œ"""
        logger.info("í•µì‹¬ ì§€í‘œ ì¶”ì¶œ ì‹œì‘")
        
        metrics = {
            'financial_metrics': {},
            'business_metrics': {},
            'market_metrics': {},
            'technology_metrics': {}
        }
        
        # ì¬ë¬´ ì§€í‘œ ì¶”ì¶œ
        if 'financial_data' in classified_content:
            financial_text = ' '.join([item['content'] for item in classified_content['financial_data']])
            
            # ìˆ«ì íŒ¨í„´ ë§¤ì¹­
            revenue_pattern = r'ë§¤ì¶œ.*?(\d{1,3}(?:,\d{3})*(?:\.\d+)?)\s*(?:ì–µ|ë§Œ|ì›|ë°±ë§Œ)'
            profit_pattern = r'(?:ì˜ì—…|ìˆœ)ì´ìµ.*?(\d{1,3}(?:,\d{3})*(?:\.\d+)?)\s*(?:ì–µ|ë§Œ|ì›|ë°±ë§Œ)'
            
            revenue_matches = re.findall(revenue_pattern, financial_text)
            profit_matches = re.findall(profit_pattern, financial_text)
            
            if revenue_matches:
                metrics['financial_metrics']['revenue'] = revenue_matches
            if profit_matches:
                metrics['financial_metrics']['profit'] = profit_matches
        
        # ì‹œì¥ ì§€í‘œ ì¶”ì¶œ
        if 'market_analysis' in classified_content:
            market_text = ' '.join([item['content'] for item in classified_content['market_analysis']])
            market_size_pattern = r'ì‹œì¥.*?ê·œëª¨.*?(\d{1,3}(?:,\d{3})*(?:\.\d+)?)\s*(?:ì–µ|ì¡°|ë§Œ|ì›|ë‹¬ëŸ¬)'
            market_matches = re.findall(market_size_pattern, market_text)
            
            if market_matches:
                metrics['market_metrics']['market_size'] = market_matches
        
        return metrics
    
    def split_pdf_into_smart_chunks(self, pdf_data: Dict[str, Any], max_chunk_size: int = 3000) -> List[Dict[str, Any]]:
        """PDFë¥¼ ì˜ë¯¸ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì²­í¬ë¡œ ë¶„í• """
        logger.info("PDF ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í•  ì‹œì‘")
        
        # ì¡°ì§/íŒ€ ê´€ë ¨ í‚¤ì›Œë“œ ì •ì˜
        org_keywords = ['íŒ€', 'ì¡°ì§', 'ê²½ì˜ì§„', 'ë©¤ë²„', 'êµ¬ì„±ì›', 'CEO', 'CTO', 'COO', 'ëŒ€í‘œ', 'ì´ì‚¬', 'ì†Œì¥', 
                       'ì—°êµ¬ì›', 'ê°œë°œì', 'ë§ˆì¼€í„°', 'MD', 'í•™ë ¥', 'ê²½ë ¥', 'ëŒ€í•™êµ', 'ì„ì‚¬', 'ë°•ì‚¬', 'ì „ë¬¸í•™ì‚¬']
        
        # ì¬ë¬´ ê´€ë ¨ í‚¤ì›Œë“œ ì •ì˜  
        finance_keywords = ['ë§¤ì¶œ', 'íˆ¬ì', 'ìê¸ˆ', 'ì¡°ë‹¬', 'ì–µ', 'ì›', 'ìˆ˜ìµ', 'ì´ìµ', 'ì†ì‹¤', 'ì¬ë¬´', 'ê³„íš', 
                           'ê¸°ì—…ê°€ì¹˜', 'íˆ¬ììœ ì¹˜', 'ë¼ìš´ë“œ', 'Pre-A', 'Series']
        
        # ê¸°ìˆ  ê´€ë ¨ í‚¤ì›Œë“œ ì •ì˜
        tech_keywords = ['íŠ¹í—ˆ', 'ê¸°ìˆ ', 'R&D', 'ì—°êµ¬', 'ê°œë°œ', 'í˜ì‹ ', 'AI', 'ì†Œí”„íŠ¸ì›¨ì–´', 'í•˜ë“œì›¨ì–´', 
                        'ì•Œê³ ë¦¬ì¦˜', 'ëª¨ë¸', 'ì‹œìŠ¤í…œ', 'í”Œë«í¼']
        
        chunks = []
        current_chunk = {"type": "general", "pages": [], "text": "", "keywords_found": set()}
        
        for page in pdf_data['pages']:
            page_text = page['raw_text']
            page_lower = page_text.lower()
            
            # í˜ì´ì§€ íƒ€ì… ì‹ë³„
            org_score = sum(1 for keyword in org_keywords if keyword.lower() in page_lower)
            finance_score = sum(1 for keyword in finance_keywords if keyword.lower() in page_lower)
            tech_score = sum(1 for keyword in tech_keywords if keyword.lower() in page_lower)
            
            # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ íƒ€ì… ê²°ì •
            page_type = "general"
            max_score = max(org_score, finance_score, tech_score)
            if max_score >= 3:  # ì„ê³„ê°’ ì„¤ì •
                if org_score == max_score:
                    page_type = "organization"
                elif finance_score == max_score:
                    page_type = "finance"
                elif tech_score == max_score:
                    page_type = "technology"
            
            # í˜„ì¬ ì²­í¬ì™€ íƒ€ì…ì´ ë‹¤ë¥´ê±°ë‚˜ í¬ê¸° ì œí•œ ì´ˆê³¼ì‹œ ìƒˆ ì²­í¬ ì‹œì‘
            if (current_chunk["type"] != page_type or 
                len(current_chunk["text"]) + len(page_text) > max_chunk_size):
                
                if current_chunk["text"]:  # ë¹ˆ ì²­í¬ê°€ ì•„ë‹ˆë©´ ì €ì¥
                    chunks.append(current_chunk)
                
                current_chunk = {
                    "type": page_type, 
                    "pages": [page['page_number']], 
                    "text": f"=== í˜ì´ì§€ {page['page_number']} ===\n{page_text}",
                    "keywords_found": set()
                }
            else:
                current_chunk["pages"].append(page['page_number'])
                current_chunk["text"] += f"\n=== í˜ì´ì§€ {page['page_number']} ===\n{page_text}"
            
            # í‚¤ì›Œë“œ ì¶”ê°€
            if org_score > 0:
                current_chunk["keywords_found"].update([kw for kw in org_keywords if kw.lower() in page_lower])
            if finance_score > 0:
                current_chunk["keywords_found"].update([kw for kw in finance_keywords if kw.lower() in page_lower])
            if tech_score > 0:
                current_chunk["keywords_found"].update([kw for kw in tech_keywords if kw.lower() in page_lower])
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk["text"]:
            chunks.append(current_chunk)
        
        logger.info(f"PDFë¥¼ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")
        for i, chunk in enumerate(chunks):
            logger.info(f"ì²­í¬ {i+1}: {chunk['type']} (í˜ì´ì§€ {chunk['pages']}, ê¸¸ì´: {len(chunk['text'])})")
        
        return chunks

    def extract_organization_info(self, chunk_text: str) -> str:
        """ì¡°ì§ ì •ë³´ ì „ìš© ì¶”ì¶œ"""
        if not self.use_llm:
            return "LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì¡°ì§ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        org_prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì¡°ì§ êµ¬ì„±ì› ì •ë³´ë¥¼ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

{chunk_text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê° êµ¬ì„±ì›ë³„ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
## ì¡°ì§ êµ¬ì„±ì›

### [ì´ë¦„]
- **ì§ì±…**: [ì§ì±…ëª…]
- **í•™ë ¥**: [í•™êµëª…, í•™ìœ„]
- **ê²½ë ¥**: [ì£¼ìš” ê²½ë ¥ì‚¬í•­]
- **ê¸°íƒ€**: [ì¶”ê°€ ì •ë³´]

ì œì•½ì‚¬í•­:
1. ë¬¸ì„œì— ëª…ì‹œëœ ì •ë³´ë§Œ ì¶”ì¶œ
2. ì—†ëŠ” ì •ë³´ëŠ” "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ í‘œê¸°
3. ê°€ìƒ ì¸ë¬¼/ì •ë³´ ìƒì„± ì ˆëŒ€ ê¸ˆì§€
4. ëª¨ë“  êµ¬ì„±ì› ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨
5. ì´ë¦„, ì§ì±…, í•™ë ¥, ê²½ë ¥ì´ ìˆëŠ” ëª¨ë“  ì¸ë¬¼ ì¶”ì¶œ

/no_think
"""
        
        try:
            response = self.llm.invoke(org_prompt)
            result = response.content if hasattr(response, 'content') else str(response)
            logger.info("ì¡°ì§ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")
            return result.strip()
        except Exception as e:
            logger.error(f"ì¡°ì§ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return f"ì¡°ì§ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"

    def extract_finance_info(self, chunk_text: str) -> str:
        """ì¬ë¬´ ì •ë³´ ì „ìš© ì¶”ì¶œ"""
        if not self.use_llm:
            return "LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì¬ë¬´ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        finance_prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì¬ë¬´ ì •ë³´ë¥¼ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

{chunk_text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¬ë¬´ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
## ì¬ë¬´ í˜„í™©

### ë§¤ì¶œ í˜„í™©
- **ì—°ë„ë³„ ë§¤ì¶œ**: [êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ ì—°ë„]
- **ì˜ˆìƒ ë§¤ì¶œ**: [í–¥í›„ ê³„íš]
- **ë§¤ì¶œ êµ¬ì„±**: [ë§¤ì¶œì›ë³„ ë¶„ì„]

### íˆ¬ì í˜„í™©  
- **íˆ¬ì ë¼ìš´ë“œ**: [Pre-A, Series A ë“±]
- **íˆ¬ì ê¸ˆì•¡**: [êµ¬ì²´ì  ê¸ˆì•¡]
- **ê¸°ì—…ê°€ì¹˜**: [valuation ì •ë³´]
- **íˆ¬ìê¸ˆ ì‚¬ìš© ê³„íš**: [ì‚¬ìš© ìš©ë„]

### ìˆ˜ìµì„±
- **ì˜ì—…ì´ìµ**: [ìˆ˜ì¹˜]
- **ë‹¹ê¸°ìˆœì´ìµ**: [ìˆ˜ì¹˜]
- **ì†ìµ ì „ë§**: [í–¥í›„ ê³„íš]

ì œì•½ì‚¬í•­:
1. ë¬¸ì„œì— ëª…ì‹œëœ ìˆ«ìë§Œ ì‚¬ìš©
2. ì¶”ì •/ì¶”ë¡  ê¸ˆì§€, íŒ©íŠ¸ë§Œ ì¶”ì¶œ
3. ì–µ/ë§Œì› ë‹¨ìœ„ ì •í™•íˆ í‘œê¸°

/no_think
"""
        
        try:
            response = self.llm.invoke(finance_prompt)
            result = response.content if hasattr(response, 'content') else str(response)
            logger.info("ì¬ë¬´ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")
            return result.strip()
        except Exception as e:
            logger.error(f"ì¬ë¬´ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return f"ì¬ë¬´ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"

    def extract_technology_info(self, chunk_text: str) -> str:
        """ê¸°ìˆ  ì •ë³´ ì „ìš© ì¶”ì¶œ"""
        if not self.use_llm:
            return "LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê¸°ìˆ  ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        tech_prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìˆ  ì •ë³´ë¥¼ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

{chunk_text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê¸°ìˆ  ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
## ê¸°ìˆ  í˜„í™©

### íŠ¹í—ˆ í˜„í™©
- **íŠ¹í—ˆëª…**: [íŠ¹í—ˆ ì œëª©]
- **íŠ¹í—ˆ ë²ˆí˜¸**: [ë²ˆí˜¸]
- **ì¶œì›ì¼**: [ë‚ ì§œ]
- **ìƒíƒœ**: [ë“±ë¡/ì¶œì› ìƒíƒœ]

### í•µì‹¬ ê¸°ìˆ 
- **ê¸°ìˆ ëª…**: [ê¸°ìˆ  ì´ë¦„]
- **ê¸°ìˆ  ì„¤ëª…**: [ê°„ë‹¨í•œ ì„¤ëª…]
- **ê¸°ìˆ  ìˆ˜ì¤€**: [ì„±ëŠ¥ ì§€í‘œ]

### R&D í˜„í™©
- **ì—°êµ¬ê°œë°œ ì¸ë ¥**: [ì¸ì›ìˆ˜]
- **ì—°êµ¬ ì„±ê³¼**: [ì£¼ìš” ì„±ê³¼]
- **ê°œë°œ ê³„íš**: [í–¥í›„ ê³„íš]

ì œì•½ì‚¬í•­:
1. ë¬¸ì„œì— ëª…ì‹œëœ ì •ë³´ë§Œ ì¶”ì¶œ
2. íŠ¹í—ˆë²ˆí˜¸, ì¶œì›ì¼ ë“± ì •í™•í•œ ì •ë³´ë§Œ ê¸°ì¬
3. ê°€ìƒ ê¸°ìˆ /íŠ¹í—ˆ ìƒì„± ê¸ˆì§€

/no_think
"""
        
        try:
            response = self.llm.invoke(tech_prompt)
            result = response.content if hasattr(response, 'content') else str(response)
            logger.info("ê¸°ìˆ  ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")
            return result.strip()
        except Exception as e:
            logger.error(f"ê¸°ìˆ  ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return f"ê¸°ìˆ  ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"

    def generate_unified_template_prompt(self, pdf_data: Dict[str, Any]) -> str:
        """í†µí•© í…œí”Œë¦¿ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„± (ê¸°ì¡´ ë°©ì‹)"""
        logger.info("í†µí•© í…œí”Œë¦¿ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œì‘")
        
        # ëª¨ë“  í˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        all_text = ""
        for page in pdf_data['pages']:
            all_text += f"\n=== í˜ì´ì§€ {page['page_number']} ===\n"
            all_text += page['raw_text']
        
        template_prompt = f"""
ë‹¤ìŒ í…œí”Œë¦¿ì˜ [ ] ë¹ˆì¹¸ì„ PDF ë‚´ìš©ì—ì„œ ì°¾ì•„ ì •í™•íˆ ì±„ì›Œì£¼ì„¸ìš”:

## íšŒì‚¬ ê°œìš”
- íšŒì‚¬ëª…: [ ]
- ì„¤ë¦½ì¼: [ ]
- ëŒ€í‘œì/CEO: [ ]
- ì‚¬ì—…ë¶„ì•¼: [ ]
- ë³¸ì‚¬ ìœ„ì¹˜: [ ]

## ê²½ì˜ì§„ êµ¬ì„±
- CEO: [ì´ë¦„] (í•™ë ¥: [ ], ê²½ë ¥: [ ])
- CTO: [ì´ë¦„] (í•™ë ¥: [ ], ê²½ë ¥: [ ])
- COO: [ì´ë¦„] (í•™ë ¥: [ ], ê²½ë ¥: [ ])
- ê¸°íƒ€ ì£¼ìš” ì„ì›: [ ]

## ì¬ë¬´ í˜„í™©
- ìµœê·¼ ë§¤ì¶œ (ì—°ë„ë³„): [ ]
- ì˜ì—…ì´ìµ: [ ]
- ìˆœì´ìµ: [ ]
- íˆ¬ì ìœ ì¹˜ í˜„í™©: [ ]
- ìê¸ˆ ì¡°ë‹¬ ê³„íš: [ ]

## ê¸°ìˆ  í˜„í™©
- í•µì‹¬ ê¸°ìˆ : [ ]
- íŠ¹í—ˆ ë³´ìœ  í˜„í™©: [ ]
- R&D íˆ¬ì: [ ]
- ê¸°ìˆ ì  ê²½ìŸë ¥: [ ]

## ì‹œì¥ ë¶„ì„
- íƒ€ê²Ÿ ì‹œì¥ ê·œëª¨: [ ]
- ì£¼ìš” ê²½ìŸì‚¬: [ ]
- ì‹œì¥ ì„±ì¥ë¥ : [ ]
- ì‹œì¥ ê¸°íšŒ/ë¦¬ìŠ¤í¬: [ ]

## ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸
- ì£¼ìš” ìˆ˜ìµì›: [ ]
- ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸: [ ]
- ê³ ê°êµ°: [ ]

PDF ì „ì²´ ë‚´ìš©:
{all_text}

ì œì•½ì‚¬í•­:
- ë¬¸ì„œì— ëª…ì‹œëœ ì •ë³´ë§Œ ì •í™•íˆ ì¶”ì¶œ
- ì—†ëŠ” ì •ë³´ëŠ” "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ í‘œì‹œ
- ê°€ìƒ ì •ë³´/ì¸ë¬¼/ìˆ«ì ìƒì„± ì ˆëŒ€ ê¸ˆì§€
- ì¶”ë¡ /ì¶”ì •/ê°€ì • ê¸ˆì§€

/no_think
"""
        return template_prompt
    
    def generate_unified_summary(self, pdf_data: Dict[str, Any]) -> str:
        """í†µí•© í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ì¼ ìš”ì•½ ìƒì„±"""
        if not self.use_llm:
            logger.warning("LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í…œí”Œë¦¿ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return self.generate_unified_template_prompt(pdf_data)
        
        logger.info("í†µí•© í…œí”Œë¦¿ì„ ì‚¬ìš©í•œ ìš”ì•½ ìƒì„± ì‹œì‘")
        
        try:
            # í†µí•© í…œí”Œë¦¿ í”„ë¡¬í”„íŠ¸ ìƒì„±
            template_prompt = self.generate_unified_template_prompt(pdf_data)
            
            # LLMì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬
            response = self.llm.invoke(template_prompt)
            summary = response.content if hasattr(response, 'content') else str(response)
            
            logger.info("í†µí•© ìš”ì•½ ìƒì„± ì™„ë£Œ")
            return summary.strip()
            
        except Exception as e:
            logger.error(f"í†µí•© ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}"
    
    def generate_summary_prompts(self, classified_content: Dict[str, List[Dict]], 
                                key_metrics: Dict[str, Any]) -> Dict[str, str]:
        """ë¡œì»¬ LLMìš© ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        logger.info("ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œì‘")
        
        prompts = {}
        
        section_prompts = {
            'company_overview': """
ë¬¸ì„œ ë‚´ìš©ì„ ë‹¤ìŒê³¼ ê°™ì´ ê°„ê²°í•˜ê²Œ ì¬êµ¬ì„±í•˜ì„¸ìš”:

1. íšŒì‚¬ëª…, ì„¤ë¦½ì¼, ëŒ€í‘œì
2. ì£¼ìš” ì‚¬ì—… ë¶„ì•¼ ë° ì œí’ˆ/ì„œë¹„ìŠ¤  
3. í•µì‹¬ ì„±ê³¼ ë° í˜„í™©

ë‚´ìš©:
{content}

ì œì•½ì‚¬í•­: ë¬¸ì„œì— ëª…ì‹œëœ íŒ©íŠ¸ë§Œ ë‚˜ì—´, ë¶„ì„/í•´ì„ ê¸ˆì§€, ê° í•­ëª© 1-2ë¬¸ì¥ ì´ë‚´, ê°€ìƒ ì •ë³´ ìƒì„± ì ˆëŒ€ ê¸ˆì§€

/no_think
""",
            
            'financial_data': """
ì¬ë¬´ ì •ë³´ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”:

1. ë§¤ì¶œ í˜„í™© (ìµœê·¼ ì—°ë„ë³„ ë§¤ì¶œ)
2. ì†ìµ í˜„í™© (ì˜ì—…ì´ìµ, ìˆœì´ìµ)  
3. ìê¸ˆ ì¡°ë‹¬ ë‚´ì—­ (íˆ¬ì ë¼ìš´ë“œ, ê¸ˆì•¡)

ë‚´ìš©:
{content}
í•µì‹¬ ì§€í‘œ: {metrics}

ì œì•½ì‚¬í•­: ë¬¸ì„œì˜ ìˆ«ìë§Œ ì‚¬ìš©, íˆ¬ìì •ë³´ ì¶”ì¸¡ ê¸ˆì§€, ê° í•­ëª© ê°„ê²°í•˜ê²Œ, ê°€ìƒ ìˆ«ì/íˆ¬ìì •ë³´ ìƒì„± ì ˆëŒ€ ê¸ˆì§€

/no_think
""",
            
            'market_analysis': """
ì‹œì¥ ì •ë³´ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”:

1. ì‹œì¥ ê·œëª¨ ë° ì„±ì¥ë¥ 
2. ì£¼ìš” ê²½ìŸì‚¬ ë° ê²½ìŸ êµ¬ë„
3. ì‹œì¥ ê¸°íšŒ ë° ë¦¬ìŠ¤í¬

ë‚´ìš©:
{content}

ì œì•½ì‚¬í•­: ë¬¸ì„œì˜ íŒ©íŠ¸ë§Œ ë‚˜ì—´, ê° í•­ëª© 1-2ë¬¸ì¥ ì´ë‚´, ê°€ìƒ ì‹œì¥ì •ë³´/ê²½ìŸì‚¬ ìƒì„± ì ˆëŒ€ ê¸ˆì§€

/no_think
""",
            
            'technology': """
ê¸°ìˆ  ì •ë³´ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”:

1. í•µì‹¬ ê¸°ìˆ  ë° íŠ¹í—ˆ í˜„í™©
2. R&D íˆ¬ì ë° ê°œë°œ ì„±ê³¼
3. ê¸°ìˆ ì  ê²½ìŸë ¥ ë° ì°¨ë³„í™” ìš”ì†Œ

ë‚´ìš©:
{content}

ì œì•½ì‚¬í•­: ë¬¸ì„œì˜ ê¸°ìˆ  íŒ©íŠ¸ë§Œ ë‚˜ì—´, ê° í•­ëª© 1-2ë¬¸ì¥ ì´ë‚´, ê°€ìƒ ê¸°ìˆ ì •ë³´/íŠ¹í—ˆ ìƒì„± ì ˆëŒ€ ê¸ˆì§€

/no_think
""",
            
            'organization': """
ì¡°ì§ ì •ë³´ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”:

1. í•µì‹¬ ê²½ì˜ì§„ (ì´ë¦„, ì§ì±…, í•™ë ¥, ê²½ë ¥)
2. ì¡°ì§ ê·œëª¨ ë° êµ¬ì„±
3. ì£¼ìš” ì—­ëŸ‰ ë° ì „ë¬¸ì„±

ë‚´ìš©:
{content}

ì œì•½ì‚¬í•­: ë¬¸ì„œì— ëª…ì‹œëœ ì¸ëª…/ì§ì±…/í•™ë ¥/ê²½ë ¥ë§Œ ì¶”ì¶œí•˜ì—¬ ë‚˜ì—´, ì—†ëŠ” ì •ë³´ëŠ” ì°½ì‘í•˜ì§€ ë§ê³  "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ í‘œê¸°

/no_think
"""
        }
        
        for section, prompt_template in section_prompts.items():
            if section in classified_content and classified_content[section]:
                content = '\n'.join([item['content'] for item in classified_content[section]])
                
                # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½
                if len(content) > 3000:
                    content = content[:3000] + "... (ë‚´ìš© ì¶•ì•½)"
                
                # ê´€ë ¨ ì§€í‘œ ì¶”ê°€
                section_metrics = ""
                if section == 'financial_data' and 'financial_metrics' in key_metrics:
                    section_metrics = str(key_metrics['financial_metrics'])
                elif section == 'market_analysis' and 'market_metrics' in key_metrics:
                    section_metrics = str(key_metrics['market_metrics'])
                
                prompts[section] = prompt_template.format(
                    content=content,
                    metrics=section_metrics
                )
        
        return prompts
    
    def generate_llm_summaries(self, summary_prompts: Dict[str, str]) -> Dict[str, str]:
        """ë¡œì»¬ LLMì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ìš”ì•½ ìƒì„±"""
        if not self.use_llm:
            logger.warning("LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return {}
        
        logger.info("ë¡œì»¬ LLMì„ ì‚¬ìš©í•œ ìš”ì•½ ìƒì„± ì‹œì‘")
        summaries = {}
        
        for section, prompt in summary_prompts.items():
            try:
                logger.info(f"{section} ì„¹ì…˜ ìš”ì•½ ìƒì„± ì¤‘...")
                
                # LLMì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬
                response = self.llm.invoke(prompt)
                summary = response.content if hasattr(response, 'content') else str(response)
                
                summaries[section] = {
                    'summary': summary.strip(),
                    'prompt_used': prompt,
                    'generated_at': datetime.now().isoformat()
                }
                
                logger.info(f"{section} ì„¹ì…˜ ìš”ì•½ ì™„ë£Œ (ê¸¸ì´: {len(summary)}ì)")
                
            except Exception as e:
                logger.error(f"{section} ì„¹ì…˜ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
                summaries[section] = {
                    'summary': f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}",
                    'prompt_used': prompt,
                    'generated_at': datetime.now().isoformat()
                }
        
        return summaries

    def create_comprehensive_summary(self, llm_summaries: Dict[str, Dict], 
                                   key_metrics: Dict[str, Any]) -> str:
        """ì „ì²´ ì¢…í•© ìš”ì•½ ìƒì„±"""
        if not self.use_llm:
            return "LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì¢…í•© ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì¢…í•© ìš”ì•½ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        comprehensive_prompt = f"""
ë‹¤ìŒ ì„¹ì…˜ë³„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”:

ì„¹ì…˜ë³„ ì •ë³´:
"""
        
        for section, data in llm_summaries.items():
            if 'summary' in data:
                comprehensive_prompt += f"\n### {section}:\n{data['summary']}\n"
        
        comprehensive_prompt += f"""
í•µì‹¬ ì§€í‘œ: {key_metrics}

ë‹¤ìŒ 6ê°œ í•­ëª©ìœ¼ë¡œ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ì •ë¦¬:
1. íšŒì‚¬ ê°œìš” (íšŒì‚¬ëª…, ì‚¬ì—…ë¶„ì•¼)
2. ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ (ì£¼ìš” ìˆ˜ìµì›)
3. ì¬ë¬´ í˜„í™© (ë§¤ì¶œ, íˆ¬ì ë“± ìˆ«ì)
4. ê¸°ìˆ  í˜„í™© (í•µì‹¬ ê¸°ìˆ )
5. ì‹œì¥ í˜„í™© (ì‹œì¥ ê·œëª¨, ê²½ìŸ)
6. íˆ¬ì í¬ì¸íŠ¸ (í•µì‹¬ ê°•ì  3ê°€ì§€)

ì œì•½ì‚¬í•­: ë¬¸ì„œ íŒ©íŠ¸ë§Œ ì‚¬ìš©, ê° í•­ëª© 2-3ë¬¸ì¥ ì´ë‚´, ë¶„ì„/ì¶”ë¡  ê¸ˆì§€, ê°€ìƒ ì¸ë¬¼/íšŒì‚¬/ìˆ«ì ìƒì„± ì ˆëŒ€ ê¸ˆì§€

/no_think
"""
        
        try:
            logger.info("ì¢…í•© ìš”ì•½ ìƒì„± ì¤‘...")
            response = self.llm.invoke(comprehensive_prompt)
            comprehensive_summary = response.content if hasattr(response, 'content') else str(response)
            logger.info("ì¢…í•© ìš”ì•½ ìƒì„± ì™„ë£Œ")
            return comprehensive_summary.strip()
        except Exception as e:
            logger.error(f"ì¢…í•© ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return f"ì¢…í•© ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}"

    def save_summary_results(self, results: Dict[str, Any], output_dir: str, pdf_name: str):
        """ìš”ì•½ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        logger.info(f"ìš”ì•½ ê²°ê³¼ ì €ì¥: {output_dir}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # DataFrameì„ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        serializable_results = {}
        for key, value in results.items():
            if key == 'tables':
                serializable_results[key] = []
                for table in value:
                    table_data = table.copy()
                    if 'dataframe' in table_data:
                        table_data['dataframe'] = table_data['dataframe'].to_dict('records')
                    serializable_results[key].append(table_data)
            else:
                serializable_results[key] = value
        
        # JSON í˜•íƒœë¡œ ì „ì²´ ê²°ê³¼ ì €ì¥
        json_file = Path(output_dir) / f"{pdf_name}_complete_analysis.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        # ì½ê¸° ì‰¬ìš´ ë§ˆí¬ë‹¤ìš´ í˜•íƒœë¡œ ìš”ì•½ ì €ì¥
        md_file = Path(output_dir) / f"{pdf_name}_summary.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(f"# {pdf_name} IR ìë£Œ ë¶„ì„ ìš”ì•½\n\n")
            f.write(f"**ë¶„ì„ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**ì´ í˜ì´ì§€**: {results['pdf_info']['total_pages']}\n")
            f.write(f"**ì¶”ì¶œëœ í‘œ ìˆ˜**: {len(results['tables'])}\n\n")
            
            # ì¢…í•© ìš”ì•½ (think ë¸”ë¡ ì œê±°)
            if 'comprehensive_summary' in results:
                comprehensive_summary = results['comprehensive_summary']
                # <think></think> ë¸”ë¡ ì œê±°
                comprehensive_summary = re.sub(r'<think>.*?</think>', '', comprehensive_summary, flags=re.DOTALL)
                f.write("## ğŸ“‹ ì¢…í•© ìš”ì•½\n\n")
                f.write(f"{comprehensive_summary.strip()}\n\n")
            
            # ì„¹ì…˜ë³„ ìš”ì•½ (think ë¸”ë¡ ì œê±°)
            if 'llm_summaries' in results and results['llm_summaries']:
                f.write("## ğŸ“Š ì„¹ì…˜ë³„ ìƒì„¸ ë¶„ì„\n\n")
                
                for section, data in results['llm_summaries'].items():
                    section_name = {
                        'financial_data': 'ğŸ’° ì¬ë¬´ í˜„í™©',
                        'market_analysis': 'ğŸ“ˆ ì‹œì¥ ë¶„ì„', 
                        'technology': 'ğŸ”¬ ê¸°ìˆ  í˜„í™©',
                        'organization': 'ğŸ‘¥ ì¡°ì§ êµ¬ì¡°',
                        'business_model': 'ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸',
                        'company_overview': 'ğŸ¢ íšŒì‚¬ ê°œìš”',
                        'investment': 'ğŸ’µ íˆ¬ì ê´€ë ¨'
                    }.get(section, section)
                    
                    f.write(f"### {section_name}\n\n")
                    if 'summary' in data:
                        # <think></think> ë¸”ë¡ ì œê±°
                        summary = re.sub(r'<think>.*?</think>', '', data['summary'], flags=re.DOTALL)
                        f.write(f"{summary.strip()}\n\n")
            
            # í•µì‹¬ ì§€í‘œ
            if 'key_metrics' in results:
                f.write("## ğŸ“ˆ í•µì‹¬ ì§€í‘œ\n\n")
                for category, metrics in results['key_metrics'].items():
                    if metrics:
                        f.write(f"**{category}**: {metrics}\n\n")
            
            # ì¶”ì¶œëœ í‘œ ì •ë³´ ì„¹ì…˜ ì œê±° (ìš”ì²­ì— ë”°ë¼)
        
        logger.info(f"ìš”ì•½ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {md_file}")
    
    def process_ir_pdf_unified(self, pdf_path: str, output_dir: str = "raw_data_parsing") -> Dict[str, Any]:
        """í†µí•© í…œí”Œë¦¿ ë°©ì‹ì˜ IR PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        logger.info(f"í†µí•© í…œí”Œë¦¿ ë°©ì‹ìœ¼ë¡œ IR PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        pdf_data = self.extract_text_from_pdf(pdf_path)
        
        # 2. í‘œ ì¶”ì¶œ (ì„ íƒì‚¬í•­)
        tables = self.extract_financial_tables(pdf_path)
        
        # 3. í†µí•© ìš”ì•½ ìƒì„±
        unified_summary = self.generate_unified_summary(pdf_data)
        
        # ê²°ê³¼ ì •ë¦¬
        results = {
            'pdf_info': {
                'file_path': pdf_path,
                'total_pages': pdf_data['total_pages'],
                'processing_time': pdf_data['extraction_time']
            },
            'unified_summary': unified_summary,
            'tables': tables,
            'statistics': {
                'total_pages': pdf_data['total_pages'],
                'tables_extracted': len(tables),
                'total_words': sum(page['word_count'] for page in pdf_data['pages']),
                'processing_method': 'unified_template'
            }
        }
        
        # ê²°ê³¼ ì €ì¥
        pdf_name = Path(pdf_path).stem
        self.save_unified_results(results, output_dir, pdf_name)
        
        return results
    
    def save_unified_results(self, results: Dict[str, Any], output_dir: str, pdf_name: str):
        """í†µí•© ìš”ì•½ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        logger.info(f"í†µí•© ìš”ì•½ ê²°ê³¼ ì €ì¥: {output_dir}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # ë§ˆí¬ë‹¤ìš´ í˜•íƒœë¡œ í†µí•© ìš”ì•½ ì €ì¥
        md_file = Path(output_dir) / f"{pdf_name}_unified_summary.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(f"# {pdf_name} IR ìë£Œ í†µí•© ë¶„ì„\n\n")
            f.write(f"**ë¶„ì„ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**ì´ í˜ì´ì§€**: {results['pdf_info']['total_pages']}\n")
            f.write(f"**ì¶”ì¶œëœ í‘œ ìˆ˜**: {len(results['tables'])}\n")
            f.write(f"**ì²˜ë¦¬ ë°©ì‹**: í†µí•© í…œí”Œë¦¿\n\n")
            
            # í†µí•© ìš”ì•½ ë‚´ìš©
            f.write("## ğŸ“‹ IR ìë£Œ ë¶„ì„ ê²°ê³¼\n\n")
            f.write(f"{results['unified_summary']}\n\n")
            
            # í†µê³„ ì •ë³´
            f.write("## ğŸ“Š ì²˜ë¦¬ í†µê³„\n\n")
            stats = results['statistics']
            f.write(f"- **ì´ í˜ì´ì§€ ìˆ˜**: {stats['total_pages']}\n")
            f.write(f"- **ì¶”ì¶œëœ í‘œ ìˆ˜**: {stats['tables_extracted']}\n")
            f.write(f"- **ì´ ë‹¨ì–´ ìˆ˜**: {stats['total_words']:,}\n")
            f.write(f"- **ì²˜ë¦¬ ë°©ì‹**: {stats['processing_method']}\n")
        
        # JSON í˜•íƒœë¡œë„ ì €ì¥ (í˜¸í™˜ì„±)
        json_file = Path(output_dir) / f"{pdf_name}_unified_analysis.json"
        serializable_results = {}
        for key, value in results.items():
            if key == 'tables':
                serializable_results[key] = []
                for table in value:
                    table_data = table.copy()
                    if 'dataframe' in table_data:
                        table_data['dataframe'] = table_data['dataframe'].to_dict('records')
                    serializable_results[key].append(table_data)
            else:
                serializable_results[key] = value
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"í†µí•© ìš”ì•½ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {md_file}")

    def process_ir_pdf_smart_chunks(self, pdf_path: str, output_dir: str = "raw_data_parsing") -> Dict[str, Any]:
        """ìŠ¤ë§ˆíŠ¸ ì²­í¬ ê¸°ë°˜ 2ë‹¨ê³„ IR PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        logger.info(f"ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë°©ì‹ìœ¼ë¡œ IR PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        pdf_data = self.extract_text_from_pdf(pdf_path)
        
        # 2. ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í• 
        chunks = self.split_pdf_into_smart_chunks(pdf_data)
        
        # 3. í‘œ ì¶”ì¶œ (ì „ì²´ PDF ëŒ€ìƒ)
        tables = self.extract_financial_tables(pdf_path)
        
        # 4. ì²­í¬ë³„ ì •ë³´ ì¶”ì¶œ
        extracted_info = {
            'organization': [],
            'finance': [],
            'technology': [],
            'general': []
        }
        
        for chunk in chunks:
            chunk_type = chunk['type']
            chunk_text = chunk['text']
            
            if chunk_type == 'organization':
                logger.info(f"ì¡°ì§ ì •ë³´ ì²­í¬ ì²˜ë¦¬ ì¤‘ (í˜ì´ì§€ {chunk['pages']})")
                org_info = self.extract_organization_info(chunk_text)
                extracted_info['organization'].append({
                    'pages': chunk['pages'],
                    'content': org_info,
                    'keywords_found': list(chunk['keywords_found'])
                })
            
            elif chunk_type == 'finance':
                logger.info(f"ì¬ë¬´ ì •ë³´ ì²­í¬ ì²˜ë¦¬ ì¤‘ (í˜ì´ì§€ {chunk['pages']})")
                finance_info = self.extract_finance_info(chunk_text)
                extracted_info['finance'].append({
                    'pages': chunk['pages'],
                    'content': finance_info,
                    'keywords_found': list(chunk['keywords_found'])
                })
            
            elif chunk_type == 'technology':
                logger.info(f"ê¸°ìˆ  ì •ë³´ ì²­í¬ ì²˜ë¦¬ ì¤‘ (í˜ì´ì§€ {chunk['pages']})")
                tech_info = self.extract_technology_info(chunk_text)
                extracted_info['technology'].append({
                    'pages': chunk['pages'],
                    'content': tech_info,
                    'keywords_found': list(chunk['keywords_found'])
                })
            
            else:  # general
                # ì¼ë°˜ ì²­í¬ëŠ” ê¸°ì¡´ í†µí•© í…œí”Œë¦¿ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
                logger.info(f"ì¼ë°˜ ì •ë³´ ì²­í¬ ì²˜ë¦¬ ì¤‘ (í˜ì´ì§€ {chunk['pages']})")
                general_info = self.extract_general_info(chunk_text)
                extracted_info['general'].append({
                    'pages': chunk['pages'],
                    'content': general_info,
                    'keywords_found': list(chunk['keywords_found'])
                })
        
        # 5. í†µí•© ìš”ì•½ ìƒì„±
        final_summary = self.merge_extracted_info(extracted_info)
        
        # ê²°ê³¼ ì •ë¦¬
        results = {
            'pdf_info': {
                'file_path': pdf_path,
                'total_pages': pdf_data['total_pages'],
                'processing_time': pdf_data['extraction_time']
            },
            'chunks_info': {
                'total_chunks': len(chunks),
                'chunk_types': [chunk['type'] for chunk in chunks],
                'chunk_pages': [chunk['pages'] for chunk in chunks]
            },
            'extracted_info': extracted_info,
            'final_summary': final_summary,
            'tables': tables,
            'statistics': {
                'total_pages': pdf_data['total_pages'],
                'chunks_processed': len(chunks),
                'tables_extracted': len(tables),
                'total_words': sum(page['word_count'] for page in pdf_data['pages']),
                'processing_method': 'smart_chunks'
            }
        }
        
        # ê²°ê³¼ ì €ì¥
        pdf_name = Path(pdf_path).stem
        self.save_smart_chunk_results(results, output_dir, pdf_name)
        
        return results

    def extract_general_info(self, chunk_text: str) -> str:
        """ì¼ë°˜ ì •ë³´ ì¶”ì¶œ (ê¸°ë³¸ í…œí”Œë¦¿ ë°©ì‹)"""
        if not self.use_llm:
            return "LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì¼ë°˜ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        general_prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

{chunk_text}

ë‹¤ìŒ í•­ëª©ë“¤ì„ ì°¾ì•„ì„œ ì •ë¦¬í•˜ì„¸ìš”:
## ì¼ë°˜ ì •ë³´

### íšŒì‚¬ ê°œìš”
- **íšŒì‚¬ëª…**: [íšŒì‚¬ ì´ë¦„]
- **ì„¤ë¦½ì¼**: [ì„¤ë¦½ ë‚ ì§œ]
- **ì‚¬ì—… ë¶„ì•¼**: [ì£¼ìš” ì‚¬ì—… ì˜ì—­]
- **ë³¸ì‚¬ ìœ„ì¹˜**: [ì£¼ì†Œ]

### ì‚¬ì—… í˜„í™©
- **ì£¼ìš” ì œí’ˆ/ì„œë¹„ìŠ¤**: [ì œí’ˆ ëª©ë¡]
- **ì‹œì¥ ì „ëµ**: [ë§ˆì¼€íŒ… ê³„íš]
- **ì„±ì¥ ê³„íš**: [í–¥í›„ ê³„íš]

ì œì•½ì‚¬í•­:
1. ë¬¸ì„œì— ëª…ì‹œëœ ì •ë³´ë§Œ ì¶”ì¶œ
2. ì—†ëŠ” ì •ë³´ëŠ” "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ í‘œê¸°
3. ê°€ìƒ ì •ë³´ ìƒì„± ì ˆëŒ€ ê¸ˆì§€

/no_think
"""
        
        try:
            response = self.llm.invoke(general_prompt)
            result = response.content if hasattr(response, 'content') else str(response)
            logger.info("ì¼ë°˜ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")
            return result.strip()
        except Exception as e:
            logger.error(f"ì¼ë°˜ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return f"ì¼ë°˜ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}"

    def merge_extracted_info(self, extracted_info: Dict[str, List]) -> str:
        """ì¶”ì¶œëœ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ìµœì¢… ìš”ì•½ ìƒì„±"""
        logger.info("ì¶”ì¶œëœ ì •ë³´ í†µí•© ì‹œì‘")
        
        merged_summary = "# IR ìë£Œ ì¢…í•© ë¶„ì„ (ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë°©ì‹)\n\n"
        
        # ì¡°ì§ ì •ë³´ í†µí•©
        if extracted_info['organization']:
            merged_summary += "## ğŸ“‹ ì¡°ì§ êµ¬ì„±\n\n"
            for org_chunk in extracted_info['organization']:
                merged_summary += f"### í˜ì´ì§€ {'-'.join(map(str, org_chunk['pages']))}\n"
                merged_summary += f"{org_chunk['content']}\n\n"
        
        # ì¬ë¬´ ì •ë³´ í†µí•©
        if extracted_info['finance']:
            merged_summary += "## ğŸ’° ì¬ë¬´ í˜„í™©\n\n"
            for finance_chunk in extracted_info['finance']:
                merged_summary += f"### í˜ì´ì§€ {'-'.join(map(str, finance_chunk['pages']))}\n"
                merged_summary += f"{finance_chunk['content']}\n\n"
        
        # ê¸°ìˆ  ì •ë³´ í†µí•©
        if extracted_info['technology']:
            merged_summary += "## ğŸ”¬ ê¸°ìˆ  í˜„í™©\n\n"
            for tech_chunk in extracted_info['technology']:
                merged_summary += f"### í˜ì´ì§€ {'-'.join(map(str, tech_chunk['pages']))}\n"
                merged_summary += f"{tech_chunk['content']}\n\n"
        
        # ì¼ë°˜ ì •ë³´ í†µí•©
        if extracted_info['general']:
            merged_summary += "## ğŸ“ˆ ê¸°íƒ€ ì •ë³´\n\n"
            for general_chunk in extracted_info['general']:
                merged_summary += f"### í˜ì´ì§€ {'-'.join(map(str, general_chunk['pages']))}\n"
                merged_summary += f"{general_chunk['content']}\n\n"
        
        logger.info("ì •ë³´ í†µí•© ì™„ë£Œ")
        return merged_summary

    def post_process_summary(self, markdown_text: str) -> str:
        """
        ë¡œì§ ê¸°ë°˜ìœ¼ë¡œ ë§ˆí¬ë‹¤ìš´ ìš”ì•½ë³¸ì„ í›„ì²˜ë¦¬í•˜ì—¬ ì¤‘ë³µì„ ì œê±°í•˜ê³  ì •ë³´ë¥¼ í†µí•©í•©ë‹ˆë‹¤.
        """
        logger.info("ë¡œì§ ê¸°ë°˜ ìš”ì•½ í›„ì²˜ë¦¬ ì‹œì‘")
        if not markdown_text or not markdown_text.strip():
            return ""

        # 'í˜ì´ì§€' ì •ë³´ë¥¼ ë‹´ì€ í—¤ë”ë¥¼ ì œì™¸í•œ ìˆœìˆ˜ í—¤ë”ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
        def get_clean_header(full_header):
            return re.sub(r'\s*\(í˜ì´ì§€:?.*?\)', '', full_header).strip()

        # í—¤ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        chunks = re.split(r'\n(?=## |### )', markdown_text)
        
        # ìµœì¢… ì„¹ì…˜ë“¤ì„ ìˆœì„œëŒ€ë¡œ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ì™€, ë‚´ìš©ì„ ë³‘í•©í•  ë”•ì…”ë„ˆë¦¬
        header_order = []
        final_sections = {}
        
        placeholder_keywords = ["ì •ë³´ ì—†ìŒ", "ì œê³µë˜ì§€ ì•ŠìŒ", "not available", "ì—†ìŒ"]

        def is_placeholder(text):
            # ë‚´ìš©ì˜ í•µì‹¬ì´ í”Œë ˆì´ìŠ¤í™€ë” í‚¤ì›Œë“œì¸ì§€ ê²€ì‚¬
            # ì—¬ëŸ¬ ì¤„ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê° ì¤„ì„ ê²€ì‚¬
            lines = text.strip().split('\n')
            # ëª¨ë“  ì¤„ì´ í”Œë ˆì´ìŠ¤í™€ë” ê´€ë ¨ ë‚´ìš©ì¼ ë•Œë§Œ True
            return all(any(keyword in line for keyword in placeholder_keywords) for line in lines if line.strip())

        for chunk in chunks:
            chunk = chunk.strip()
            if not chunk:
                continue
            
            lines = chunk.split('\n')
            full_header = lines[0].strip()
            clean_header = get_clean_header(full_header)
            content = '\n'.join(lines[1:]).strip()

            # í—¤ë”ê°€ ì²˜ìŒ ë‚˜íƒ€ë‚˜ëŠ” ê²½ìš°, ìˆœì„œì™€ ë‚´ìš©ì„ ê¸°ë¡
            if clean_header not in final_sections:
                header_order.append(clean_header)
                final_sections[clean_header] = {'header': full_header, 'content': content}
            # ì¤‘ë³µëœ í—¤ë”ë¥¼ ë§Œë‚œ ê²½ìš°
            else:
                existing_section = final_sections[clean_header]
                existing_content = existing_section['content']
                
                # ë³‘í•© ê·œì¹™ ì ìš©
                # 1. ê¸°ì¡´ ë‚´ìš©ì´ í”Œë ˆì´ìŠ¤í™€ë”ì´ê³  ìƒˆ ë‚´ìš©ì— ì •ë³´ê°€ ìˆìœ¼ë©´ êµì²´
                if is_placeholder(existing_content) and not is_placeholder(content):
                    logger.info(f"'{clean_header}' ì„¹ì…˜ ì—…ë°ì´íŠ¸: í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‹¤ì œ ì •ë³´ë¡œ êµì²´í•©ë‹ˆë‹¤.")
                    final_sections[clean_header]['content'] = content
                    # í˜ì´ì§€ ì •ë³´ë„ ìµœì‹  ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
                    final_sections[clean_header]['header'] = full_header
                # 2. ë‘˜ ë‹¤ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°, ë‚´ìš©ì„ í•©ì¹¨
                elif not is_placeholder(existing_content) and not is_placeholder(content):
                    logger.info(f"'{clean_header}' ì„¹ì…˜ ë³‘í•©: ê¸°ì¡´ ì •ë³´ì— ìƒˆ ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")
                    # ì¤‘ë³µ ë¼ì¸ì„ í”¼í•˜ë©´ì„œ ë³‘í•©
                    existing_lines = set(ex.strip() for ex in existing_content.split('\n') if ex.strip())
                    new_lines = set(ne.strip() for ne in content.split('\n') if ne.strip())
                    combined_lines = sorted(list(existing_lines.union(new_lines)))
                    final_sections[clean_header]['content'] = '\n'.join(combined_lines)
                # 3. ê·¸ ì™¸ì˜ ê²½ìš° (ìƒˆ ë‚´ìš©ì´ í”Œë ˆì´ìŠ¤í™€ë” ë“±)ëŠ” ê¸°ì¡´ ë‚´ìš©ì„ ìœ ì§€

        # ìµœì¢… ë§ˆí¬ë‹¤ìš´ ì¬êµ¬ì„±
        final_markdown_parts = []
        for header_key in header_order:
            section = final_sections.get(header_key)
            if section and section['content']:
                final_markdown_parts.append(f"{section['header']}\n{section['content']}")
        
        logger.info("ë¡œì§ ê¸°ë°˜ ìš”ì•½ í›„ì²˜ë¦¬ ì™„ë£Œ")
        return "\n\n".join(final_markdown_parts)

    def save_smart_chunk_results(self, results: Dict[str, Any], output_dir: str, pdf_name: str):
        """ìŠ¤ë§ˆíŠ¸ ì²­í¬ ì²˜ë¦¬ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (í›„ì²˜ë¦¬ í¬í•¨)"""
        logger.info(f"ìŠ¤ë§ˆíŠ¸ ì²­í¬ ê²°ê³¼ ì €ì¥: {output_dir}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # 1ì°¨ ìš”ì•½ë³¸ì— ëŒ€í•´ ë¡œì§ ê¸°ë°˜ í›„ì²˜ë¦¬ ìˆ˜í–‰
        raw_summary = results['final_summary']
        polished_summary = self.post_process_summary(raw_summary)
        
        # ë§ˆí¬ë‹¤ìš´ í˜•íƒœë¡œ ìµœì¢… ìš”ì•½ ì €ì¥
        md_file = Path(output_dir) / f"{pdf_name}_smart_chunk_summary.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(f"# {pdf_name} IR ìë£Œ ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„ì„\n\n")
            f.write(f"**ë¶„ì„ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**ì²˜ë¦¬ ë°©ì‹**: ìŠ¤ë§ˆíŠ¸ ì²­í¬ + ë¡œì§ ê¸°ë°˜ í›„ì²˜ë¦¬\n\n")
            f.write("## ğŸ“‹ ìµœì¢… ë¶„ì„ ê²°ê³¼\n\n")
            f.write(f"{polished_summary}\n\n")
            
            # í†µê³„ ì •ë³´ ì¶”ê°€
            f.write("## ğŸ“Š ì²˜ë¦¬ í†µê³„\n\n")
            stats = results['statistics']
            f.write(f"- **ì´ í˜ì´ì§€ ìˆ˜**: {stats['total_pages']}\n")
            f.write(f"- **ì²˜ë¦¬ëœ ì²­í¬ ìˆ˜**: {stats['chunks_processed']}\n")
            f.write(f"- **ì¶”ì¶œëœ í‘œ ìˆ˜**: {stats['tables_extracted']}\n")
            f.write(f"- **ì´ ë‹¨ì–´ ìˆ˜**: {stats['total_words']:,}\n")

        # JSON í˜•íƒœë¡œë„ ì €ì¥ (í›„ì²˜ë¦¬ëœ ë‚´ìš© í¬í•¨)
        json_file = Path(output_dir) / f"{pdf_name}_smart_chunk_analysis.json"
        serializable_results = results.copy()
        serializable_results['polished_summary'] = polished_summary
        
        # DataFrame ì§ë ¬í™”
        if 'tables' in serializable_results:
            for table in serializable_results['tables']:
                if 'dataframe' in table and isinstance(table['dataframe'], pd.DataFrame):
                    table['dataframe'] = table['dataframe'].to_dict('records')
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ìŠ¤ë§ˆíŠ¸ ì²­í¬ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {md_file}")

    def process_ir_pdf(self, pdf_path: str, output_dir: str = "raw_data_parsing") -> Dict[str, Any]:
        """IR PDF ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (LLM ìš”ì•½ í¬í•¨)"""
        logger.info(f"IR PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        pdf_data = self.extract_text_from_pdf(pdf_path)
        
        # 2. ì„¹ì…˜ë³„ ë¶„ë¥˜
        classified_content = self.classify_content_by_section(pdf_data['pages'])
        
        # 3. í‘œ ì¶”ì¶œ
        tables = self.extract_financial_tables(pdf_path)
        
        # 4. í•µì‹¬ ì§€í‘œ ì¶”ì¶œ
        key_metrics = self.extract_key_metrics(classified_content)
        
        # 5. ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±
        summary_prompts = self.generate_summary_prompts(classified_content, key_metrics)
        
        # 6. LLMì„ ì‚¬ìš©í•œ ì‹¤ì œ ìš”ì•½ ìƒì„±
        llm_summaries = {}
        comprehensive_summary = ""
        
        if self.use_llm and summary_prompts:
            llm_summaries = self.generate_llm_summaries(summary_prompts)
            comprehensive_summary = self.create_comprehensive_summary(llm_summaries, key_metrics)
        
        # ê²°ê³¼ ì •ë¦¬
        results = {
            'pdf_info': {
                'file_path': pdf_path,
                'total_pages': pdf_data['total_pages'],
                'processing_time': pdf_data['extraction_time']
            },
            'classified_content': classified_content,
            'tables': tables,
            'key_metrics': key_metrics,
            'summary_prompts': summary_prompts,
            'llm_summaries': llm_summaries,
            'comprehensive_summary': comprehensive_summary,
            'statistics': {
                'sections_found': len([k for k, v in classified_content.items() if v and k != 'uncategorized']),
                'tables_extracted': len(tables),
                'total_words': sum(sum(item['word_count'] for item in section) for section in classified_content.values()),
                'summaries_generated': len(llm_summaries)
            }
        }
        
        # ê²°ê³¼ ì €ì¥
        pdf_name = Path(pdf_path).stem
        self.save_summary_results(results, output_dir, pdf_name)
        
        # ê¸°ì¡´ JSONë„ í•¨ê»˜ ì €ì¥ (í˜¸í™˜ì„±)
        output_file = Path(output_dir) / f"{pdf_name}_analysis.json"
        self.save_results(results, str(output_file))
        
        return results
    
    def save_results(self, results: Dict[str, Any], output_path: str):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        logger.info(f"ê²°ê³¼ ì €ì¥: {output_path}")
        
        # DataFrameì„ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        serializable_results = {}
        
        for key, value in results.items():
            if key == 'tables':
                serializable_results[key] = []
                for table in value:
                    table_data = table.copy()
                    if 'dataframe' in table_data:
                        table_data['dataframe'] = table_data['dataframe'].to_dict('records')
                    serializable_results[key].append(table_data)
            else:
                serializable_results[key] = value
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        logger.info("ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
    
    def print_summary_report(self, results: Dict[str, Any]):
        """ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥"""
        print("\n" + "="*80)
        print("IR PDF ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        # ê¸°ë³¸ ì •ë³´
        pdf_info = results['pdf_info']
        print(f"íŒŒì¼: {pdf_info['file_path']}")
        print(f"ì´ í˜ì´ì§€: {pdf_info['total_pages']}")
        print(f"ì²˜ë¦¬ ì‹œê°„: {pdf_info['processing_time']}")
        
        # ì„¹ì…˜ë³„ ë¶„ë¥˜ ê²°ê³¼
        print(f"\nğŸ“Š ì„¹ì…˜ë³„ ë¶„ë¥˜ ê²°ê³¼:")
        classified = results['classified_content']
        for section, content in classified.items():
            if content:
                pages = [item['page_number'] for item in content]
                words = sum(item['word_count'] for item in content)
                print(f"  â€¢ {section}: {len(content)}í˜ì´ì§€ (í˜ì´ì§€ {pages}, {words:,}ë‹¨ì–´)")
        
        # í‘œ ì¶”ì¶œ ê²°ê³¼
        tables = results['tables']
        if tables:
            print(f"\nğŸ“‹ ì¶”ì¶œëœ í‘œ: {len(tables)}ê°œ")
            for table in tables:
                print(f"  â€¢ í‘œ {table['table_id']}: í˜ì´ì§€ {table['page']} ({table['dataframe'].shape[0]}í–‰ x {table['dataframe'].shape[1]}ì—´)")
        
        # í•µì‹¬ ì§€í‘œ
        metrics = results['key_metrics']
        print(f"\nğŸ“ˆ ì¶”ì¶œëœ í•µì‹¬ ì§€í‘œ:")
        for category, data in metrics.items():
            if data:
                print(f"  â€¢ {category}: {data}")
        
        # LLM ìš”ì•½ ê²°ê³¼
        if 'llm_summaries' in results and results['llm_summaries']:
            print(f"\nğŸ¤– LLM ìƒì„± ìš”ì•½: {len(results['llm_summaries'])}ê°œ ì„¹ì…˜")
            for section in results['llm_summaries'].keys():
                print(f"  â€¢ {section}")
        
        # ì¢…í•© ìš”ì•½ ë¯¸ë¦¬ë³´ê¸°
        if 'comprehensive_summary' in results and results['comprehensive_summary']:
            print(f"\nğŸ“ ì¢…í•© ìš”ì•½ (ë¯¸ë¦¬ë³´ê¸°):")
            summary_preview = results['comprehensive_summary'][:200] + "..." if len(results['comprehensive_summary']) > 200 else results['comprehensive_summary']
            print(f"  {summary_preview}")
        
        # ìš”ì•½ í”„ë¡¬í”„íŠ¸
        prompts = results['summary_prompts']
        print(f"\nğŸ“ ìƒì„±ëœ ìš”ì•½ í”„ë¡¬í”„íŠ¸: {len(prompts)}ê°œ ì„¹ì…˜")
        for section in prompts.keys():
            print(f"  â€¢ {section}")
        
        print(f"\nğŸ“Š í†µê³„:")
        stats = results['statistics']
        print(f"  â€¢ ë°œê²¬ëœ ì„¹ì…˜: {stats['sections_found']}ê°œ")
        print(f"  â€¢ ì¶”ì¶œëœ í‘œ: {stats['tables_extracted']}ê°œ")
        print(f"  â€¢ ì´ ë‹¨ì–´ ìˆ˜: {stats['total_words']:,}ê°œ")
        if 'summaries_generated' in stats:
            print(f"  â€¢ ìƒì„±ëœ ìš”ì•½: {stats['summaries_generated']}ê°œ")
        
        print("="*80)
    
    def print_unified_report(self, results: Dict[str, Any]):
        """í†µí•© ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥"""
        print("\n" + "="*80)
        print("IR PDF í†µí•© ë¶„ì„ ê²°ê³¼")
        print("="*80)
        
        # ê¸°ë³¸ ì •ë³´
        pdf_info = results['pdf_info']
        print(f"íŒŒì¼: {pdf_info['file_path']}")
        print(f"ì´ í˜ì´ì§€: {pdf_info['total_pages']}")
        print(f"ì²˜ë¦¬ ì‹œê°„: {pdf_info['processing_time']}")
        
        # ì²˜ë¦¬ í†µê³„
        stats = results['statistics']
        print(f"\nğŸ“Š ì²˜ë¦¬ í†µê³„:")
        print(f"  â€¢ ì´ í˜ì´ì§€ ìˆ˜: {stats['total_pages']}")
        print(f"  â€¢ ì¶”ì¶œëœ í‘œ: {stats['tables_extracted']}ê°œ")
        print(f"  â€¢ ì´ ë‹¨ì–´ ìˆ˜: {stats['total_words']:,}ê°œ")
        print(f"  â€¢ ì²˜ë¦¬ ë°©ì‹: {stats['processing_method']}")
        
        # í†µí•© ìš”ì•½ ë¯¸ë¦¬ë³´ê¸°
        if 'unified_summary' in results and results['unified_summary']:
            print(f"\nğŸ“ í†µí•© ìš”ì•½ (ë¯¸ë¦¬ë³´ê¸°):")
            summary_preview = results['unified_summary'][:300] + "..." if len(results['unified_summary']) > 300 else results['unified_summary']
            print(f"  {summary_preview}")
        
        print("="*80)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="IR PDF íŒŒì„œ ë° ìš”ì•½ê¸°")
    parser.add_argument("pdf_path", type=str, help="ë¶„ì„í•  IR PDF íŒŒì¼ì˜ ê²½ë¡œ")
    parser.add_argument(
        "--method", 
        type=str, 
        default="smart_chunks", 
        choices=["unified", "smart_chunks"],
        help="ë¶„ì„ ë°©ë²• ì„ íƒ: 'unified' (í†µí•© í…œí”Œë¦¿) ë˜ëŠ” 'smart_chunks' (ìŠ¤ë§ˆíŠ¸ ì²­í¬)"
    )
    parser.add_argument(
        "--no-llm",
        action="store_true",
        help="LLM ì‚¬ìš©ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤."
    )
    args = parser.parse_args()

    print(f"IR PDF íŒŒì„œ ì‹œì‘: {args.pdf_path}")
    print(f"ë¶„ì„ ë°©ë²•: {args.method}, LLM ì‚¬ìš©: {not args.no_llm}")

    # íŒŒì„œ ì´ˆê¸°í™”
    parser_instance = IRPDFParser(use_llm=not args.no_llm)
    
    try:
        if args.method == "unified":
            # IR PDF ì²˜ë¦¬ (í†µí•© í…œí”Œë¦¿ ë°©ì‹ ì‚¬ìš©)
            results = parser_instance.process_ir_pdf_unified(args.pdf_path, output_dir="raw_data_parsing")
            
            # í†µí•© ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥
            parser_instance.print_unified_report(results)
            
            pdf_name = Path(args.pdf_path).stem
            print(f"\nâœ… í†µí•© í…œí”Œë¦¿ ë°©ì‹ ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"ğŸ“ ìƒì„¸ ê²°ê³¼: raw_data_parsing/{pdf_name}_unified_analysis.json")
            print(f"ğŸ“„ í†µí•© ìš”ì•½: raw_data_parsing/{pdf_name}_unified_summary.md")

        elif args.method == "smart_chunks":
            # IR PDF ì²˜ë¦¬ (ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë°©ì‹ ì‚¬ìš©)
            results = parser_instance.process_ir_pdf_smart_chunks(args.pdf_path, output_dir="raw_data_parsing")
            
            # ìŠ¤ë§ˆíŠ¸ ì²­í¬ ê²°ê³¼ ë³´ê³ ì„œ ì¶œë ¥ (ê°„ë‹¨í•˜ê²Œ)
            pdf_name = Path(args.pdf_path).stem
            print("\n" + "="*80)
            print("IR PDF ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„ì„ ê²°ê³¼")
            print("="*80)
            print(f"íŒŒì¼: {results['pdf_info']['file_path']}")
            print(f"ì´ í˜ì´ì§€: {results['pdf_info']['total_pages']}")
            print(f"ì²˜ë¦¬ëœ ì²­í¬ ìˆ˜: {results['chunks_info']['total_chunks']}")
            print(f"ì²˜ë¦¬ ë°©ì‹: {results['statistics']['processing_method']}")
            print("="*80)

            print(f"\nâœ… ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë°©ì‹ ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"ğŸ“ ìƒì„¸ ê²°ê³¼: raw_data_parsing/{pdf_name}_smart_chunk_analysis.json")
            print(f"ğŸ“„ ìŠ¤ë§ˆíŠ¸ ì²­í¬ ìš”ì•½: raw_data_parsing/{pdf_name}_smart_chunk_summary.md")

    except FileNotFoundError:
        logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.pdf_path}")
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)


if __name__ == "__main__":
    main()
 