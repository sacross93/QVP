#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Vision-LLMìœ¼ë¡œ ìƒì„±ëœ IR ë§ˆí¬ë‹¤ìš´ ë¶„ì„ê¸° (v5 - ìµœì¢… ì•ˆì •í™”)

'ê³„ì¸µì  ë¶„ì„ ë° ì¢…í•© íŒŒì´í”„ë¼ì¸'ì„ ì‚¬ìš©í•˜ì—¬ íˆ¬ì ë¶„ì„ì— í•„ìš”í•œ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
LLMì˜ ì¶œë ¥ì—ì„œ <think> ë¸”ë¡ì„ ì •ê·œì‹ìœ¼ë¡œ ì œê±°í•œ í›„ JSON íŒŒì‹±ì„ ìˆ˜í–‰í•˜ì—¬ ì•ˆì •ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
"""

import re
import json
import logging
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional

# ë¡œì»¬ LLM ì—°ë™ (langchain_ollama)
try:
    from langchain_ollama import ChatOllama
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import PromptTemplate
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    print("âš ï¸ langchain_ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install langchain-ollama langchain-core ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
    class ChatOllama: pass
    class StrOutputParser: pass
    class PromptTemplate: pass

# --- ì„¤ì • ---
MODEL_ID = "qwen3:32b" # ëª¨ë¸ IDëŠ” í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš” (ì˜ˆ: "qwen2:32b")
BASE_URL = "http://192.168.120.102:11434" # Ollama ì„œë²„ ì£¼ì†Œ

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class VisionMDAnalyzer:
    """
    Vision LLMì´ ìƒì„±í•œ ë§ˆí¬ë‹¤ìš´(.md) íŒŒì¼ì„ ë¶„ì„í•˜ì—¬
    í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  êµ¬ì¡°í™”í•˜ëŠ” í´ë˜ìŠ¤. (ìµœì¢… ì•ˆì •í™” ë²„ì „)
    """

    def __init__(self, model_id: str = MODEL_ID, base_url: str = BASE_URL):
        if not OLLAMA_AVAILABLE:
            logger.error("í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self.use_llm = False
            return
        try:
            self.llm = ChatOllama(model=model_id, base_url=base_url, temperature=0)
            self.use_llm = True
            logger.info(f"LLM ì´ˆê¸°í™” ì™„ë£Œ: {model_id} at {base_url}")
        except Exception as e:
            logger.error(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. LLM ê¸°ëŠ¥ ì—†ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            self.use_llm = False

    def _clean_and_parse_json(self, raw_output: str) -> Optional[Any]:
        """
        LLMì˜ ì›ì‹œ ì¶œë ¥ì—ì„œ <think> ë¸”ë¡ì„ ì œê±°í•˜ê³ , ë‚¨ì€ ë¬¸ìì—´ì„ JSONìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
        """
        try:
            # <think>...</think> ë¸”ë¡ì„ re.subìœ¼ë¡œ ì œê±°í•©ë‹ˆë‹¤.
            cleaned_output = re.sub(r'<think>.*?</think>', '', raw_output, flags=re.DOTALL).strip()
            
            # ì •ì œëœ ë¬¸ìì—´ì—ì„œ JSON ê°ì²´ ë˜ëŠ” ë°°ì—´ì„ ì°¾ìŠµë‹ˆë‹¤.
            match = re.search(r'(\{.*\}|\[.*\])', cleaned_output, re.DOTALL)
            if match:
                json_string = match.group(0)
                return json.loads(json_string)
            else:
                logger.warning(f"ì •ì œëœ ì¶œë ¥ì—ì„œ JSONì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. LLM ì¶œë ¥:\n---\n{cleaned_output}\n---")
                return None
        except json.JSONDecodeError as e:
            logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}\nì›ì‹œ ì¶œë ¥ ì¼ë¶€: {raw_output[:300]}...")
            return None
        except Exception as e:
            logger.error(f"JSON ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def _split_md_by_headers(self, md_content: str) -> List[Dict[str, str]]:
        logger.info("1ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ êµ¬ì¡°ì  ë¶„í•  ì‹œì‘...")
        chunks = re.split(r'\n\s*\n(?=^#{1,2} )', md_content.strip(), flags=re.MULTILINE)
        structured_chunks = []
        for chunk in chunks:
            if not chunk.strip(): continue
            lines = chunk.strip().split('\n')
            header = lines[0].strip()
            content = '\n'.join(lines[1:]).strip()
            if not header.startswith('#'):
                header = "# ì„œë¬¸"
                content = chunk.strip()
            structured_chunks.append({'header': header, 'content': content})
        logger.info(f"ë¶„í•  ì™„ë£Œ: {len(structured_chunks)}ê°œì˜ êµ¬ì¡°ì  ì„¹ì…˜ ìƒì„±.")
        return structured_chunks

    def _generate_chunk_summaries(self, chunks: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        if not self.use_llm: return [{**c, 'summary': '', 'keywords': []} for c in chunks]
        logger.info("2ë‹¨ê³„: ì„¹ì…˜ë³„ ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œ (ì¸ë±ì‹±) ì‹œì‘...")
        template = """ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ê³ , ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ 5ê°œë§Œ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•´ì¤˜.\n\n--- í…ìŠ¤íŠ¸ ì‹œì‘ ---\n{text}\n--- í…ìŠ¤íŠ¸ ë ---\n\nê²°ê³¼ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´ì¤˜.\nìš”ì•½: [ì—¬ê¸°ì— ìš”ì•½]\ní‚¤ì›Œë“œ: [ì—¬ê¸°ì— í‚¤ì›Œë“œ]\n/no_think\n"""
        prompt = PromptTemplate.from_template(template)
        chain = prompt | self.llm | StrOutputParser()
        indexed_chunks = []
        for i, chunk in enumerate(chunks):
            logger.info(f"  - ì¸ë±ì‹± ì²˜ë¦¬ ì¤‘... ({i+1}/{len(chunks)}) : {chunk['header']}")
            try:
                response = chain.invoke({"text": chunk['content']})
                summary_match = re.search(r"ìš”ì•½:\s*(.*)", response)
                keywords_match = re.search(r"í‚¤ì›Œë“œ:\s*(.*)", response)
                summary = summary_match.group(1).strip() if summary_match else "ìš”ì•½ ì‹¤íŒ¨"
                keywords = [kw.strip() for kw in keywords_match.group(1).strip().split(',')] if keywords_match else []
                indexed_chunks.append({**chunk, 'summary': summary, 'keywords': keywords})
            except Exception as e:
                logger.warning(f"  - ì¸ë±ì‹± ì‹¤íŒ¨: {chunk['header']}. ì›ë³¸ ë‚´ìš©ë§Œ ì‚¬ìš©. ì˜¤ë¥˜: {e}")
                indexed_chunks.append({**chunk, 'summary': '', 'keywords': []})
        logger.info("ì¸ë±ì‹± ì™„ë£Œ.")
        return indexed_chunks

    def _group_chunks_by_theme(self, indexed_chunks: List[Dict[str, Any]]) -> Dict[str, List[str]]:
        if not self.use_llm: return {}
        logger.info("3-1ë‹¨ê³„: ì£¼ì œë³„ ê·¸ë£¹í™” ì‹œì‘...")
        summaries_text = "\n".join(f"- í—¤ë”: {chunk['header']}\n  ìš”ì•½: {chunk.get('summary', '')}" for chunk in indexed_chunks if chunk.get('summary'))
        template = """ë‹¹ì‹ ì€ íˆ¬ì ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì€ í•œ IR ë¬¸ì„œì˜ ì„¹ì…˜ë³„ ìš”ì•½ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ê° ì„¹ì…˜ì˜ 'í—¤ë”'ë¥¼ ì½ê³ , ì•„ë˜ 6ê°€ì§€ ì£¼ì œ ì¤‘ ê°€ì¥ ì í•©í•œ ì£¼ì œë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”. ê²°ê³¼ëŠ” ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”. ì˜ˆì‹œ: {{"íšŒì‚¬ ê°œìš”": ["í—¤ë”1", "í—¤ë”2"], "ê²½ì˜ì§„ ë° ì¡°ì§": ["í—¤ë”3"]}}\n\n**ë¶„ë¥˜í•  ì£¼ì œ:**\n["íšŒì‚¬ ê°œìš”", "ê²½ì˜ì§„ ë° ì¡°ì§", "ì¬ë¬´ í˜„í™©", "ê¸°ìˆ  ë° ì œí’ˆ", "ì‚¬ì—… ë° ì‹œì¥ ì „ëµ", "ê¸°íƒ€"]\n\n**ì„¹ì…˜ë³„ ìš”ì•½ ë¦¬ìŠ¤íŠ¸:**\n---\n{summaries}\n---\n\n**JSON ì¶œë ¥:**\n/no_think\n"""
        prompt = PromptTemplate.from_template(template)
        chain = prompt | self.llm | StrOutputParser()
        all_keys = ["íšŒì‚¬ ê°œìš”", "ê²½ì˜ì§„ ë° ì¡°ì§", "ì¬ë¬´ í˜„í™©", "ê¸°ìˆ  ë° ì œí’ˆ", "ì‚¬ì—… ë° ì‹œì¥ ì „ëµ", "ê¸°íƒ€"]
        final_groups = {key: [] for key in all_keys}
        try:
            raw_output = chain.invoke({"summaries": summaries_text})
            response_data = self._clean_and_parse_json(raw_output)
            
            if isinstance(response_data, dict):
                for key, value in response_data.items():
                    if key in final_groups and isinstance(value, list):
                        final_groups[key].extend(value)
            logger.info("ì£¼ì œë³„ ê·¸ë£¹í™” ì™„ë£Œ.")
        except Exception as e:
            logger.error(f"ì£¼ì œë³„ ê·¸ë£¹í™” ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}. ë¹ˆ ê·¸ë£¹ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
        return final_groups

    def _extract_detailed_info(self, grouped_headers: Dict[str, List[str]], original_chunks: List[Dict[str, str]]) -> Dict[str, Any]:
        if not self.use_llm: return {}
        logger.info("3-2ë‹¨ê³„: ì£¼ì œë³„ ì‹¬ì¸µ ì •ë³´ ì¶”ì¶œ ì‹œì‘...")
        chunk_map = {chunk['header']: chunk['content'] for chunk in original_chunks}
        extraction_prompts = {
            "íšŒì‚¬ ê°œìš”": """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•˜ëŠ” AIì…ë‹ˆë‹¤. ì ˆëŒ€ë¡œ ìš”ì•½ì´ë‚˜ ì„¤ëª…ì„ ìƒì„±í•˜ì§€ ë§ˆì‹œì˜¤.
ì‘ë‹µì€ ì•„ë˜ì— ëª…ì‹œëœ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

**ì¶”ì¶œí•  ì •ë³´:**
- íšŒì‚¬ëª… (corporation_name)
- ì„¤ë¦½ì¼ (foundation_date)
- ëŒ€í‘œì (ceo)
- ì£¼ìš” ì‚¬ì—… ë¶„ì•¼ (main_business)
- ë³¸ì‚¬ ë° ì£¼ìš” ì‹œì„¤ ìœ„ì¹˜ (locations)
- íšŒì‚¬ ì—°í˜ì˜ ì£¼ìš” ë§ˆì¼ìŠ¤í†¤ (history_milestones)

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
```{context}```

**JSON ìŠ¤í‚¤ë§ˆ:**
```json
{{
    "properties": {{
        "corporation_name": {{"type": "string", "description": "íšŒì‚¬ëª…"}},
        "foundation_date": {{"type": "string", "description": "ì„¤ë¦½ì¼"}},
        "ceo": {{"type": "string", "description": "ëŒ€í‘œì ì´ë¦„"}},
        "main_business": {{"type": "string", "description": "ì£¼ìš” ì‚¬ì—… ë¶„ì•¼"}},
        "locations": {{"type": "array", "items": {{"type": "string"}}, "description": "ë³¸ì‚¬, ì—°êµ¬ì†Œ ë“± ì£¼ìš” ìœ„ì¹˜ ëª©ë¡"}},
        "history_milestones": {{"type": "array", "items": {{"type": "string"}}, "description": "íšŒì‚¬ ì—°í˜ì˜ ì£¼ìš” ì´ë²¤íŠ¸ ëª©ë¡"}}
    }},
    "required": ["corporation_name", "foundation_date", "ceo", "main_business"]
}}
```

/no_think""",
            "ê²½ì˜ì§„ ë° ì¡°ì§": """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•˜ëŠ” AIì…ë‹ˆë‹¤. ì ˆëŒ€ë¡œ ìš”ì•½ì´ë‚˜ ì„¤ëª…ì„ ìƒì„±í•˜ì§€ ë§ˆì‹œì˜¤.
ì‘ë‹µì€ ì•„ë˜ì— ëª…ì‹œëœ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

**ì¶”ì¶œí•  ì •ë³´:**
- ê° ê²½ì˜ì§„ì˜ ì´ë¦„(name), ì§ì±…(position), ìµœì¢… í•™ë ¥(education), ì£¼ìš” ê²½ë ¥(career). ëª¨ë“  ì¸ë¬¼ì„ ë¹ ì§ì—†ì´ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
```{context}```

**JSON ìŠ¤í‚¤ë§ˆ:**
```json
{{
    "type": "array",
    "items": {{
        "type": "object",
        "properties": {{
            "name": {{"type": "string", "description": "ì´ë¦„"}},
            "position": {{"type": "string", "description": "ì§ì±…"}},
            "education": {{"type": "string", "description": "ìµœì¢… í•™ë ¥"}},
            "career": {{"type": "string", "description": "ì£¼ìš” ê²½ë ¥"}}
        }},
        "required": ["name", "position"]
    }}
}}
```
/no_think""",
            "ì¬ë¬´ í˜„í™©": """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•˜ëŠ” AIì…ë‹ˆë‹¤. ì ˆëŒ€ë¡œ ìš”ì•½ì´ë‚˜ ì„¤ëª…ì„ ìƒì„±í•˜ì§€ ë§ˆì‹œì˜¤.
ì‘ë‹µì€ ì•„ë˜ì— ëª…ì‹œëœ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

**ì¶”ì¶œí•  ì •ë³´:**
- `financial_summary`: ì—°ë„ë³„ ì¬ë¬´ ì§€í‘œ (ë§¤ì¶œ, ì˜ì—…ì´ìµ, ìˆœì´ìµ, ìì‚°, ë¶€ì±„, ìë³¸).
- `funding_history`: íˆ¬ì ìœ ì¹˜ ë‚´ì—­.
- `financial_plan`: í–¥í›„ ìê¸ˆ ê³„íš.

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
```{context}```

**JSON ìŠ¤í‚¤ë§ˆ:**
```json
{{
    "properties": {{
        "financial_summary": {{
            "type": "array",
            "items": {{
                "type": "object",
                "properties": {{
                    "year": {{"type": "string"}},
                    "revenue": {{"type": "string"}},
                    "operating_profit": {{"type": "string"}},
                    "net_income": {{"type": "string"}},
                    "assets": {{"type": "string"}},
                    "liabilities": {{"type": "string"}},
                    "equity": {{"type": "string"}}
                }}
            }}
        }},
        "funding_history": {{"type": "string"}},
        "financial_plan": {{"type": "string"}}
    }}
}}
```
/no_think""",
            "ê¸°ìˆ  ë° ì œí’ˆ": """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•˜ëŠ” AIì…ë‹ˆë‹¤. ì ˆëŒ€ë¡œ ìš”ì•½ì´ë‚˜ ì„¤ëª…ì„ ìƒì„±í•˜ì§€ ë§ˆì‹œì˜¤.
ì‘ë‹µì€ ì•„ë˜ì— ëª…ì‹œëœ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

**ì¶”ì¶œí•  ì •ë³´:**
- í•µì‹¬ ê¸°ìˆ , ë³´ìœ  íŠ¹í—ˆ, ì£¼ìš” ì œí’ˆ ë¼ì¸ì—…, ê¸°ìˆ ì  ê²½ìŸ ìš°ìœ„.

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
```{context}```

**JSON ìŠ¤í‚¤ë§ˆ:**
```json
{{
    "properties": {{
        "core_technologies": {{"type": "array", "items": {{"type": "string"}}, "description": "í•µì‹¬ ê¸°ìˆ  ëª©ë¡"}},
        "patents": {{"type": "array", "items": {{"type": "string"}}, "description": "ë³´ìœ  íŠ¹í—ˆ ëª©ë¡"}},
        "product_lineup": {{"type": "array", "items": {{"type": "string"}}, "description": "ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤ ë¼ì¸ì—…"}},
        "tech_advantage": {{"type": "string", "description": "ê²½ìŸì‚¬ ëŒ€ë¹„ ê¸°ìˆ ì  ìš°ìœ„ ìš”ì•½"}}
    }}
}}
```
/no_think""",
            "ì‚¬ì—… ë° ì‹œì¥ ì „ëµ": """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•˜ëŠ” AIì…ë‹ˆë‹¤. ì ˆëŒ€ë¡œ ìš”ì•½ì´ë‚˜ ì„¤ëª…ì„ ìƒì„±í•˜ì§€ ë§ˆì‹œì˜¤.
ì‘ë‹µì€ ì•„ë˜ì— ëª…ì‹œëœ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

**ì¶”ì¶œí•  ì •ë³´:**
- íƒ€ê²Ÿ ì‹œì¥, ì£¼ìš” ê²½ìŸì‚¬, ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸, í–¥í›„ í™•ì¥ ì „ëµ.

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
```{context}```

**JSON ìŠ¤í‚¤ë§ˆ:**
```json
{{
    "properties": {{
        "target_market": {{"type": "string", "description": "íƒ€ê²Ÿ ì‹œì¥ì˜ ê·œëª¨ ë° íŠ¹ì§•"}},
        "competitors": {{"type": "array", "items": {{"type": "string"}}, "description": "ì£¼ìš” ê²½ìŸì‚¬ ëª©ë¡"}},
        "business_model": {{"type": "string", "description": "ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ë° ì£¼ìš” ìˆ˜ìµì›"}},
        "scale_up_strategy": {{"type": "string", "description": "í–¥í›„ ì‚¬ì—…í™” ë˜ëŠ” í™•ì¥ ì „ëµ"}}
    }}
}}
```
/no_think"""
        }
        detailed_info = {}
        for theme, headers in grouped_headers.items():
            if theme not in extraction_prompts or not headers: continue
            logger.info(f"  - ì‹¬ì¸µ ë¶„ì„ ì¤‘... ({theme})")
            context = "\n\n---\n\n".join(chunk_map.get(h, '') for h in headers if h in chunk_map)
            if not context.strip():
                logger.warning(f"  - '{theme}' ì£¼ì œì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                detailed_info[theme] = {}
                continue
            template = extraction_prompts[theme]
            prompt = PromptTemplate.from_template(template)
            chain = prompt | self.llm | StrOutputParser()
            try:
                raw_output = chain.invoke({"context": context})
                extracted_data = self._clean_and_parse_json(raw_output)
                if extracted_data is not None:
                    detailed_info[theme] = extracted_data
                else:
                    detailed_info[theme] = {"error": "LLM ì¶œë ¥ì—ì„œ JSONì„ ì°¾ì§€ ëª»í•¨"}
            except Exception as e:
                logger.error(f"  - '{theme}' ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                detailed_info[theme] = {"error": str(e)}
        logger.info("ì‹¬ì¸µ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ.")
        return detailed_info

    def analyze(self, md_file_path: str) -> Dict[str, Any]:
        logger.info(f"ë¶„ì„ ì‹œì‘: {md_file_path}")
        try:
            md_content = Path(md_file_path).read_text(encoding='utf-8')
        except FileNotFoundError:
            logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {md_file_path}")
            return {}
        chunks = self._split_md_by_headers(md_content)
        indexed_chunks = self._generate_chunk_summaries(chunks)
        grouped_headers = self._group_chunks_by_theme(indexed_chunks)
        final_analysis = self._extract_detailed_info(grouped_headers, chunks)
        logger.info("ë¶„ì„ ì™„ë£Œ.")
        return {
            "file_info": {"path": md_file_path},
            "analysis_result": final_analysis,
            "debug_info": {"total_chunks": len(chunks), "grouped_headers": grouped_headers}
        }

    def save_results(self, analysis_result: Dict[str, Any], output_dir: str, file_stem: str):
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        json_path = output_path / f"{file_stem}_vision_analysis.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_result, f, ensure_ascii=False, indent=2)
        logger.info(f"JSON ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {json_path}")
        md_path = output_path / f"{file_stem}_vision_summary.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(f"# {file_stem} Vision IR ë¶„ì„ ë³´ê³ ì„œ\n\n")
            data = analysis_result.get("analysis_result", {})
            for theme, content in data.items():
                f.write(f"## ğŸ“Š {theme}\n\n")
                if isinstance(content, dict):
                    for key, value in content.items():
                        f.write(f"- **{key}**: {json.dumps(value, ensure_ascii=False, indent=2)}\n")
                elif isinstance(content, list):
                    for item in content:
                        f.write("- ---\n")
                        if isinstance(item, dict):
                            for key, value in item.items():
                                f.write(f"  - **{key}**: {value}\n")
                        else:
                            f.write(f"  - {item}\n")
                else:
                    f.write(f"{content}\n")
                f.write("\n")
        logger.info(f"Markdown ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {md_path}")

def main():
    parser = argparse.ArgumentParser(description="Vision-LLMìœ¼ë¡œ ìƒì„±ëœ IR ë§ˆí¬ë‹¤ìš´ ë¶„ì„ê¸°")
    parser.add_argument("md_path", type=str, help="ë¶„ì„í•  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì˜ ê²½ë¡œ")
    parser.add_argument("--output_dir", type=str, default="analysis_results", help="ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë  ë””ë ‰í† ë¦¬")
    parser.add_argument("--model", type=str, default=MODEL_ID, help="Ollama ëª¨ë¸ ID")
    parser.add_argument("--url", type=str, default=BASE_URL, help="Ollama ì„œë²„ URL")
    args = parser.parse_args()
    if not OLLAMA_AVAILABLE:
        logger.error("í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ì„¤ì¹˜ ì•ˆë‚´ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return
    analyzer = VisionMDAnalyzer(model_id=args.model, base_url=args.url)
    if not analyzer.use_llm:
        logger.error("LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    results = analyzer.analyze(args.md_path)
    if results:
        file_stem = Path(args.md_path).stem
        analyzer.save_results(results, args.output_dir, file_stem)
        print("\n" + "="*80)
        print("âœ… ë¶„ì„ ë° ê²°ê³¼ ì €ì¥ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“ JSON ê²°ê³¼: {Path(args.output_dir) / f'{file_stem}_vision_analysis.json'}")
        print(f"ğŸ“„ MD ë³´ê³ ì„œ: {Path(args.output_dir) / f'{file_stem}_vision_summary.md'}")
        print("="*80)

if __name__ == "__main__":
    main()