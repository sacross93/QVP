#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Vision-LLMìœ¼ë¡œ ìƒì„±ëœ IR ë§ˆí¬ë‹¤ìš´ ë¶„ì„ê¸° (v5 - ìµœì¢… ì•ˆì •í™”)

'ê³„ì¸µì  ë¶„ì„ ë° ì¢…í•© íŒŒì´í”„ë¼ì¸'ì„ ì‚¬ìš©í•˜ì—¬ íˆ¬ì ë¶„ì„ì— í•„ìš”í•œ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
LLMì˜ ì¶œë ¥ì—ì„œ <think> ë¸”ë¡ì„ ì •ê·œì‹ìœ¼ë¡œ ì œê±°í•œ í›„ JSON íŒŒì‹±ì„ ìˆ˜í–‰í•˜ì—¬ ì•ˆì •ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
"""

import re
import json
import logging
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional

# ë¡œì»¬ LLM ì—°ë™ (langchain_ollama)
try:
    from langchain_ollama import ChatOllama
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import PromptTemplate
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    print("âš ï¸ langchain_ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install langchain-ollama langchain-core ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
    class ChatOllama: pass
    class StrOutputParser: pass
    class PromptTemplate: pass

# --- ì„¤ì • ---
MODEL_ID = "qwen3:32b" # ëª¨ë¸ IDëŠ” í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš” (ì˜ˆ: "qwen2:32b")
BASE_URL = "http://192.168.120.102:11434" # Ollama ì„œë²„ ì£¼ì†Œ

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class VisionMDAnalyzer:
    """
    Vision LLMì´ ìƒì„±í•œ ë§ˆí¬ë‹¤ìš´(.md) íŒŒì¼ì„ ë¶„ì„í•˜ì—¬
    í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  êµ¬ì¡°í™”í•˜ëŠ” í´ë˜ìŠ¤. (ìµœì¢… ì•ˆì •í™” ë²„ì „)
    """

    def __init__(self, model_id: str = MODEL_ID, base_url: str = BASE_URL):
        if not OLLAMA_AVAILABLE:
            logger.error("í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self.use_llm = False
            return
        try:
            self.llm = ChatOllama(model=model_id, base_url=base_url, temperature=0)
            self.use_llm = True
            logger.info(f"LLM ì´ˆê¸°í™” ì™„ë£Œ: {model_id} at {base_url}")
        except Exception as e:
            logger.error(f"LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. LLM ê¸°ëŠ¥ ì—†ì´ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            self.use_llm = False

    def _clean_and_parse_json(self, raw_output: str) -> Optional[Any]:
        """
        LLMì˜ ì›ì‹œ ì¶œë ¥ì—ì„œ <think> ë¸”ë¡ì„ ì œê±°í•˜ê³ , ë‚¨ì€ ë¬¸ìì—´ì„ JSONìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
        """
        try:
            # <think>...</think> ë¸”ë¡ì„ re.subìœ¼ë¡œ ì œê±°í•©ë‹ˆë‹¤.
            cleaned_output = re.sub(r'<think>.*?</think>', '', raw_output, flags=re.DOTALL).strip()
            
            # ì •ì œëœ ë¬¸ìì—´ì—ì„œ JSON ê°ì²´ ë˜ëŠ” ë°°ì—´ì„ ì°¾ìŠµë‹ˆë‹¤.
            match = re.search(r'(\{.*\}|\[.*\])', cleaned_output, re.DOTALL)
            if match:
                json_string = match.group(0)
                return json.loads(json_string)
            else:
                logger.warning(f"ì •ì œëœ ì¶œë ¥ì—ì„œ JSONì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. LLM ì¶œë ¥:\n---\n{cleaned_output}\n---")
                return None
        except json.JSONDecodeError as e:
            logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}\nì›ì‹œ ì¶œë ¥ ì¼ë¶€: {raw_output[:300]}...")
            return None
        except Exception as e:
            logger.error(f"JSON ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None

    def _split_md_by_headers(self, md_content: str) -> List[Dict[str, str]]:
        logger.info("1ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ êµ¬ì¡°ì  ë¶„í•  ì‹œì‘...")
        chunks = re.split(r'\n\s*\n(?=^#{1,2} )', md_content.strip(), flags=re.MULTILINE)
        structured_chunks = []
        for chunk in chunks:
            if not chunk.strip(): continue
            lines = chunk.strip().split('\n')
            header = lines[0].strip()
            content = '\n'.join(lines[1:]).strip()
            if not header.startswith('#'):
                header = "# ì„œë¬¸"
                content = chunk.strip()
            structured_chunks.append({'header': header, 'content': content})
        logger.info(f"ë¶„í•  ì™„ë£Œ: {len(structured_chunks)}ê°œì˜ êµ¬ì¡°ì  ì„¹ì…˜ ìƒì„±.")
        return structured_chunks

    def _generate_chunk_summaries(self, chunks: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        if not self.use_llm: return [{**c, 'summary': '', 'keywords': []} for c in chunks]
        logger.info("2ë‹¨ê³„: ì„¹ì…˜ë³„ ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œ (ì¸ë±ì‹±) ì‹œì‘...")
        template = """ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ê³ , ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ 5ê°œë§Œ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•´ì¤˜.\n\n--- í…ìŠ¤íŠ¸ ì‹œì‘ ---\n{text}\n--- í…ìŠ¤íŠ¸ ë ---\n\nê²°ê³¼ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´ì¤˜.\nìš”ì•½: [ì—¬ê¸°ì— ìš”ì•½]\ní‚¤ì›Œë“œ: [ì—¬ê¸°ì— í‚¤ì›Œë“œ]\n/no_think\n"""
        prompt = PromptTemplate.from_template(template)
        chain = prompt | self.llm | StrOutputParser()
        indexed_chunks = []
        for i, chunk in enumerate(chunks):
            logger.info(f"  - ì¸ë±ì‹± ì²˜ë¦¬ ì¤‘... ({i+1}/{len(chunks)}) : {chunk['header']}")
            try:
                response = chain.invoke({"text": chunk['content']})
                summary_match = re.search(r"ìš”ì•½:\s*(.*)", response)
                keywords_match = re.search(r"í‚¤ì›Œë“œ:\s*(.*)", response)
                summary = summary_match.group(1).strip() if summary_match else "ìš”ì•½ ì‹¤íŒ¨"
                keywords = [kw.strip() for kw in keywords_match.group(1).strip().split(',')] if keywords_match else []
                indexed_chunks.append({**chunk, 'summary': summary, 'keywords': keywords})
            except Exception as e:
                logger.warning(f"  - ì¸ë±ì‹± ì‹¤íŒ¨: {chunk['header']}. ì›ë³¸ ë‚´ìš©ë§Œ ì‚¬ìš©. ì˜¤ë¥˜: {e}")
                indexed_chunks.append({**chunk, 'summary': '', 'keywords': []})
        logger.info("ì¸ë±ì‹± ì™„ë£Œ.")
        return indexed_chunks

    def _group_chunks_by_theme(self, indexed_chunks: List[Dict[str, Any]]) -> Dict[str, List[str]]:
        if not self.use_llm: return {}
        logger.info("3-1ë‹¨ê³„: ì£¼ì œë³„ ê·¸ë£¹í™” ì‹œì‘...")
        summaries_text = "\n".join(f"- í—¤ë”: {chunk['header']}\n  ìš”ì•½: {chunk.get('summary', '')}" for chunk in indexed_chunks if chunk.get('summary'))
        template = """ë‹¹ì‹ ì€ íˆ¬ì ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì€ í•œ IR ë¬¸ì„œì˜ ì„¹ì…˜ë³„ í—¤ë”ì™€ ìš”ì•½ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ê° í—¤ë”ë¥¼ ì•„ë˜ 6ê°€ì§€ ì£¼ì œë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.

**ë¶„ë¥˜ ê¸°ì¤€:**
- íšŒì‚¬ ê°œìš”: íšŒì‚¬ëª…, ì„¤ë¦½ì¼, ëŒ€í‘œì, ì‚¬ì—… ë¶„ì•¼, íšŒì‚¬ ì†Œê°œ, ì—°ë½ì²˜
- ê²½ì˜ì§„ ë° ì¡°ì§: íŒ€ êµ¬ì„±ì›, ê²½ì˜ì§„ í”„ë¡œí•„, ì¡°ì§ë„
- ì¬ë¬´ í˜„í™©: ë§¤ì¶œ, íˆ¬ìê¸ˆ, ìê¸ˆ ê³„íš, ì¬ë¬´ ì§€í‘œ
- ê¸°ìˆ  ë° ì œí’ˆ: í•µì‹¬ ê¸°ìˆ , ì œí’ˆ ë¼ì¸ì—…, íŠ¹í—ˆ, ì„ìƒì‹œí—˜, ê²€ì‚¬ ê¸°ìˆ 
- ì‚¬ì—… ë° ì‹œì¥ ì „ëµ: ì‹œì¥ ë¶„ì„, ê²½ìŸì‚¬, ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸, ê¸€ë¡œë²Œ ì§„ì¶œ
- ê¸°íƒ€: ìœ„ ì¹´í…Œê³ ë¦¬ì— ë§ì§€ ì•ŠëŠ” ë‚´ìš©

**ì„¹ì…˜ë³„ í—¤ë” ë¦¬ìŠ¤íŠ¸:**
---
{summaries}
---

**ì¤‘ìš”: ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”:**
{{
  "íšŒì‚¬ ê°œìš”": ["í—¤ë”1", "í—¤ë”2"],
  "ê²½ì˜ì§„ ë° ì¡°ì§": ["í—¤ë”3"],
  "ì¬ë¬´ í˜„í™©": ["í—¤ë”4"],
  "ê¸°ìˆ  ë° ì œí’ˆ": ["í—¤ë”5"],
  "ì‚¬ì—… ë° ì‹œì¥ ì „ëµ": ["í—¤ë”6"],
  "ê¸°íƒ€": ["í—¤ë”7"]
}}

/no_think
"""
        prompt = PromptTemplate.from_template(template)
        chain = prompt | self.llm | StrOutputParser()
        all_keys = ["íšŒì‚¬ ê°œìš”", "ê²½ì˜ì§„ ë° ì¡°ì§", "ì¬ë¬´ í˜„í™©", "ê¸°ìˆ  ë° ì œí’ˆ", "ì‚¬ì—… ë° ì‹œì¥ ì „ëµ", "ê¸°íƒ€"]
        final_groups = {key: [] for key in all_keys}
        try:
            logger.info(f"ì£¼ì œë³„ ê·¸ë£¹í™”ë¥¼ ìœ„í•´ {len(indexed_chunks)}ê°œ ì„¹ì…˜ ì²˜ë¦¬ ì¤‘...")
            raw_output = chain.invoke({"summaries": summaries_text})
            logger.info(f"LLM ì‘ë‹µ ê¸¸ì´: {len(raw_output)} ë¬¸ì")
            
            response_data = self._clean_and_parse_json(raw_output)
            
            if isinstance(response_data, dict):
                for key, value in response_data.items():
                    if key in final_groups and isinstance(value, list):
                        final_groups[key].extend(value)
                        logger.info(f"ì£¼ì œ '{key}': {len(value)}ê°œ í—¤ë” í• ë‹¹")
                total_assigned = sum(len(headers) for headers in final_groups.values())
                logger.info(f"ì´ {total_assigned}ê°œ í—¤ë”ê°€ ì£¼ì œë³„ë¡œ í• ë‹¹ë¨")
            else:
                logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨. í´ë°± ë¶„ë¥˜ ì‹œë„...")
                final_groups = self._fallback_grouping(indexed_chunks)
                
            # ê²°ê³¼ ê²€ì¦
            if all(not headers for headers in final_groups.values()):
                logger.error("ëª¨ë“  ì£¼ì œê°€ ë¹„ì–´ìˆìŒ. í´ë°± ë¶„ë¥˜ ì‹œë„...")
                final_groups = self._fallback_grouping(indexed_chunks)
                
            logger.info("ì£¼ì œë³„ ê·¸ë£¹í™” ì™„ë£Œ.")
        except Exception as e:
            logger.error(f"ì£¼ì œë³„ ê·¸ë£¹í™” ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}. í´ë°± ë¶„ë¥˜ ì‹œë„...")
            final_groups = self._fallback_grouping(indexed_chunks)
            
        return final_groups

    def _fallback_grouping(self, indexed_chunks: List[Dict[str, Any]]) -> Dict[str, List[str]]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ê·¸ë£¹í™”"""
        logger.info("í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ê·¸ë£¹í™” ì‹œì‘...")
        
        # ì£¼ì œë³„ í‚¤ì›Œë“œ ë§¤í•‘ (ê°œì„ ëœ ë²„ì „)
        keyword_mapping = {
            "íšŒì‚¬ ê°œìš”": ["íšŒì‚¬", "ì‹­ì¼ë¦¬í„°", "ì£¼ì‹íšŒì‚¬", "ê°œìš”", "ì†Œê°œ", "ì—°ë½ì²˜", "ëŒ€í‘œ", "ì„¤ë¦½", "ë³¸ì‚¬", "ìœ„ì¹˜", "lifet", "robos", "ë¡œë³´ìŠ¤", "ì¼ë°˜ í˜„í™©", "ì°½ì—…ê¸°ì—…"],
            "ê²½ì˜ì§„ ë° ì¡°ì§": ["íŒ€", "ê²½ì˜ì§„", "ceo", "cto", "coo", "cfo", "ëŒ€í‘œ", "ì¡°ì§", "êµ¬ì„±ì›", "ì„ì›", "ì§ì›", "íŒ€ êµ¬ì„±ì›", "ë³¸ë¶€ì¥", "ì—°êµ¬ì›", "ì±…ì„", "ì„ ì„", "ë§ˆì¼€í„°", "ë””ìì´ë„ˆ", "ê°œë°œì", "ìˆ˜ì˜ì‚¬", "director"],
            "ì¬ë¬´ í˜„í™©": ["ë§¤ì¶œ", "íˆ¬ì", "ìê¸ˆ", "ì¬ë¬´", "ìˆ˜ìµ", "ë¹„ìš©", "íˆ¬ìê¸ˆ", "ê³„íš", "ì˜ˆì‚°", "billion", "million", "ì–µ", "ë§Œ", "ì›"],
            "ê¸°ìˆ  ë° ì œí’ˆ": ["ê¸°ìˆ ", "ì œí’ˆ", "ai", "ê²€ì‚¬", "ì§„ë‹¨", "íŠ¹í—ˆ", "ê°œë°œ", "ì„ìƒ", "ì‹œí—˜", "ì •í™•ë„", "ì•Œê³ ë¦¬ì¦˜", "vision", "ë¡œë´‡", "ìë™í™”", "ë¨¸ì‹ ë¹„ì „", "ë”¥ëŸ¬ë‹", "ìƒì²´", "ë„ì¶•"],
            "ì‚¬ì—… ë° ì‹œì¥ ì „ëµ": ["ì‹œì¥", "ì‚¬ì—…", "ì „ëµ", "ê²½ìŸ", "ê¸€ë¡œë²Œ", "ì§„ì¶œ", "í™ˆì¼€ì–´", "ë°˜ë ¤ë™ë¬¼", "í«", "ë¹„ì¦ˆë‹ˆìŠ¤", "í™•ëŒ€", "ìŠ¤ì¼€ì¼ì—…", "ì‚¬ì—…í™”", "ë„ì¶•ì¥", "ê·œëª¨"],
            "ê¸°íƒ€": ["chapter", "ë¶€ë¡", "ì°¸ê³ ", "ê¸°íƒ€", "ìƒì„¸", "ì¶”ê°€", "ë¬¸ì œì ", "í•´ê²°ë°©ì•ˆ"]
        }
        
        all_keys = ["íšŒì‚¬ ê°œìš”", "ê²½ì˜ì§„ ë° ì¡°ì§", "ì¬ë¬´ í˜„í™©", "ê¸°ìˆ  ë° ì œí’ˆ", "ì‚¬ì—… ë° ì‹œì¥ ì „ëµ", "ê¸°íƒ€"]
        final_groups = {key: [] for key in all_keys}
        
        for chunk in indexed_chunks:
            header = chunk['header'].lower()
            summary = chunk.get('summary', '').lower()
            content = f"{header} {summary}"
            
            assigned = False
            for theme, keywords in keyword_mapping.items():
                if any(keyword in content for keyword in keywords):
                    final_groups[theme].append(chunk['header'])
                    assigned = True
                    break
            
            if not assigned:
                final_groups["ê¸°íƒ€"].append(chunk['header'])
        
        total_assigned = sum(len(headers) for headers in final_groups.values())
        logger.info(f"í´ë°± ê·¸ë£¹í™” ì™„ë£Œ: ì´ {total_assigned}ê°œ í—¤ë” í• ë‹¹")
        return final_groups

    def _extract_detailed_info(self, grouped_headers: Dict[str, List[str]], original_chunks: List[Dict[str, str]]) -> Dict[str, Any]:
        if not self.use_llm: return {}
        logger.info("3-2ë‹¨ê³„: ì£¼ì œë³„ ì‹¬ì¸µ ì •ë³´ ì¶”ì¶œ ì‹œì‘...")
        chunk_map = {chunk['header']: chunk['content'] for chunk in original_chunks}
        
        # í—¤ë” ë§¤ì¹­ì„ ìœ„í•œ í¬ê´„ì  ê²€ìƒ‰ í•¨ìˆ˜
        def find_matching_content(target_header: str) -> str:
            """ì£¼ì–´ì§„ í—¤ë”ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ ë‚´ìš©ì„ ì°¾ê³  ê´€ë ¨ ì •ë³´ë¥¼ í†µí•©í•˜ëŠ” í•¨ìˆ˜"""
            logger.info(f"í—¤ë” ë§¤ì¹­ ì‹œì‘: '{target_header}'")
            
            # ì£¼ì œë³„ í‚¤ì›Œë“œ ì •ì˜ (ë²”ìš©ì  íŒ¨í„´)
            theme_keywords = {
                "ê²½ì˜ì§„": ["ê²½ì˜ì§„", "íŒ€", "êµ¬ì„±ì›", "ì¡°ì§", "ì°½ì—…ì", "ëŒ€í‘œ", "ì´ì‚¬", "ì „ë¬´", "íŒ€ì¥", 
                         "ceo", "cto", "cfo", "coo", "ì„ì›", "ì§ì›", "founder", "team", "member", "organization"],
                "íšŒì‚¬ê°œìš”": ["íšŒì‚¬", "ê°œìš”", "ì†Œê°œ", "ì„¤ë¦½", "ë²•ì¸", "company", "overview", "ì •ë³´", "who we are"],
                "ì¬ë¬´": ["ë§¤ì¶œ", "íˆ¬ì", "ìê¸ˆ", "ì¬ë¬´", "ì£¼ì£¼", "ì§€ë¶„", "ìœ ì¹˜", "financial", "revenue", "funding", "seed", "series"],
                "ê¸°ìˆ ": ["ê¸°ìˆ ", "ì œí’ˆ", "íŠ¹í—ˆ", "ê°œë°œ", "ì†”ë£¨ì…˜", "ê¸°ìˆ ë ¥", "technology", "product", "patent"],
                "ì‚¬ì—…": ["ì‚¬ì—…", "ì‹œì¥", "ì „ëµ", "ë¹„ì¦ˆë‹ˆìŠ¤", "ê³ ê°", "íŒŒíŠ¸ë„ˆ", "business", "market", "strategy"]
            }
            
            # 1. ì •í™•í•œ ë§¤ì¹­ ì‹œë„
            if target_header in chunk_map:
                logger.info(f"ì •í™•í•œ ë§¤ì¹­ ë°œê²¬: {target_header} (ê¸¸ì´: {len(chunk_map[target_header])})")
                return chunk_map[target_header]
            
            # 2. ë¶€ë¶„ ë§¤ì¹­ ì‹œë„ (ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ)
            target_lower = target_header.lower().strip()
            for header, content in chunk_map.items():
                if target_lower in header.lower() or header.lower() in target_lower:
                    return content
            
            # 3. í‚¤ì›Œë“œ ë§¤ì¹­ ì‹œë„
            target_keywords = target_header.lower().replace('#', '').replace('-', '').split()
            target_keywords = [kw.strip() for kw in target_keywords if kw.strip()]
            
            for header, content in chunk_map.items():
                header_lower = header.lower()
                if any(keyword in header_lower for keyword in target_keywords):
                    return content
            
            # 4. ì£¼ì œë³„ í†µí•© ê²€ìƒ‰ (ê²½ì˜ì§„/íŒ€ ì •ë³´ì— íŠ¹í™”)
            current_theme = None
            for theme, keywords in theme_keywords.items():
                if any(kw in target_lower for kw in keywords):
                    current_theme = theme
                    break
            
            logger.info(f"ëŒ€ìƒ í—¤ë”: {target_header}, ê°ì§€ëœ í…Œë§ˆ: {current_theme}")
            
            if current_theme:
                matching_contents = []
                theme_keywords_list = theme_keywords[current_theme]
                
                # ëª¨ë“  ê´€ë ¨ ì„¹ì…˜ì„ ì°¾ì•„ì„œ í†µí•©
                for header, content in chunk_map.items():
                    header_lower = header.lower()
                    if any(kw in header_lower for kw in theme_keywords_list):
                        # ë‚´ìš©ì´ ì˜ë¯¸ìˆëŠ”ì§€ í™•ì¸ (ë‹¨ìˆœ ì œëª©ì´ ì•„ë‹Œ)
                        if content and len(content.strip()) > 50:
                            matching_contents.append(content)
                
                # íŠ¹ë³„íˆ ê²½ì˜ì§„ ì •ë³´ì˜ ê²½ìš°, ë” ì„¸ë°€í•œ ê²€ìƒ‰ ìˆ˜í–‰
                if "ê²½ì˜ì§„" in current_theme:
                    logger.info(f"ê²½ì˜ì§„ í…Œë§ˆ ê°ì§€: {current_theme}, ì„¸ë°€í•œ ê²€ìƒ‰ ì‹œì‘...")
                    
                    # êµ¬ì¡°ì  íŒ¨í„´ ì¸ì‹: #### ì´ë¦„ ì§ì±… í˜•íƒœ
                    position_patterns = ["ëŒ€í‘œì´ì‚¬", "ì „ë¬´", "ì´ì‚¬", "íŒ€ì¥", "ceo", "cto", "cfo", "coo"]
                    for header, content in chunk_map.items():
                        # ë ˆë²¨ 4 í—¤ë”ì—ì„œ ì§ì±… íŒ¨í„´ ê²€ìƒ‰
                        if header.startswith("#### ") and any(pos in header.lower() for pos in position_patterns):
                            if content and len(content.strip()) > 20:  # ì§§ì€ í”„ë¡œí•„ë„ í—ˆìš©
                                logger.info(f"ì§ì±… íŒ¨í„´ ë§¤ì¹­: {header} (ê¸¸ì´: {len(content)})")
                                if content not in matching_contents:
                                    matching_contents.append(content)
                    
                    # ê¸°ì¡´ íŒ¨í„´ ê²€ìƒ‰
                    specific_patterns = ["íŒ€ êµ¬ì„±ì›", "ì¡°ì§ êµ¬ì„±", "êµ¬ì„±ì›", "ê²½ì˜ì§„", "organization", "team member", "ì£¼ì£¼"]
                    for pattern in specific_patterns:
                        for header, content in chunk_map.items():
                            if pattern in header.lower() and content and len(content.strip()) > 100:
                                logger.info(f"íŒ¨í„´ '{pattern}' ë§¤ì¹­: {header} (ê¸¸ì´: {len(content)})")
                                if content not in matching_contents:
                                    matching_contents.append(content)
                    
                    # ë‹¤ì¸µ í—¤ë” í™•ì¸ (ë ˆë²¨ 2-3)
                    for header, content in chunk_map.items():
                        if (header.startswith("## ") or header.startswith("### ")) and \
                           any(kw in header.lower() for kw in ["íŒ€", "êµ¬ì„±ì›", "ì¡°ì§", "ê²½ì˜ì§„", "ì£¼ì£¼"]) and \
                           content and len(content.strip()) > 50:
                            logger.info(f"ë‹¤ì¸µ í—¤ë” ë§¤ì¹­: {header} (ê¸¸ì´: {len(content)})")
                            if content not in matching_contents:
                                matching_contents.append(content)
                    
                    logger.info(f"ê²½ì˜ì§„ ì„¸ë°€ ê²€ìƒ‰ ì™„ë£Œ: {len(matching_contents)}ê°œ ì»¨í…ì¸  ë°œê²¬")
                
                # ì—¬ëŸ¬ ì„¹ì…˜ì˜ ë‚´ìš©ì„ í†µí•©
                if matching_contents:
                    return "\n\n---\n\n".join(matching_contents)
            
            return ""
        extraction_prompts = {
            "íšŒì‚¬ ê°œìš”": """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•˜ëŠ” AIì…ë‹ˆë‹¤. ì ˆëŒ€ë¡œ ìš”ì•½ì´ë‚˜ ì„¤ëª…ì„ ìƒì„±í•˜ì§€ ë§ˆì‹œì˜¤.
ì‘ë‹µì€ ì•„ë˜ì— ëª…ì‹œëœ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

**ì¶”ì¶œí•  ì •ë³´:**
- íšŒì‚¬ëª… (corporation_name)
- ì„¤ë¦½ì¼ (foundation_date)
- ëŒ€í‘œì (ceo)
- ì£¼ìš” ì‚¬ì—… ë¶„ì•¼ (main_business)
- ë³¸ì‚¬ ë° ì£¼ìš” ì‹œì„¤ ìœ„ì¹˜ (locations)
- íšŒì‚¬ ì—°í˜ì˜ ì£¼ìš” ë§ˆì¼ìŠ¤í†¤ (history_milestones)

**ì¤‘ìš”: ë¶„ì‚°ëœ ì •ë³´ í†µí•© ê°€ì´ë“œ:**
- "WHO WE ARE", "íšŒì‚¬ ì •ë³´", "ë²•ì¸ì„¤ë¦½" ë“± ë‹¤ì–‘í•œ ì„¹ì…˜ì—ì„œ ì •ë³´ë¥¼ ì°¾ìœ¼ì„¸ìš”
- ì—°í˜ í…Œì´ë¸”ì´ë‚˜ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì„¤ë¦½ì¼ê³¼ ì£¼ìš” ì´ë²¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”
- íšŒì‚¬ ì†Œê°œê¸€ì—ì„œ ì‚¬ì—… ë¶„ì•¼ì™€ ìœ„ì¹˜ ì •ë³´ë¥¼ íŒŒì•…í•˜ì„¸ìš”

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
```{context}```

**JSON ìŠ¤í‚¤ë§ˆ:**
```json
{{
    "properties": {{
        "corporation_name": {{"type": "string", "description": "íšŒì‚¬ëª…"}},
        "foundation_date": {{"type": "string", "description": "ì„¤ë¦½ì¼"}},
        "ceo": {{"type": "string", "description": "ëŒ€í‘œì ì´ë¦„"}},
        "main_business": {{"type": "string", "description": "ì£¼ìš” ì‚¬ì—… ë¶„ì•¼"}},
        "locations": {{"type": "array", "items": {{"type": "string"}}, "description": "ë³¸ì‚¬, ì—°êµ¬ì†Œ ë“± ì£¼ìš” ìœ„ì¹˜ ëª©ë¡"}},
        "history_milestones": {{"type": "array", "items": {{"type": "string"}}, "description": "íšŒì‚¬ ì—°í˜ì˜ ì£¼ìš” ì´ë²¤íŠ¸ ëª©ë¡"}}
    }},
    "required": ["corporation_name", "foundation_date", "ceo", "main_business"]
}}
```

/no_think""",
            "ê²½ì˜ì§„ ë° ì¡°ì§": """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•˜ëŠ” AIì…ë‹ˆë‹¤. ì ˆëŒ€ë¡œ ìš”ì•½ì´ë‚˜ ì„¤ëª…ì„ ìƒì„±í•˜ì§€ ë§ˆì‹œì˜¤.
ì‘ë‹µì€ ì•„ë˜ì— ëª…ì‹œëœ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

**ì¶”ì¶œí•  ì •ë³´:**
- ê° ê²½ì˜ì§„ì˜ ì´ë¦„(name), ì§ì±…(position), ìµœì¢… í•™ë ¥(education), ì£¼ìš” ê²½ë ¥(career). ëª¨ë“  ì¸ë¬¼ì„ ë¹ ì§ì—†ì´ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

**ì¤‘ìš”: ë‹¤ì–‘í•œ í˜•ì‹ ì²˜ë¦¬ ê°€ì´ë“œ:**
- "#### ë°•í˜¸ì˜ ëŒ€í‘œì´ì‚¬" í˜•íƒœì˜ í—¤ë”ì—ì„œ ì´ë¦„ê³¼ ì§ì±…ì„ ë¶„ë¦¬í•˜ì„¸ìš”
- ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ì´ ìˆë‹¤ë©´ ê° í–‰ì˜ ì •ë³´ë¥¼ ê°œë³„ ì¸ë¬¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”
- ì£¼ì£¼ ì •ë³´ í…Œì´ë¸”ì—ì„œë„ ê²½ì˜ì§„ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ í¬í•¨í•˜ì„¸ìš”
- ì§§ì€ í”„ë¡œí•„ì´ë¼ë„ ì´ë¦„ê³¼ ì§ì±…ì´ ëª…í™•í•˜ë©´ í¬í•¨í•˜ì„¸ìš”

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
```{context}```

**JSON ìŠ¤í‚¤ë§ˆ:**
```json
{{
    "type": "array",
    "items": {{
        "type": "object",
        "properties": {{
            "name": {{"type": "string", "description": "ì´ë¦„"}},
            "position": {{"type": "string", "description": "ì§ì±…"}},
            "education": {{"type": "string", "description": "ìµœì¢… í•™ë ¥"}},
            "career": {{"type": "string", "description": "ì£¼ìš” ê²½ë ¥"}}
        }},
        "required": ["name", "position"]
    }}
}}
```
/no_think""",
            "ì¬ë¬´ í˜„í™©": """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•˜ëŠ” AIì…ë‹ˆë‹¤. ì ˆëŒ€ë¡œ ìš”ì•½ì´ë‚˜ ì„¤ëª…ì„ ìƒì„±í•˜ì§€ ë§ˆì‹œì˜¤.
ì‘ë‹µì€ ì•„ë˜ì— ëª…ì‹œëœ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

**ì¶”ì¶œí•  ì •ë³´:**
- `financial_summary`: ì—°ë„ë³„ ì¬ë¬´ ì§€í‘œ (ë§¤ì¶œ, ì˜ì—…ì´ìµ, ìˆœì´ìµ, ìì‚°, ë¶€ì±„, ìë³¸).
- `funding_history`: íˆ¬ì ìœ ì¹˜ ë‚´ì—­.
- `financial_plan`: í–¥í›„ ìê¸ˆ ê³„íš.

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
```{context}```

**JSON ìŠ¤í‚¤ë§ˆ:**
```json
{{
    "properties": {{
        "financial_summary": {{
            "type": "array",
            "items": {{
                "type": "object",
                "properties": {{
                    "year": {{"type": "string"}},
                    "revenue": {{"type": "string"}},
                    "operating_profit": {{"type": "string"}},
                    "net_income": {{"type": "string"}},
                    "assets": {{"type": "string"}},
                    "liabilities": {{"type": "string"}},
                    "equity": {{"type": "string"}}
                }}
            }}
        }},
        "funding_history": {{"type": "string"}},
        "financial_plan": {{"type": "string"}}
    }}
}}
```
/no_think""",
            "ê¸°ìˆ  ë° ì œí’ˆ": """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•˜ëŠ” AIì…ë‹ˆë‹¤. ì ˆëŒ€ë¡œ ìš”ì•½ì´ë‚˜ ì„¤ëª…ì„ ìƒì„±í•˜ì§€ ë§ˆì‹œì˜¤.
ì‘ë‹µì€ ì•„ë˜ì— ëª…ì‹œëœ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

**ì¶”ì¶œí•  ì •ë³´:**
- í•µì‹¬ ê¸°ìˆ , ë³´ìœ  íŠ¹í—ˆ, ì£¼ìš” ì œí’ˆ ë¼ì¸ì—…, ê¸°ìˆ ì  ê²½ìŸ ìš°ìœ„.

**ì¤‘ìš”: ê¸°ìˆ  ì¶”ì¶œ ê°€ì´ë“œ:**
- AI ê²€ì‚¬, ì§„ë‹¨ ê¸°ìˆ , Vision AI ë“±ì˜ í•µì‹¬ ê¸°ìˆ ì„ ì°¾ì•„ í¬í•¨í•˜ì„¸ìš”
- "ìŠ¬ê°œê³¨ íƒˆêµ¬ AI ê²€ì‚¬", "ì¹˜ì£¼ ì§ˆí™˜ AI ê²€ì‚¬" ë“± êµ¬ì²´ì ì¸ ê¸°ìˆ ëª…ì„ í¬í•¨í•˜ì„¸ìš”
- ì •í™•ë„, ì˜ë£Œê¸°ê¸° í—ˆê°€ ìƒíƒœ ë“± ê¸°ìˆ ì  íŠ¹ì§•ì„ í•¨ê»˜ ê¸°ë¡í•˜ì„¸ìš”
- ì œí’ˆëª…ê³¼ ì„œë¹„ìŠ¤ëª…ì„ êµ¬ë¶„í•˜ì—¬ ê¸°ë¡í•˜ì„¸ìš” (ì˜ˆ: "ë¼ì´í« ì•±", "API ì„œë¹„ìŠ¤" ë“±)

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
```{context}```

**JSON ìŠ¤í‚¤ë§ˆ:**
```json
{{
    "properties": {{
        "core_technologies": {{"type": "array", "items": {{"type": "string"}}, "description": "í•µì‹¬ ê¸°ìˆ  ëª©ë¡ - êµ¬ì²´ì ì¸ ê¸°ìˆ ëª…ê³¼ íŠ¹ì§• í¬í•¨"}},
        "patents": {{"type": "array", "items": {{"type": "string"}}, "description": "ë³´ìœ  íŠ¹í—ˆ ëª©ë¡"}},
        "product_lineup": {{"type": "array", "items": {{"type": "string"}}, "description": "ì£¼ìš” ì œí’ˆ ë° ì„œë¹„ìŠ¤ ë¼ì¸ì—…"}},
        "tech_advantage": {{"type": "string", "description": "ê²½ìŸì‚¬ ëŒ€ë¹„ ê¸°ìˆ ì  ìš°ìœ„ ìš”ì•½"}}
    }}
}}
```
/no_think""",
            "ì‚¬ì—… ë° ì‹œì¥ ì „ëµ": """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•˜ëŠ” AIì…ë‹ˆë‹¤. ì ˆëŒ€ë¡œ ìš”ì•½ì´ë‚˜ ì„¤ëª…ì„ ìƒì„±í•˜ì§€ ë§ˆì‹œì˜¤.
ì‘ë‹µì€ ì•„ë˜ì— ëª…ì‹œëœ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.

**ì¶”ì¶œí•  ì •ë³´:**
- íƒ€ê²Ÿ ì‹œì¥, ì£¼ìš” ê²½ìŸì‚¬, ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸, í–¥í›„ í™•ì¥ ì „ëµ.

**ì¤‘ìš”: ê²½ìŸì‚¬ ì¶”ì¶œ ê°€ì´ë“œ:**
- í…ìŠ¤íŠ¸ì— í‘œ(table) í˜•íƒœë¡œ ëœ ê²½ìŸì‚¬ ì •ë³´ê°€ ìˆì„ ê²½ìš°, ëª¨ë“  ê¸°ì—…ëª…ì„ ì¶”ì¶œí•˜ì„¸ìš”
- "ì£¼ìš” ë°˜ë ¤ë™ë¬¼ ì§„ë‹¨ê¸°ì—…", "ê²½ìŸì‚¬", "competitor" ë“±ì˜ í‚¤ì›Œë“œ ì£¼ë³€ ì •ë³´ë¥¼ ì£¼ì˜ê¹Šê²Œ í™•ì¸í•˜ì„¸ìš”
- ê¸°ì—…ëª…ë§Œ ì¶”ì¶œí•˜ê³ , ì„œë¹„ìŠ¤ëª…ì´ë‚˜ ì„¤ëª…ì€ ì œì™¸í•˜ì„¸ìš” (ì˜ˆ: "í”¼í‹°í«", "ê·¸ë¦°í«", "ì—‘ìŠ¤ì¹¼ë¦¬ë²„" ë“±)

**ë¶„ì„í•  í…ìŠ¤íŠ¸:**
```{context}```

**JSON ìŠ¤í‚¤ë§ˆ:**
```json
{{
    "properties": {{
        "target_market": {{"type": "string", "description": "íƒ€ê²Ÿ ì‹œì¥ì˜ ê·œëª¨ ë° íŠ¹ì§•"}},
        "competitors": {{"type": "array", "items": {{"type": "string"}}, "description": "ì£¼ìš” ê²½ìŸì‚¬ ëª©ë¡ - ê¸°ì—…ëª…ë§Œ í¬í•¨"}},
        "business_model": {{"type": "string", "description": "ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ë° ì£¼ìš” ìˆ˜ìµì›"}},
        "scale_up_strategy": {{"type": "string", "description": "í–¥í›„ ì‚¬ì—…í™” ë˜ëŠ” í™•ì¥ ì „ëµ"}}
    }}
}}
```
/no_think"""
        }
        detailed_info = {}
        for theme, headers in grouped_headers.items():
            if theme not in extraction_prompts or not headers: continue
            logger.info(f"  - ì‹¬ì¸µ ë¶„ì„ ì¤‘... ({theme})")
            # ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œ ì¶”ê°€ ê²€ìƒ‰ ë¡œì§
            context_parts = []
            for h in headers:
                content = find_matching_content(h)
                if content:
                    context_parts.append(content)
            
            # ëª¨ë“  ì£¼ì œì— ëŒ€í•´ ë‹¤ì¸µ í—¤ë” ë° í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ê°€ ê²€ìƒ‰
            theme_specific_keywords = {
                "ê²½ì˜ì§„ ë° ì¡°ì§": ["íŒ€", "êµ¬ì„±ì›", "ì¡°ì§", "ê²½ì˜ì§„", "ceo", "cto", "cfo", "coo", "ì°½ì—…ì", "ëŒ€í‘œ", "ì´ì‚¬", "ì „ë¬´", "íŒ€ì¥", "ì„ì›", "ì§ì›", "ì£¼ì£¼"],
                "ê¸°ìˆ  ë° ì œí’ˆ": ["ê¸°ìˆ ", "ai", "ê²€ì‚¬", "ì§„ë‹¨", "ì œí’ˆ", "vision", "ì •í™•ë„", "ì•Œê³ ë¦¬ì¦˜", "ëª¨ë¸", "íŠ¹í—ˆ", "ê°œë°œ", "ë°¸ëŸ°ì‹±", "ë°°í„°ë¦¬", "ì†”ë£¨ì…˜"],
                "ì‚¬ì—… ë° ì‹œì¥ ì „ëµ": ["ì‹œì¥", "ê²½ìŸ", "ê²½ìŸì‚¬", "ì „ëµ", "ë¹„ì¦ˆë‹ˆìŠ¤", "ê³ ê°", "íŒŒíŠ¸ë„ˆ", "í˜‘ë ¥"],
                "ì¬ë¬´ í˜„í™©": ["ë§¤ì¶œ", "íˆ¬ì", "ìê¸ˆ", "ì¬ë¬´", "billion", "million", "ì–µ", "ì›", "ê³„íš", "seed", "series", "ì£¼ì£¼", "ì§€ë¶„", "ìœ ì¹˜"],
                "íšŒì‚¬ ê°œìš”": ["íšŒì‚¬", "ì„¤ë¦½", "ê°œìš”", "ì†Œê°œ", "ë³¸ì‚¬", "ì—°ë½ì²˜", "ì •ë³´", "ë²•ì¸", "who we are"]
            }
            
            if theme in theme_specific_keywords:
                keywords = theme_specific_keywords[theme]
                logger.info(f"{theme} ì¶”ê°€ ê²€ìƒ‰ ì‹œì‘ (í‚¤ì›Œë“œ: {len(keywords)}ê°œ)")
                
                for chunk_header, chunk_content in chunk_map.items():
                    # ë‹¤ì¸µ í—¤ë” ê²€ìƒ‰ (ë ˆë²¨ 2-4)
                    header_level = ""
                    if chunk_header.startswith("#### "):
                        header_level = "L4"
                    elif chunk_header.startswith("### "):
                        header_level = "L3"
                    elif chunk_header.startswith("## "):
                        header_level = "L2"
                    
                    # í—¤ë” ë ˆë²¨ë³„ í‚¤ì›Œë“œ ë§¤ì¹­
                    if header_level and any(kw in chunk_header.lower() for kw in keywords):
                        min_length = 30 if header_level == "L4" else 50  # L4 í—¤ë”ëŠ” ë” ì§§ì€ ë‚´ìš©ë„ í—ˆìš©
                        if chunk_content and len(chunk_content.strip()) > min_length:
                            logger.info(f"{theme} {header_level} í—¤ë” ë°œê²¬: {chunk_header} (ê¸¸ì´: {len(chunk_content)})")
                            if chunk_content not in context_parts:
                                context_parts.append(chunk_content)
                    
                    # ì¼ë°˜ í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ê°€ ê²€ìƒ‰ (í—¤ë”ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°)
                    elif not header_level and any(kw in chunk_header.lower() for kw in keywords):
                        if chunk_content and len(chunk_content.strip()) > 100:
                            logger.info(f"{theme} í‚¤ì›Œë“œ ë§¤ì¹­: {chunk_header} (ê¸¸ì´: {len(chunk_content)})")
                            if chunk_content not in context_parts:
                                context_parts.append(chunk_content)
            
            # ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ê¸°ë°˜ ì •ë ¬ (ê¸¸ì´ì™€ êµ¬ì¡°ì  ì™„ì„±ë„ ê³ ë ¤)
            if context_parts:
                context_with_scores = []
                for content in context_parts:
                    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ê¸¸ì´ + êµ¬ì¡°ì  íŠ¹ì„±)
                    score = len(content)
                    if "####" in content:  # ë ˆë²¨ 4 í—¤ë” í¬í•¨ì‹œ ê°€ì 
                        score += 100
                    if "|" in content and "---" in content:  # í…Œì´ë¸” êµ¬ì¡° ê°€ì 
                        score += 200
                    context_with_scores.append((content, score))
                
                # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê³ í’ˆì§ˆ ì»¨í…ìŠ¤íŠ¸ ìš°ì„  ë°°ì¹˜
                context_with_scores.sort(key=lambda x: x[1], reverse=True)
                context_parts = [content for content, score in context_with_scores]
                
                logger.info(f"{theme} ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜: {[score for content, score in context_with_scores]}")
            
            context = "\n\n---\n\n".join(context_parts)
            if not context.strip():
                logger.warning(f"  - '{theme}' ì£¼ì œì— í•´ë‹¹í•˜ëŠ” ë‚´ìš©ì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                logger.warning(f"  - ì°¾ìœ¼ë ¤ë˜ í—¤ë”: {headers}")
                detailed_info[theme] = {}
                continue
            
            # ê²½ì˜ì§„ ë° ì¡°ì§ ì£¼ì œì— ëŒ€í•œ ì¶”ê°€ ë””ë²„ê¹… ì •ë³´
            if theme == "ê²½ì˜ì§„ ë° ì¡°ì§":
                logger.info(f"  - ê²½ì˜ì§„ í—¤ë” ëª©ë¡: {headers}")
                logger.info(f"  - ì¶”ì¶œëœ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)} ë¬¸ì")
                logger.info(f"  - ì»¨í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {context[:200]}...")
            template = extraction_prompts[theme]
            prompt = PromptTemplate.from_template(template)
            chain = prompt | self.llm | StrOutputParser()
            try:
                raw_output = chain.invoke({"context": context})
                extracted_data = self._clean_and_parse_json(raw_output)
                if extracted_data is not None:
                    detailed_info[theme] = extracted_data
                else:
                    detailed_info[theme] = {"error": "LLM ì¶œë ¥ì—ì„œ JSONì„ ì°¾ì§€ ëª»í•¨"}
            except Exception as e:
                logger.error(f"  - '{theme}' ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                detailed_info[theme] = {"error": str(e)}
        logger.info("ì‹¬ì¸µ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ.")
        return detailed_info

    def analyze(self, md_file_path: str) -> Dict[str, Any]:
        logger.info(f"ë¶„ì„ ì‹œì‘: {md_file_path}")
        try:
            md_content = Path(md_file_path).read_text(encoding='utf-8')
        except FileNotFoundError:
            logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {md_file_path}")
            return {}
        chunks = self._split_md_by_headers(md_content)
        indexed_chunks = self._generate_chunk_summaries(chunks)
        grouped_headers = self._group_chunks_by_theme(indexed_chunks)
        final_analysis = self._extract_detailed_info(grouped_headers, chunks)
        logger.info("ë¶„ì„ ì™„ë£Œ.")
        return {
            "file_info": {"path": md_file_path},
            "analysis_result": final_analysis,
            "debug_info": {"total_chunks": len(chunks), "grouped_headers": grouped_headers}
        }

    def save_results(self, analysis_result: Dict[str, Any], output_dir: str, file_stem: str):
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        json_path = output_path / f"{file_stem}_vision_analysis.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_result, f, ensure_ascii=False, indent=2)
        logger.info(f"JSON ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {json_path}")
        md_path = output_path / f"{file_stem}_vision_summary.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(f"# {file_stem} Vision IR ë¶„ì„ ë³´ê³ ì„œ\n\n")
            data = analysis_result.get("analysis_result", {})
            for theme, content in data.items():
                f.write(f"## ğŸ“Š {theme}\n\n")
                if isinstance(content, dict):
                    for key, value in content.items():
                        f.write(f"- **{key}**: {json.dumps(value, ensure_ascii=False, indent=2)}\n")
                elif isinstance(content, list):
                    for item in content:
                        f.write("- ---\n")
                        if isinstance(item, dict):
                            for key, value in item.items():
                                f.write(f"  - **{key}**: {value}\n")
                        else:
                            f.write(f"  - {item}\n")
                else:
                    f.write(f"{content}\n")
                f.write("\n")
        logger.info(f"Markdown ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: {md_path}")

def main():
    parser = argparse.ArgumentParser(description="Vision-LLMìœ¼ë¡œ ìƒì„±ëœ IR ë§ˆí¬ë‹¤ìš´ ë¶„ì„ê¸°")
    parser.add_argument("md_path", type=str, help="ë¶„ì„í•  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì˜ ê²½ë¡œ")
    parser.add_argument("--output_dir", type=str, default="analysis_results", help="ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë  ë””ë ‰í† ë¦¬")
    parser.add_argument("--model", type=str, default=MODEL_ID, help="Ollama ëª¨ë¸ ID")
    parser.add_argument("--url", type=str, default=BASE_URL, help="Ollama ì„œë²„ URL")
    args = parser.parse_args()
    if not OLLAMA_AVAILABLE:
        logger.error("í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ì„¤ì¹˜ ì•ˆë‚´ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return
    analyzer = VisionMDAnalyzer(model_id=args.model, base_url=args.url)
    if not analyzer.use_llm:
        logger.error("LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    results = analyzer.analyze(args.md_path)
    if results:
        file_stem = Path(args.md_path).stem
        analyzer.save_results(results, args.output_dir, file_stem)
        print("\n" + "="*80)
        print("âœ… ë¶„ì„ ë° ê²°ê³¼ ì €ì¥ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“ JSON ê²°ê³¼: {Path(args.output_dir) / f'{file_stem}_vision_analysis.json'}")
        print(f"ğŸ“„ MD ë³´ê³ ì„œ: {Path(args.output_dir) / f'{file_stem}_vision_summary.md'}")
        print("="*80)

if __name__ == "__main__":
    main()