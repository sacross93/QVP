import json
import re
import os
import time # ì‹œê°„ ì§€ì—°ì„ ìœ„í•´ time ëª¨ë“ˆ ì¶”ê°€
import argparse # ì¸ì ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¶”ê°€
import datetime # íŒŒì¼ ì €ì¥ì„ ìœ„í•´ datetime ëª¨ë“ˆ ì¶”ê°€
from glob import glob
from typing import List, Dict, Any

from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_ollama import ChatOllama
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel
from langchain.retrievers.multi_query import MultiQueryRetriever
import sys
from langchain_core.embeddings import Embeddings

# --- 0. ìƒìˆ˜ ì •ì˜ ---
VECTOR_DB_PATH = "chroma_db_bge_m3_ko"
DOCUMENT_PATH = "data/merged.txt"
SUMMARY_PATH = "data/summary_refined.txt" # ìš”ì•½ íŒŒì¼ ê²½ë¡œ ì¶”ê°€
MODEL_ID = "dragonkue/bge-m3-ko"

# ì½”ìŠ¤ë‹¥ ìƒì¥ ê·œì • í…ìŠ¤íŠ¸ (ì§€ì‹ ë² ì´ìŠ¤ë¡œ í™œìš©)
KOSDAQ_LISTING_REQUIREMENTS = """
### ì½”ìŠ¤ë‹¥ ìƒì¥ ìˆ˜ì¹˜ ê¸°ì¤€

#### ê¸°ë³¸ ìš”ê±´
- **ë‚©ì…ìë³¸ê¸ˆ**: 3ì–µ ì› ì´ìƒ
- **ìê¸°ìë³¸**: 10ì–µ ì› ì´ìƒ
- **ì†Œì•¡ì£¼ì£¼**: 500ëª… ì´ìƒ
- **ì˜ì—…í™œë™ ê¸°ê°„**: 3ë…„ ì´ìƒ

#### ì£¼ì‹ë¶„ì‚° ìš”ê±´ (íƒ1)
- **íŠ¸ë™ 1**: ì†Œì•¡ì£¼ì£¼ 500ëª… ì´ìƒ + ì†Œì•¡ì£¼ì£¼ ì§€ë¶„ìœ¨ 25% ì´ìƒ + ê³µëª¨ 5% ì´ìƒ
- **íŠ¸ë™ 2**: ìê¸°ìë³¸ 500ì–µ ì› ì´ìƒ + ì†Œì•¡ì£¼ì£¼ 500ëª… ì´ìƒ + ê³µëª¨ 10% ì´ìƒ
- **íŠ¸ë™ 3**: ê³µëª¨ 25% ì´ìƒ + ì†Œì•¡ì£¼ì£¼ 500ëª… ì´ìƒ

#### ê²½ì˜ì„±ê³¼ ìš”ê±´ (ë‹¤ì–‘í•œ íŠ¸ë™ ì¤‘ íƒ1)
- **ì´ìµì‹¤í˜„ ìƒì¥ìš”ê±´**:
  - ë²•ì¸ì„¸ ì°¨ê°ì „ ê³„ì†ì‚¬ì—…ì´ìµ 20ì–µ ì› + ì‹œê°€ì´ì•¡ 90ì–µ ì› ì´ìƒ
  - ë²•ì¸ì„¸ ì°¨ê°ì „ ê³„ì†ì‚¬ì—…ì´ìµ 20ì–µ ì› + ìê¸°ìë³¸ 30ì–µ ì› ì´ìƒ
  - ì‹œê°€ì´ì•¡ 200ì–µ ì› + ë§¤ì¶œì•¡ 100ì–µ ì› ì´ìƒ + ê³„ì†ì‚¬ì—…ì´ìµ ë°œìƒ
  - ê³„ì†ì‚¬ì—…ì´ìµ 50ì–µ ì› ì´ìƒ (ë‹¨ë… ìš”ê±´)
- **ì´ìµë¯¸ì‹¤í˜„ ìƒì¥ìš”ê±´(í…ŒìŠ¬ë¼ ìš”ê±´)**:
  - ì‹œê°€ì´ì•¡ 500ì–µ ì› + ë§¤ì¶œì•¡ 30ì–µ ì› + ìµœê·¼ 2ë…„ ì—°ì† ë§¤ì¶œì¦ê°€ìœ¨ 20% ì´ìƒ
  - ì‹œê°€ì´ì•¡ 1,000ì–µ ì› ì´ìƒ (ë‹¨ë… ìš”ê±´)
  - ìê¸°ìë³¸ 250ì–µ ì› ì´ìƒ (ë‹¨ë… ìš”ê±´)
  - ì‹œê°€ì´ì•¡ 300ì–µ ì› + ë§¤ì¶œì•¡ 100ì–µ ì› ì´ìƒ

#### ê¸°ìˆ ì„±ì¥ê¸°ì—… íŠ¹ë¡€
- **ìê¸°ìë³¸**: 10ì–µ ì› ì´ìƒ
- **ì‹œê°€ì´ì•¡**: 90ì–µ ì› ì´ìƒ
- **ê¸°ìˆ í‰ê°€**: ì „ë¬¸í‰ê°€ê¸°ê´€ Aë“±ê¸‰ & BBBë“±ê¸‰ ì´ìƒ

#### ê¸°íƒ€ ì§ˆì  ì‹¬ì‚¬ìš”ê±´
- **ê°ì‚¬ ìš”ê±´**: ìµœê·¼ ì‚¬ì—…ì—°ë„ ì¬ë¬´ì œí‘œì— ëŒ€í•œ ì§€ì •ê°ì‚¬ì¸ì˜ 'ì ì •' ê°ì‚¬ì˜ê²¬ í•„ìˆ˜
- **ê¸°ì—… ê³„ì†ì„±, ê²½ì˜ íˆ¬ëª…ì„±, ê²½ì˜ ì•ˆì •ì„±** ì¢…í•© í‰ê°€
"""

# --- 1. LLM, ì„ë² ë”© ëª¨ë¸, í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • ---
print("LLM ë° ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
llm = ChatOllama(
    model="qwen3:32b",
    base_url="http://192.168.120.102:11434",
    temperature=0
)

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (HuggingFaceEmbeddings ë˜í¼ ëŒ€ì‹  ì§ì ‘ ì‚¬ìš©)
embedding_model = SentenceTransformer(MODEL_ID)

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000, # ì²­í¬ í¬ê¸°ë¥¼ ì•½ê°„ ì¤„ì—¬ ë§¥ë½ ì§‘ì¤‘
    chunk_overlap=200,
    length_function=len,
    is_separator_regex=False,
)

# Langchainì˜ Embeddings í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ ì§ì ‘ êµ¬í˜„
class CustomSentenceTransformerEmbeddings(Embeddings):
    def __init__(self, model):
        self.model = model

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ ëª©ë¡ì„ ì„ë² ë”©í•©ë‹ˆë‹¤."""
        return self.model.encode(texts, normalize_embeddings=True).tolist()

    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤."""
        return self.model.encode([text], normalize_embeddings=True)[0].tolist()

# --- ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ ì¶”ê°€ ---
def web_search_company_info(company_name: str, search_query: str) -> str:
    """
    ì›¹ ê²€ìƒ‰ì„ í†µí•´ íšŒì‚¬ ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    DuckDuckGoë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê³  ê°œì¸ì •ë³´ ë³´í˜¸ì— ì¤‘ì ì„ ë‘” ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    try:
        # duckduckgo-search íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì•¼ í•¨: pip install duckduckgo-search
        from duckduckgo_search import DDGS
        
        ddgs = DDGS()
        search_term = f"{company_name} {search_query}"
        
        print(f"  ğŸ” ì›¹ ê²€ìƒ‰: '{search_term}'")
        
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (ìµœëŒ€ 5ê°œ)
        results = ddgs.text(search_term, max_results=5)
        
        if not results:
            return "ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬ë§·íŒ…í•©ë‹ˆë‹¤
        formatted_results = []
        for i, result in enumerate(results, 1):
            title = result.get('title', 'N/A')
            snippet = result.get('body', 'N/A')
            url = result.get('href', 'N/A')
            
            formatted_result = f"""
[ê²€ìƒ‰ ê²°ê³¼ {i}]
ì œëª©: {title}
ë‚´ìš©: {snippet}
ì¶œì²˜: {url}
"""
            formatted_results.append(formatted_result)
        
        return "\n".join(formatted_results)
        
    except ImportError:
        print("  âš ï¸ duckduckgo-search íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("  ì„¤ì¹˜ ëª…ë ¹: pip install duckduckgo-search")
        return "ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ duckduckgo-search íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”."
    except Exception as e:
        print(f"  âŒ ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return f"ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# ì •ë³´ ì¶©ì¡±ë„ ë¶„ì„ í•¨ìˆ˜
def analyze_information_sufficiency(content: str) -> bool:
    """
    ì£¼ì–´ì§„ ë‚´ìš©ì´ ì¶©ë¶„í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    if not content or content.strip() == "":
        return False
    
    # ì •ë³´ ë¶€ì¡±ì„ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì›Œë“œë“¤
    insufficient_indicators = [
        "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ",
        "ì •ë³´ ë¶€ì¡±",
        "í™•ì¸ í•„ìš”",
        "ë°ì´í„° ì—†ìŒ",
        "N/A",
        "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤",
        "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
    ]
    
    content_lower = content.lower()
    for indicator in insufficient_indicators:
        if indicator.lower() in content_lower:
            return False
    
    # ìµœì†Œ ê¸¸ì´ ì²´í¬ (ë§¤ìš° ì§§ì€ ë‚´ìš©ì€ ë¶ˆì¶©ë¶„í•˜ë‹¤ê³  íŒë‹¨)
    if len(content.strip()) < 50:
        return False
        
    return True

# ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ì²´ì¸
web_search_query_prompt = PromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì›¹ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
    
íšŒì‚¬ëª…: {company_name}
ë¶„ì„ í•­ëª©: {analysis_item}

ìœ„ íšŒì‚¬ì— ëŒ€í•´ í•´ë‹¹ ë¶„ì„ í•­ëª©ì˜ ì •ë³´ë¥¼ ì°¾ê¸° ìœ„í•œ íš¨ê³¼ì ì¸ ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
ê²€ìƒ‰ ì¿¼ë¦¬ëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì‹¤ì œ ê²€ìƒ‰ ì—”ì§„ì—ì„œ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ì´ê³  ëª…í™•í•´ì•¼ í•©ë‹ˆë‹¤.

JSON í˜•ì‹ìœ¼ë¡œ 3ê°œì˜ ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
{{"queries": ["ê²€ìƒ‰ì¿¼ë¦¬1", "ê²€ìƒ‰ì¿¼ë¦¬2", "ê²€ìƒ‰ì¿¼ë¦¬3"]}}"""
)

web_search_query_chain = (
    web_search_query_prompt
    | llm
    | JsonOutputParser()
    | RunnableLambda(lambda x: x.get("queries", []))
)

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---
def read_file_content(filepath: str) -> str:
    """íŒŒì¼ ë‚´ìš©ì„ ì½ì–´ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        print(f"ê²½ê³ : {filepath} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return ""

# --- 2. ì˜¤í”„ë¼ì¸: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • (ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•) ---
def get_vectorstore() -> Chroma:
    """
    ë¬¸ì„œì—ì„œ ë²¡í„° DBë¥¼ ìƒì„±í•˜ê±°ë‚˜ ê¸°ì¡´ DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    'ì˜¤í”„ë¼ì¸: ë¬¸ì„œ ì²˜ë¦¬ ë° ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•' ë‹¨ê³„ì— í•´ë‹¹í•©ë‹ˆë‹¤.
    """
    # Langchain í˜¸í™˜ ì„ë² ë”© í•¨ìˆ˜ ë˜í¼ë¥¼ ë¨¼ì € ìƒì„±
    embedding_function_wrapper = CustomSentenceTransformerEmbeddings(embedding_model)

    if os.path.exists(VECTOR_DB_PATH):
        print(f"ê¸°ì¡´ ë²¡í„° DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {VECTOR_DB_PATH}")
        vectorstore = Chroma(
            persist_directory=VECTOR_DB_PATH,
            embedding_function=embedding_function_wrapper # ë¡œë“œ ì‹œì ë¶€í„° ë˜í¼ë¥¼ ì§€ì •
        )
        return vectorstore
    else:
        print(f"ìƒˆë¡œìš´ ë²¡í„° DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë¬¸ì„œ: {DOCUMENT_PATH}")
        print("ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤...")
        with open(DOCUMENT_PATH, 'r', encoding='utf-8') as f:
            text = f.read()

        docs_list = text_splitter.split_text(text)
        
        print(f"{len(docs_list)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë˜ì—ˆìŠµë‹ˆë‹¤. ì„ë² ë”© ë° DB ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        vectorstore = Chroma.from_texts(
            texts=docs_list,
            embedding=embedding_function_wrapper, # ìƒì„± ì‹œì—ë„ ë˜í¼ë¥¼ ì‚¬ìš©
            ids=[str(i) for i in range(len(docs_list))],
            persist_directory=VECTOR_DB_PATH,
        )
        vectorstore.persist()
        print("ë²¡í„° DB ìƒì„± ë° ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return vectorstore


# --- 3. ì˜¨ë¼ì¸: RAG íŒŒì´í”„ë¼ì¸ êµ¬ì„± ---

# 3-1. ì§ˆë¬¸ ë¶„í•´ (Query Decomposition)
query_decomposition_prompt = PromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ë³µí•©ì ì¸ ì§ˆë¬¸ì„ ì—¬ëŸ¬ ê°œì˜ ê°„ë‹¨í•œ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê° í•˜ìœ„ ì§ˆë¬¸ì€ ë…ë¦½ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ê¸° ìœ„í•´ í•„ìš”í•œ ëª¨ë“  ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{question}"

ê° í•˜ìœ„ ì§ˆë¬¸ì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ JSON í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”. ì˜ˆ: {{"queries": ["ì§ˆë¬¸ 1", "ì§ˆë¬¸ 2", "ì§ˆë¬¸ 3"]}}
"""
)

decompose_chain = (
    query_decomposition_prompt
    | llm
    | JsonOutputParser()
    | RunnableLambda(lambda x: x.get("queries", []))
)

# 3-2. ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
def format_docs(docs: List[Document]) -> str:
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í”„ë¡¬í”„íŠ¸ì— ì‚½ì…í•˜ê¸° ì¢‹ì€ í˜•íƒœë¡œ í¬ë§·í•©ë‹ˆë‹¤."""
    return "\n\n---\n\n".join(doc.page_content for doc in docs)

# 3-3. ìµœì¢… ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
final_prompt_template = ChatPromptTemplate.from_messages([
    ("system",
     """ë‹¹ì‹ ì€ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¢…í•©ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. ëª¨ë“  ë‹µë³€ì€ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•´ì•¼ í•©ë‹ˆë‹¤.
ë§Œì•½ ì»¨í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë‹µë³€í•˜ê¸° ì–´ë ¤ìš´ ê²½ìš°, "ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ì†”ì§í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

[ì»¨í…ìŠ¤íŠ¸ ì •ë³´]
{context}
"""),
    ("human", "[ì›ë³¸ ì§ˆë¬¸]\n{question}"),
    ("ai", "ë‹µë³€:")
])


# --- 4. ìƒˆë¡œìš´ íŒŒì´í”„ë¼ì¸: IPO ë³´ê³ ì„œ ìƒì„± ---

# 4-1. ë¶„ì„ ì²´í¬ë¦¬ìŠ¤íŠ¸ ìƒì„± ì²´ì¸
checklist_generation_prompt = PromptTemplate.from_template(
    """ë‹¹ì‹ ì€ IPO ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì½”ìŠ¤ë‹¥ ìƒì¥ ê·œì •ì„ ë°”íƒ•ìœ¼ë¡œ íŠ¹ì • íšŒì‚¬ì˜ ìƒì¥ ê°€ëŠ¥ì„±ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ ë°˜ë“œì‹œ í™•ì¸í•´ì•¼ í•  í•­ëª©ë“¤ì„ êµ¬ì²´ì ì¸ ì§ˆë¬¸ í˜•íƒœì˜ JSON ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

[ì½”ìŠ¤ë‹¥ ìƒì¥ ê·œì •]
{regulations}

JSON í˜•ì‹ ì˜ˆì‹œ: {{"checklist": ["ì§ˆë¬¸ 1", "ì§ˆë¬¸ 2", "ì§ˆë¬¸ 3"]}}"""
)

checklist_chain = (
    checklist_generation_prompt
    | llm
    | JsonOutputParser()
    | RunnableLambda(lambda x: x.get("checklist", []))
)

# 4-2. ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì²´ì¸
report_generation_prompt = ChatPromptTemplate.from_messages([
    ("system",
     """ë‹¹ì‹ ì€ M&A ë° IPO ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ [ì •ë³´]ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ '{company_name}'ì˜ ì½”ìŠ¤ë‹¥ ìƒì¥ ê°€ëŠ¥ì„±ì„ ë¶„ì„í•˜ê³ , í–¥í›„ 1~2ë…„ ë‚´ ìƒì¥ì„ ìœ„í•œ ë¡œë“œë§µì„ ì œì‹œí•˜ëŠ” ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ ì£¼ì‹­ì‹œì˜¤.

ëª¨ë“  ë¶„ì„ê³¼ ì£¼ì¥ì€ ë°˜ë“œì‹œ ì œê³µëœ [ì •ë³´]ì— ê·¼ê±°í•´ì•¼ í•©ë‹ˆë‹¤. ë§Œì•½ íŠ¹ì • í•­ëª©ì— ëŒ€í•œ ì •ë³´ê°€ ë¶€ì¡±í•˜ë‹¤ë©´, "ì •ë³´ ë¶€ì¡±" ë˜ëŠ” "í™•ì¸ í•„ìš”"ë¼ê³  ëª…í™•í•˜ê²Œ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤. ë³´ê³ ì„œëŠ” ë‹¤ìŒ ë‘ ê°€ì§€ í•µì‹¬ ì„¹ì…˜ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

1. **í˜„í™© ì§„ë‹¨**: ê° ìƒì¥ ìš”ê±´(ìë³¸, ê²½ì˜ ì„±ê³¼, ê¸°ìˆ ì„± ë“±)ì— ëŒ€í•´ '{company_name}'ì˜ í˜„ì¬ ìƒíƒœë¥¼ ì¡°ëª©ì¡°ëª© ë¹„êµ ë¶„ì„í•˜ì—¬ 'ì¶©ì¡±', 'ë¯¸ì¶©ì¡±', 'ì •ë³´ ë¶€ì¡±'ìœ¼ë¡œ íŒì •í•©ë‹ˆë‹¤.
2. **ì‹¤í–‰ ê³„íš (Roadmap)**: 'ë¯¸ì¶©ì¡±' ë˜ëŠ” 'ì •ë³´ ë¶€ì¡±'ìœ¼ë¡œ íŒë‹¨ëœ í•­ëª©ë“¤ì„ í•´ê²°í•˜ê³  1~2ë…„ ë‚´ ìƒì¥ì„ ìœ„í•´ ë¬´ì—‡ì„, ì–¸ì œê¹Œì§€, ì–´ë–»ê²Œ ì¤€ë¹„í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì‹¤í–‰ ê³„íšì„ ì œì‹œí•©ë‹ˆë‹¤.

---
[ì •ë³´ 1: ì½”ìŠ¤ë‹¥ ìƒì¥ ê·œì •]
{regulations}
---
[ì •ë³´ 2: {company_name} í˜„í™© ìš”ì•½]
{summary}
---
[ì •ë³´ 3: í•­ëª©ë³„ ì‹¬ì¸µ ë¶„ì„ ìë£Œ (ë‚´ë¶€ ë°ì´í„° ê¸°ë°˜)]
{analysis_results}
---

[ë³´ê³ ì„œ ì‘ì„± ì‹œì‘]"""),
    ("human", "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."),
])

report_chain = (
    report_generation_prompt
    | llm
    | StrOutputParser()
)


# --- 5. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ---
def generate_ipo_report(company_name: str):
    """IPO ìƒì¥ ê°€ëŠ¥ì„± ë¶„ì„ ë³´ê³ ì„œ ìƒì„± íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""

    # 1ë‹¨ê³„: ì§€ì‹ ë² ì´ìŠ¤ ì¤€ë¹„ (VectorDB, ìš”ì•½ë³¸, ê·œì •)
    print("[1ë‹¨ê³„] ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤ (VectorDB, ìš”ì•½ë³¸, ê·œì •)...")
    vectorstore = get_vectorstore()
    base_retriever = vectorstore.as_retriever(search_kwargs={"k": 3}) # ê° í•­ëª© ë‹¹ 3ê°œ ì²­í¬ ê²€ìƒ‰

    # MultiQueryRetrieverë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì •ì˜ (í‚¤ì›Œë“œ ì¶”ì¶œ ê°•í™”)
    MULTI_QUERY_PROMPT = PromptTemplate(
        input_variables=["question"],
        template="""ë‹¹ì‹ ì€ AI ì–¸ì–´ ëª¨ë¸ ë³´ì¡°ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ì— ì‚¬ìš©í•  ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë¯¸ë¥¼ í­ë„“ê²Œ íŒŒì•…í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ í˜•íƒœì˜ 'í•˜ìœ„ ì§ˆë¬¸'ê³¼, ê°€ì¥ í•µì‹¬ì ì¸ ì •ë³´ë¥¼ ì§ì ‘ ì°¾ê¸° ìœ„í•œ 'í•µì‹¬ í‚¤ì›Œë“œ'ë¥¼ ëª¨ë‘ ìƒì„±í•´ì£¼ì„¸ìš”.
ì´ë ‡ê²Œ ìƒì„±ëœ ì§ˆë¬¸ê³¼ í‚¤ì›Œë“œëŠ” ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
ê° ì¿¼ë¦¬ëŠ” ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì œê³µí•˜ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {question}"""
    )

    multi_query_retriever = MultiQueryRetriever.from_llm(
        retriever=base_retriever, llm=llm, prompt=MULTI_QUERY_PROMPT
    )

    company_summary = read_file_content(SUMMARY_PATH)
    
    if not company_summary:
        print("ì˜¤ë¥˜: íšŒì‚¬ ìš”ì•½ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ì–´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    # 2ë‹¨ê³„: ë¶„ì„ ì²´í¬ë¦¬ìŠ¤íŠ¸ ìƒì„±
    print("\n[2ë‹¨ê³„] ì½”ìŠ¤ë‹¥ ìƒì¥ ê·œì •ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    checklist = checklist_chain.invoke({"regulations": KOSDAQ_LISTING_REQUIREMENTS})
    print("ìƒì„±ëœ ë¶„ì„ ì²´í¬ë¦¬ìŠ¤íŠ¸:")
    for item in checklist:
        print(f"- {item}")

    # 3ë‹¨ê³„: ê° ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª©ì— ëŒ€í•´ RAG ìˆ˜í–‰ (ë¡œì»¬ DB + ì›¹ ê²€ìƒ‰)
    print("\n[3ë‹¨ê³„] ê° ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª©ì— ëŒ€í•´ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤ (ë¡œì»¬ DB + ì›¹ ê²€ìƒ‰)...")
    analysis_results_list = []
    
    for i, item in enumerate(checklist, 1):
        print(f"\n  ğŸ“‹ [{i}/{len(checklist)}] ë¶„ì„ ì¤‘: \"{item}\"")
        
        # 3-1: ë¨¼ì € ë¡œì»¬ ë²¡í„° DBì—ì„œ ê²€ìƒ‰
        print("    ğŸ“š ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰...")
        retrieved_docs = multi_query_retriever.invoke(item)
        local_context = format_docs(retrieved_docs)
        
        # 3-2: ë¡œì»¬ ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
        is_local_sufficient = analyze_information_sufficiency(local_context)
        web_context = ""
        
        if not is_local_sufficient:
            print("    ğŸŒ ë¡œì»¬ ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ ì›¹ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            # ì›¹ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            search_queries = web_search_query_chain.invoke({
                "company_name": company_name,
                "analysis_item": item
            })
            
            web_results = []
            for query in search_queries[:2]:  # ì²˜ìŒ 2ê°œ ì¿¼ë¦¬ë§Œ ì‚¬ìš©
                web_result = web_search_company_info(company_name, query)
                web_results.append(f"ê²€ìƒ‰ì¿¼ë¦¬: {query}\n{web_result}")
                time.sleep(2)  # ì›¹ ê²€ìƒ‰ ê°„ ì§€ì—°
            
            web_context = "\n\n---\n\n".join(web_results)
        else:
            print("    âœ… ë¡œì»¬ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        # 3-3: ìµœì¢… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        final_context = ""
        if local_context and local_context.strip():
            final_context += f"[ë¡œì»¬ ë°ì´í„°]\n{local_context}"
        
        if web_context and web_context.strip():
            if final_context:
                final_context += "\n\n---\n\n"
            final_context += f"[ì›¹ ê²€ìƒ‰ ê²°ê³¼]\n{web_context}"
        
        if not final_context.strip():
            final_context = "ë¡œì»¬ ë° ì›¹ ê²€ìƒ‰ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ."
        
        analysis_results_list.append(f"- ì§ˆë¬¸: {item}\n- ìë£Œ: {final_context}")

    analysis_results_str = "\n".join(analysis_results_list)
    print("\nâœ… ëª¨ë“  ì •ë³´ ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 4ë‹¨ê³„: ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œ ìƒì„±
    print("\n[4ë‹¨ê³„] ìˆ˜ì§‘ëœ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    final_report = report_chain.invoke({
        "company_name": company_name,
        "regulations": KOSDAQ_LISTING_REQUIREMENTS,
        "summary": company_summary,
        "analysis_results": analysis_results_str
    })
    
    # 5ë‹¨ê³„: ê²°ê³¼ ì¶œë ¥
    print("\n=========================================")
    print(f"        {company_name} KOSDAQ ìƒì¥ ê°€ëŠ¥ì„± ë¶„ì„ ë³´ê³ ì„œ")
    print("=========================================\n")
    print(final_report)

    # 6ë‹¨ê³„: ê²°ê³¼ íŒŒì¼ë¡œ ì €ì¥
    output_dir = "result"
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filepath = os.path.join(output_dir, f"IPO_report_{timestamp}.md")

    # <think> ë¸”ë¡ì„ ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì œê±°
    report_content_for_file = re.sub(r'<think>.*?</think>', '', final_report, flags=re.DOTALL).strip()

    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"# {company_name} KOSDAQ ìƒì¥ ê°€ëŠ¥ì„± ë¶„ì„ ë³´ê³ ì„œ\n\n")
            f.write(f"ë³´ê³ ì„œ ìƒì„± ì‹œê°: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(report_content_for_file)
        print(f"\në³´ê³ ì„œê°€ ë‹¤ìŒ ê²½ë¡œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
    except Exception as e:
        print(f"\nì˜¤ë¥˜: ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ - {e}")


if __name__ == '__main__':
    # parser = argparse.ArgumentParser(description="ì§€ëŠ¥í˜• RAG íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë³µí•© ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.")
    # parser.add_argument("question", type=str, help="LLMì—ê²Œ í•  ë³µí•© ì§ˆë¬¸")
    # args = parser.parse_args()
    # main(args.question)

    generate_ipo_report(company_name="(ì£¼)ë¹„ì—ìŠ¤ì›")

    # ì˜ˆì‹œ ì§ˆë¬¸:
    # python IPO_deep_research.py "ROBOSì˜ ì£¼ìš” ì œí’ˆê³¼ ê¸°ìˆ ì€ ë¬´ì—‡ì´ë©°, ì£¼ìš” ê²½ìŸì‚¬ëŠ” ì–´ë””ì¸ê°€ìš”? ê·¸ë¦¬ê³  íšŒì‚¬ì˜ ì¬ë¬´ ìƒíƒœëŠ” ì–´ë–¤ê°€ìš”?"