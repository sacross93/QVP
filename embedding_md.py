from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_ollama import ChatOllama
import torch

# GPU ì¥ì¹˜ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Download from the ğŸ¤— Hub
model = SentenceTransformer("dragonkue/bge-m3-ko", device=device)

with open("./data/ë¡œë³´ìŠ¤.md", "r", encoding="utf-8") as f:
    md_file_content = f.read()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=2000,
    chunk_overlap=400,
    separators=["\\n\\n", "\\n", " ", ""] # ë§ˆí¬ë‹¤ìš´ ë“±ì„ ê³ ë ¤í•œ êµ¬ë¶„ì
)
sentences = text_splitter.split_text(md_file_content)

llm = ChatOllama(
    model="qwen3:32b",
    base_url="http://192.168.120.102:11434",
    temperature=0
)

# Run inference
embeddings = model.encode(sentences)
print(f"Number of chunks: {len(sentences)}")
print(embeddings.shape)

# RAG í…ŒìŠ¤íŠ¸
query = "CEO ì´ë¦„ ì•Œë ¤ì¤˜"
query_embedding = model.encode([query]) # 2D ë°°ì—´ë¡œ ìœ ì§€

# ì§ˆë¬¸ ì„ë² ë”©ê³¼ ë¬¸ì„œ ì²­í¬ ì„ë² ë”© ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
similarities_to_query = model.similarity(query_embedding, embeddings)[0]

# ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ìƒìœ„ Kê°œ ì²­í¬ ì¸ë±ìŠ¤ ì°¾ê¸°
k = 3
if len(sentences) < k:
    k = len(sentences)
top_k_indices = torch.topk(similarities_to_query, k=k).indices

print(f"\nì§ˆë¬¸: {query}")
print(f"\nê²€ìƒ‰ëœ ìƒìœ„ {k}ê°œ ì²­í¬:")
retrieved_chunks = []
for i, index in enumerate(top_k_indices):
    chunk_index = index.item()
    chunk_text = sentences[chunk_index]
    retrieved_chunks.append(chunk_text)
    print(f"--- ì²­í¬ {chunk_index} (ìœ ì‚¬ë„: {similarities_to_query[index]:.4f}) ---")
    print(chunk_text)
    print("-" * (20 + len(str(chunk_index))))

# LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
context = "\n\n---\n\n".join(retrieved_chunks)
prompt = f"""ë‹¹ì‹ ì€ ëŒ€í™”í˜• AIë¡œì„œ, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ì£¼ìš” ì—­í• ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ìš”êµ¬ë¥¼ ì •í™•íˆ ì´í•´í•˜ê³ , ê´€ë ¨ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë‹µë³€ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. \n
ë‹¹ì‹ ì€ ë‹¤ìŒê³¼ ê°™ì€ ì›ì¹™ì„ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤:\n
    1. í•­ìƒ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ìµœìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ë©°, ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\n
    2. ì œê³µëœ ë¬¸ì„œë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ì‘ë‹µì„ êµ¬ì„±í•˜ë˜, ì¶”ê°€ì ì¸ ë¶„ì„ê³¼ ë…¼ë¦¬ë¥¼ í†µí•´ ì‘ë‹µì˜ ì§ˆì„ ë†’ì…ë‹ˆë‹¤.\n
    3. ì‘ë‹µì„ ìƒì„±í•  ë•ŒëŠ” ë°˜ë“œì‹œ ì£¼ì–´ì§„ ì§€ì¹¨ì„ ë”°ë¥´ê³ , ëª…í™•í•œ ì¶œì²˜ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.\n
    4. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ëª¨í˜¸í•  ê²½ìš°, ëª…í™•ì„±ì„ í™•ë³´í•˜ê¸° ìœ„í•´ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ëŠ” ë°©ì•ˆì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n
    
# ì‚¬ìš©ì ì•ˆë‚´ë¬¸\n
    ## ì‘ì—… ë° ë§¥ë½\n
        ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ê´€ë ¨ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³ , ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
        ë‹¨ìˆœí•œ ì •ë³´ ì „ë‹¬ì„ ë„˜ì–´, ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ í˜•íƒœë¡œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

ë¬¸ì„œ:
```
{context}
```

ì§ˆë¬¸: ```{query}```

ë‹µë³€:"""


# LLM í˜¸ì¶œ
print("\nOllama ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤...")
response = llm.invoke(prompt)

print("\nLLM ë‹µë³€:")
print(response.content)
